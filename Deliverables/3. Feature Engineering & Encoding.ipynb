{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<h3 align=\"center\">Machine Learning - Project</h3>**\n",
    "## **<h3 align=\"center\">3. Feature Engineering & Encoding</h3>**\n",
    "### **<h3 align=\"center\">Group 30 - Project</h3>**\n",
    "\n",
    "\n",
    "### Group Members\n",
    "| Name              | Email                        | Student ID |\n",
    "|-------------------|------------------------------|------------|\n",
    "| Alexandra Pinto   | 20211599@novaims.unl.pt      | 20211599   |\n",
    "| Gon√ßalo Peres     | 20211625@novaims.unl.pt      | 20211625   |\n",
    "| Leonor Mira       | 20240658@novaims.unl.pt      | 20240658   |\n",
    "| Miguel Nat√°rio    | 20240498@novaims.unl.pt      | 20240498   |\n",
    "| Nuno Bernardino   | 20211546@novaims.unl.pt      | 20211546   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### **3. Feature Engineering & Encoding Notebook**\n",
    "**Description:**  \n",
    "This notebook builds upon the preprocessed dataset from the Preprocessing & Cleaning notebook to prepare features for hierarchical classification. Key steps include:  \n",
    "- **Feature Engineering:** Create or transform features to enhance predictive power, including interaction terms, date-based calculations, and aggregations.  \n",
    "- **Encoding Categorical Variables:** Apply encoding techniques suited to the cardinality and nature of categorical variables, such as ordinal encoding, one-hot encoding, or frequency encoding.  \n",
    "- **Tailored Feature Preparation:** Prepare separate datasets for each level of hierarchical classification, ensuring optimal feature sets for Level 1 (binary classification) and Level 2 (binary and multi-class classification).  \n",
    "- **Output:** Save the feature-engineered datasets (in CSV or Pickle format) for modeling in subsequent notebooks.  \n",
    "\n",
    "This notebook ensures the dataset is optimally prepared for hierarchical classification, balancing feature relevance and computational efficiency.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"toc\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "* [1. Import the Libraries](#chapter1)\n",
    "* [2. Import the Datasets](#chapter2)       \n",
    "* [3. Feature Engineering](#chapter3)\n",
    "    * [3.1. Carrier-District Interaction](#section_3_1)\n",
    "    * [3.2. Income Category](#section_3_2)\n",
    "    * [3.3. Days_To_First_Hearing](#section_3_3)\n",
    "    * [3.4. Accident Quarter](#section_3_4)\n",
    "    * [3.5. Accident Year](#section_3_5)\n",
    "    * [3.6. Accident on Day and Weekend](#section_3_6)\n",
    "    * [3.7. Age Group](#section_3_7)\n",
    "    * [3.8. Time from Assembly Date to C-2 Filing](#section_3_8)\n",
    "    * [3.9. Time from Accident to C-2 Filing](#section_3_9)\n",
    "    * [3.10. Zip_Code_Simplified](#section_3_10)\n",
    "    * [3.11. Carrier Type Merged](#section_3_11)\n",
    "    * [3.12. Carrier_Name_Simplified](#section_3_12)\n",
    "    * [3.13. Body_Part_Category](#section_3_13)\n",
    "    * [3.14. Injury_Nature_Category](#section_3_14)\n",
    "    * [3.15. Injury_Cause_Category](#section_3_15)\n",
    "    * [3.16. Risk of Each Job](#section_3_16)\n",
    "    * [3.17. Relation between Salary and Dependents](#section_3_17)\n",
    "* [4. Encoding](#chapter4)\n",
    "* [5. Save Dataset for Modelling](#chapter5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# 1. Import the Libraries üìö<a class=\"anchor\" id=\"chapter1\"></a>\n",
    "\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "In this section we will imported the needed libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import zipfile\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "# --- Scikit-Learn Modules for Data Partitioning and Preprocessing ---\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Warnings ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Import functions from utils\n",
    "# from utils import analyze_numerical_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and Prepare Datasets üìÅ<a class=\"anchor\" id=\"chapter2\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "Now, we will load the dataset prepared in **Notebook 2: Preprocessing & Cleaning**, where we addressed key inconsistencies such as missing values and outliers. This preprocessed dataset serves as the foundation for the feature engineering steps in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets saved from Notebook 2\n",
    "X_train = pd.read_csv(\"/mnt/data/X_train_cleaned.csv\", index_col=\"Claim Identifier\")\n",
    "X_val = pd.read_csv(\"/mnt/data/X_val_cleaned.csv\", index_col=\"Claim Identifier\")\n",
    "df_test = pd.read_csv(\"/mnt/data/df_test_cleaned.csv\", index_col=\"Claim Identifier\")\n",
    "y_train = pd.read_csv(\"/mnt/data/y_train_cleaned.csv\", index_col=\"Claim Identifier\")\n",
    "y_val = pd.read_csv(\"/mnt/data/y_val_cleaned.csv\", index_col=\"Claim Identifier\")\n",
    "\n",
    "# Verify the datasets are loaded successfully\n",
    "X_train.head(), X_val.head(), df_test.head(), y_train.head(), y_val.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "\n",
    "Feature engineering is the process of preparing data for machine learning models by transforming raw data into meaningful features that enhance model performance. In this section, we create, select, and modify variables to capture significant patterns within the data, making it more informative and useful for the model‚Äôs learning process. Through these transformations, we aim to improve the model‚Äôs accuracy and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Carrier-District Interaction <a class=\"anchor\" id=\"section_3_1\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "Combining **Carrier Type** with **District Name** may reveal regional preferences for certain insurance carriers, which could be useful in understanding regional biases or regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature by combining Carrier Type and District Name\n",
    "X_train['Carrier_District_Interaction'] = X_train['Carrier Type'] + \"_\" + X_train['District Name']\n",
    "\n",
    "# Apply to the val X_val\n",
    "X_val['Carrier_District_Interaction'] = X_val['Carrier Type'] + \"_\" + X_val['District Name']\n",
    "\n",
    "# Apply to the test set\n",
    "df_test['Carrier_District_Interaction'] = df_test['Carrier Type'] + \"_\" + df_test['District Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Income Category  <a class=\"anchor\" id=\"section_3_2\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "Creating categories for **Average Weekly Wage** can simplify the continuous nature of income into meaningful segments such as Low, Average, and High, which could help the model understand different socioeconomic statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25     858.0\n",
      "0.50    1198.0\n",
      "0.75    1506.0\n",
      "0.90    1683.0\n",
      "Name: Average Weekly Wage, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate key percentiles\n",
    "percentiles = X_train['Average Weekly Wage'].quantile([0.25, 0.5, 0.75, 0.9])\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the bins and labels for categorizing income based on percentiles\n",
    "income_bins = [0, 764.0, 1056.0, 1455.0, 1895.0, float('inf')]  # float('inf') allows us to set an open-ended range\n",
    "income_labels = ['Low Income', 'Lower-Middle Income', 'Middle Income', 'Upper-Middle Income', 'High Income']\n",
    "\n",
    "# Creating the new feature for income categories for the train set\n",
    "X_train['Income_Category'] = pd.cut(X_train['Average Weekly Wage'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val['Income_Category'] = pd.cut(X_val['Average Weekly Wage'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test['Income_Category'] = pd.cut(df_test['Average Weekly Wage'], bins=income_bins, labels=income_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating this categorical feature, we drop the original Average Weekly Wage column since it‚Äôs now represented by Income_Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the 'Average Weekly Wage' column as it's represented by 'Income_Category'\n",
    "# X_train_processed = X_train_processed.drop(columns=['Average Weekly Wage'])\n",
    "# X_val_processed = X_val_processed.drop(columns=['Average Weekly Wage'])\n",
    "# df_test_processed = df_test_processed.drop(columns=['Average Weekly Wage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Days_To_First_Hearing  <a class=\"anchor\" id=\"section_3_3\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "The feature **Days_To_First_Hearing** was created to capture the number of days between the Accident Date and the First Hearing Date. If a First Hearing Date is available, the feature represents the time elapsed, which can help the model understand the speed of the claim process. If the First Hearing Date is missing, it is represented as 0, indicating that a hearing has not occurred yet. This approach provides more nuanced information than simply indicating whether the hearing occurred or not, allowing the model to learn from both the presence and timing of the first hearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Hearing Date column after conversion:\n",
      "0          NaT\n",
      "1   2022-08-29\n",
      "2   2023-03-24\n",
      "3   2022-11-14\n",
      "4          NaT\n",
      "Name: First Hearing Date, dtype: datetime64[ns]\n",
      "C-2 Date column after conversion:\n",
      "0   2020-05-08\n",
      "1   2022-07-14\n",
      "2   2021-11-08\n",
      "3   2022-02-04\n",
      "4   2021-10-29\n",
      "Name: C-2 Date, dtype: datetime64[ns]\n",
      "C-3 Date column after conversion:\n",
      "0          NaT\n",
      "1   2022-06-16\n",
      "2   2021-11-04\n",
      "3          NaT\n",
      "4          NaT\n",
      "Name: C-3 Date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# List of columns to convert to datetime\n",
    "date_columns = ['First Hearing Date', 'C-2 Date', 'C-3 Date']\n",
    "\n",
    "# Replace 0 with NaT and convert to datetime\n",
    "for col in date_columns:\n",
    "    if col in X_train.columns:  # Check if the column exists in the DataFrame\n",
    "        X_train[col] = pd.to_datetime(X_train[col].replace(0, pd.NaT), errors='coerce')\n",
    "    if col in X_val.columns:\n",
    "        X_val[col] = pd.to_datetime(X_val[col].replace(0, pd.NaT), errors='coerce')\n",
    "    if col in df_test.columns:\n",
    "        df_test[col] = pd.to_datetime(df_test[col].replace(0, pd.NaT), errors='coerce')\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"First Hearing Date column after conversion:\")\n",
    "print(X_train['First Hearing Date'].head())\n",
    "print(\"C-2 Date column after conversion:\")\n",
    "print(X_train['C-2 Date'].head())\n",
    "print(\"C-3 Date column after conversion:\")\n",
    "print(X_train['C-3 Date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days_To_First_Hearing in Train Set:\n",
      "count    401156.000000\n",
      "mean         87.779585\n",
      "std         265.765399\n",
      "min        -429.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%          71.000000\n",
      "max       16373.000000\n",
      "Name: Days_To_First_Hearing, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Convert date columns to datetime\n",
    "date_columns = ['First Hearing Date', 'Accident Date']\n",
    "for col in date_columns:\n",
    "    X_train[col] = pd.to_datetime(X_train[col], errors='coerce')\n",
    "    X_val[col] = pd.to_datetime(X_val[col], errors='coerce')\n",
    "    df_test[col] = pd.to_datetime(df_test[col], errors='coerce')\n",
    "\n",
    "# Function to calculate days to first hearing\n",
    "def calculate_hearing_days(row):\n",
    "    if pd.notna(row['First Hearing Date']) and pd.notna(row['Accident Date']):\n",
    "        return (row['First Hearing Date'] - row['Accident Date']).days\n",
    "    return 0  # If no hearing date exists, return 0\n",
    "\n",
    "# Apply the function to create the new feature\n",
    "X_train['Days_To_First_Hearing'] = X_train.apply(calculate_hearing_days, axis=1)\n",
    "X_val['Days_To_First_Hearing'] = X_val.apply(calculate_hearing_days, axis=1)\n",
    "df_test['Days_To_First_Hearing'] = df_test.apply(calculate_hearing_days, axis=1)\n",
    "\n",
    "# Verify the new feature\n",
    "print(\"Days_To_First_Hearing in Train Set:\")\n",
    "print(X_train['Days_To_First_Hearing'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating this binary feature, we can drop the original First Hearing Date column from the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop First Hearing Date from the train, val, and test sets\n",
    "X_train_processed = X_train_processed.drop(columns=['First Hearing Date'])\n",
    "X_val_processed = X_val_processed.drop(columns=['First Hearing Date'])\n",
    "df_test_processed = df_test_processed.drop(columns=['First Hearing Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Accident Quarter  <a class=\"anchor\" id=\"section_3_4\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "Temporal data can often influence outcomes. Extracting the quarter of the accident (e.g., 1st, 2nd, etc.) helps the model capture seasonal patterns that may impact accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the quarter of the Accident Date\n",
    "X_train_processed['Accident_Quarter'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce').dt.quarter\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Accident_Quarter'] = pd.to_datetime(X_val_processed['Accident Date'], errors='coerce').dt.quarter\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Accident_Quarter'] = pd.to_datetime(df_test_processed['Accident Date'], errors='coerce').dt.quarter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Accident Year <a class=\"anchor\" id=\"section_3_5\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "The year can help the model understand seasonal or yearly effects, like accident patterns during different times of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the year from the Accident Date\n",
    "X_train_processed['Accident_Year'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce').dt.year\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Accident_Year'] = pd.to_datetime(X_val_processed['Accident Date'], errors='coerce').dt.year\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Accident_Year'] = pd.to_datetime(df_test_processed['Accident Date'], errors='coerce').dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Accident on Day and Weekend <a class=\"anchor\" id=\"section_3_6\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "The day of the accident could be significant, as weekends might have different risk factors compared to weekdays. We will extract the day of the week and create a feature to indicate if the accident occurred on a weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the day of the week and creating a feature to indicate if the accident occurred on a weekend\n",
    "X_train_processed['Accident Day'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce').dt.dayofweek\n",
    "X_train_processed['Accident on Weekend'] = X_train_processed['Accident Day'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Accident Day'] = pd.to_datetime(X_val_processed['Accident Date'], errors='coerce').dt.dayofweek\n",
    "X_val_processed['Accident on Weekend'] = X_val_processed['Accident Day'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Accident Day'] = pd.to_datetime(df_test_processed['Accident Date'], errors='coerce').dt.dayofweek\n",
    "df_test_processed['Accident on Weekend'] = df_test_processed['Accident Day'].apply(lambda x: 1 if x >= 5 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Age Group <a class=\"anchor\" id=\"section_3_7\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "Grouping ages can help simplify the model‚Äôs understanding of different age demographics (e.g., Youth, Young Adult, Middle Age, Senior). This could potentially improve model interpretability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    362830.000000\n",
       "mean         42.961731\n",
       "std          13.621215\n",
       "min           5.000000\n",
       "25%          31.000000\n",
       "50%          43.000000\n",
       "75%          54.000000\n",
       "max          82.000000\n",
       "Name: Age at Injury, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display unique values in 'Age at Injury' to understand the range\n",
    "X_train_processed['Age at Injury'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins and labels for age groups\n",
    "age_bins = [0, 25, 45, 65, float('inf')]\n",
    "age_labels = ['Youth', 'Young Adult', 'Middle Age', 'Senior']\n",
    "\n",
    "# Creating a new feature for age groups\n",
    "X_train_processed['Age Group'] = pd.cut(X_train_processed['Age at Injury'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Age Group'] = pd.cut(X_val_processed['Age at Injury'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Age Group'] = pd.cut(df_test_processed['Age at Injury'], bins=age_bins, labels=age_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop 'Age at Injury' from the train, val and test set\n",
    "# X_train_processed = X_train_processed.drop(columns=['Age at Injury'])\n",
    "# X_val_processed = X_val_processed.drop(columns=['Age at Injury'])\n",
    "# df_test_processed = df_test_processed.drop(columns=['Age at Injury'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Promptness_category <a class=\"anchor\" id=\"section_3_8\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "The `promptness_category` feature categorizes the time taken between key events in the claims process, specifically measuring the difference between the `Accident Date` and the `Assembly Date`. This feature quantifies the speed or delay in assembling the claim and provides insight into how promptly claims are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_promptness(df, date1_col, date2_col, new_col_name):\n",
    "    \"\"\"\n",
    "    Calculate and categorize promptness between two date columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to process.\n",
    "    - date1_col: The column representing the first date (e.g., Assembly Date).\n",
    "    - date2_col: The column representing the second date (e.g., Accident Date).\n",
    "    - new_col_name: The name of the new categorical column for promptness.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with new categorized promptness column.\n",
    "    \"\"\"\n",
    "    # Ensure the date columns are datetime\n",
    "    df[date1_col] = pd.to_datetime(df[date1_col], errors='coerce')\n",
    "    df[date2_col] = pd.to_datetime(df[date2_col], errors='coerce')\n",
    "\n",
    "    # Calculate the difference in days\n",
    "    df['Days_Difference'] = (df[date1_col] - df[date2_col]).dt.days\n",
    "\n",
    "    # Assign categories based on conditions\n",
    "    def assign_category(row):\n",
    "        if pd.isna(row[date1_col]) or row['Days_Difference'] <= 0:\n",
    "            return 'Form Not Received'\n",
    "        elif row['Days_Difference'] <= 7:\n",
    "            return 'Until 1 week'\n",
    "        elif row['Days_Difference'] <= 14:\n",
    "            return 'Between 1 and 2 weeks'\n",
    "        elif row['Days_Difference'] <= 30:\n",
    "            return 'Between 2 weeks and 1 month'\n",
    "        elif row['Days_Difference'] <= 90:\n",
    "            return '1 to 3 months'\n",
    "        elif row['Days_Difference'] <= 180:\n",
    "            return '3 to 6 months'\n",
    "        elif row['Days_Difference'] <= 365:\n",
    "            return '6 months to 1 year'\n",
    "        else:\n",
    "            return 'More than 1 year'\n",
    "\n",
    "    # Apply the function to assign categories\n",
    "    df[new_col_name] = df.apply(assign_category, axis=1)\n",
    "\n",
    "    # Drop the intermediate column\n",
    "    df.drop(columns=['Days_Difference'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to X_train_processed\n",
    "X_train_processed = categorize_promptness(X_train_processed, 'Assembly Date', 'Accident Date', 'promptness_category')\n",
    "\n",
    "# Apply the function to X_val_processed\n",
    "X_val_processed = categorize_promptness(X_val_processed, 'Assembly Date', 'Accident Date', 'promptness_category')\n",
    "\n",
    "# Apply the function to df_test_processed\n",
    "df_test_processed = categorize_promptness(df_test_processed, 'Assembly Date', 'Accident Date', 'promptness_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "promptness_category\n",
       "Until 1 week                   159623\n",
       "Between 1 and 2 weeks           85224\n",
       "Between 2 weeks and 1 month     71377\n",
       "1 to 3 months                   50045\n",
       "3 to 6 months                   15345\n",
       "More than 1 year                 9990\n",
       "6 months to 1 year               9547\n",
       "Form Not Received                   5\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display value counts for the new column\n",
    "X_train_processed['promptness_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These categories allow us to observe the promptness in claim processing, with the majority falling within Until 1 week, indicating a generally swift assembly of claims. However, a significant portion extends beyond a month, with a small subset taking more than a year. This feature can provide insights into patterns of delays or rapid processing, possibly indicating areas for improvement in claim management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9. promptness_C2_category <a class=\"anchor\" id=\"section_3_9\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "The \"promptness_C2_category\" feature tracks the time taken to register the C-2 Date (the receipt of the employer's report of work-related injury/illness) after the Accident Date. It evaluates employers' promptness in reporting accidents, offering insights into compliance and potential administrative delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where 'C-2 Date' is earlier than 'Accident Date': 627\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'C-2 Date' and 'Accident Date' are datetime\n",
    "X_train_processed['C-2 Date'] = pd.to_datetime(X_train_processed['C-2 Date'], errors='coerce')\n",
    "X_train_processed['Accident Date'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce')\n",
    "\n",
    "# Count the number of rows where 'C-2 Date' is earlier than 'Accident Date'\n",
    "num_negative_values = (X_train_processed['C-2 Date'] < X_train_processed['Accident Date']).sum()\n",
    "\n",
    "# Print the number of rows with this condition\n",
    "print(f\"Number of rows where 'C-2 Date' is earlier than 'Accident Date': {num_negative_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##this isnt supposed well :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to X_train_processed\n",
    "X_train_processed = categorize_promptness(X_train_processed, 'C-2 Date', 'Accident Date', 'promptness_C2_category')\n",
    "\n",
    "# Apply the function to X_val_processed\n",
    "X_val_processed = categorize_promptness(X_val_processed, 'C-2 Date', 'Accident Date', 'promptness_C2_category')\n",
    "\n",
    "# Apply the function to df_test_processed\n",
    "df_test_processed = categorize_promptness(df_test_processed, 'C-2 Date', 'Accident Date', 'promptness_C2_category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "promptness_C2_category\n",
       "Until 1 week                   160468\n",
       "Between 1 and 2 weeks           76055\n",
       "Between 2 weeks and 1 month     60347\n",
       "1 to 3 months                   49633\n",
       "3 to 6 months                   18602\n",
       "Form Not Received               13641\n",
       "6 months to 1 year              12550\n",
       "More than 1 year                 9860\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display value counts for the new column\n",
    "X_train_processed['promptness_C2_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. promptness_C3_category <a class=\"anchor\" id=\"section_3_10\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "The \"promptness_C3_category\" feature tracks the time taken to register the C-3 Date (the receipt of the employer's report of work-related injury/illness) after the Accident Date. It evaluates employers' promptness in reporting accidents, offering insights into compliance and potential administrative delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to X_train_processed\n",
    "X_train_processed = categorize_promptness(X_train_processed, 'C-3 Date', 'Accident Date', 'promptness_C3_category')\n",
    "\n",
    "# Apply the function to X_val_processed\n",
    "X_val_processed = categorize_promptness(X_val_processed, 'C-3 Date', 'Accident Date', 'promptness_C3_category')\n",
    "\n",
    "# Apply the function to df_test_processed\n",
    "df_test_processed = categorize_promptness(df_test_processed, 'C-3 Date', 'Accident Date', 'promptness_C3_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "promptness_C3_category\n",
       "Form Not Received              273433\n",
       "1 to 3 months                   31694\n",
       "Between 2 weeks and 1 month     30784\n",
       "Between 1 and 2 weeks           19363\n",
       "Until 1 week                    18328\n",
       "3 to 6 months                   12393\n",
       "6 months to 1 year               8614\n",
       "More than 1 year                 6547\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display value counts for the new column\n",
    "X_train_processed['promptness_C3_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating new features based on the existing date columns, we will remove the original date features to avoid redundancy and simplify the dataset. We believe that the impact of these date features is adequately captured in the newly engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = X_train_processed.drop(columns=['Accident Date', 'Assembly Date', 'C-2 Date','C-3 Date'])\n",
    "\n",
    "#Apply to the val set\n",
    "X_val_processed = X_val_processed.drop(columns=['Accident Date','Assembly Date', 'C-2 Date','C-3 Date'])\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed = df_test_processed.drop(columns=['Accident Date', 'Assembly Date', 'C-2 Date', 'C-3 Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. Zip_Code_Simplified <a class=\"anchor\" id=\"section_3_10\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "To reduce the dimensionality of the Zip Code feature, we will create a new feature called Zip_Code_Simplified. This feature will group all zip codes that appear less than 2,000 times in the training dataset into a category labeled as 'Other'. By doing this, we effectively reduce the number of unique zip codes, simplifying the model while retaining the most significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent Zip Codes with their counts:\n",
      "Zip Code\n",
      "11236.0    3836\n",
      "11717.0    3779\n",
      "11434.0    3636\n",
      "11550.0    2849\n",
      "10467.0    2794\n",
      "10940.0    2335\n",
      "10701.0    2176\n",
      "10029.0    2089\n",
      "14150.0    2005\n",
      "10314.0    1751\n",
      "14609.0    1703\n",
      "11706.0    1629\n",
      "11207.0    1551\n",
      "12601.0    1541\n",
      "11368.0    1541\n",
      "11212.0    1523\n",
      "11208.0    1519\n",
      "12550.0    1519\n",
      "11226.0    1450\n",
      "11234.0    1365\n",
      "11203.0    1364\n",
      "10466.0    1347\n",
      "13440.0    1285\n",
      "11385.0    1280\n",
      "10462.0    1279\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the most frequent Carrier Names along with their counts\n",
    "most_frequent_zipcode = X_train_processed['Zip Code'].value_counts().head(25)  # Adjust the number if you need more\n",
    "print(\"Most frequent Zip Codes with their counts:\")\n",
    "print(most_frequent_zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of 'Zip_Code_Simplified' feature in X_train_processed:\n",
      "Zip_Code_Simplified\n",
      "OTHER      324113\n",
      "11236.0      3836\n",
      "11717.0      3779\n",
      "11434.0      3636\n",
      "11550.0      2849\n",
      "10467.0      2794\n",
      "10940.0      2335\n",
      "10701.0      2176\n",
      "10029.0      2089\n",
      "14150.0      2005\n",
      "10314.0      1751\n",
      "14609.0      1703\n",
      "11706.0      1629\n",
      "11207.0      1551\n",
      "11368.0      1541\n",
      "12601.0      1541\n",
      "11212.0      1523\n",
      "12550.0      1519\n",
      "11208.0      1519\n",
      "11226.0      1450\n",
      "11234.0      1365\n",
      "11203.0      1364\n",
      "10466.0      1347\n",
      "13440.0      1285\n",
      "11385.0      1280\n",
      "10462.0      1279\n",
      "10456.0      1246\n",
      "14094.0      1237\n",
      "10469.0      1208\n",
      "11003.0      1179\n",
      "12180.0      1172\n",
      "11757.0      1146\n",
      "10977.0      1143\n",
      "11413.0      1142\n",
      "11758.0      1137\n",
      "11520.0      1121\n",
      "13090.0      1120\n",
      "11746.0      1099\n",
      "10473.0      1098\n",
      "11221.0      1096\n",
      "11722.0      1095\n",
      "11233.0      1093\n",
      "10453.0      1091\n",
      "11704.0      1091\n",
      "11772.0      1083\n",
      "12303.0      1077\n",
      "12603.0      1071\n",
      "11373.0      1066\n",
      "13021.0      1039\n",
      "11756.0      1038\n",
      "11377.0      1009\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a new feature called 'Zip_Code_Simplified' based on 'Zip Code' for train, validation, and test sets\n",
    "X_train_processed['Zip_Code_Simplified'] = X_train_processed['Zip Code']\n",
    "X_val_processed['Zip_Code_Simplified'] = X_val_processed['Zip Code']\n",
    "df_test_processed['Zip_Code_Simplified'] = df_test_processed['Zip Code']\n",
    "\n",
    "# Identify carrier names that occur fewer than 1000 times in X_train_processed\n",
    "zipcode_counts = X_train_processed['Zip Code'].value_counts()\n",
    "zipcode_to_replace = zipcode_counts[zipcode_counts < 1000].index\n",
    "\n",
    "# Replace carrier names with fewer than 1000 occurrences with 'OTHER' in all datasets using the identified carriers from X_train\n",
    "for dataset in [X_train_processed, X_val_processed, df_test_processed]:\n",
    "    dataset['Zip_Code_Simplified'] = dataset['Zip_Code_Simplified'].replace(zipcode_to_replace, 'OTHER')\n",
    "\n",
    "# Print the counts of the simplified carrier names in X_train_processed to verify the result\n",
    "print(\"Counts of 'Zip_Code_Simplified' feature in X_train_processed:\")\n",
    "print(X_train_processed['Zip_Code_Simplified'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ZIP Code uniqueness: 9705\n",
      "Simplified ZIP Code uniqueness: 51\n"
     ]
    }
   ],
   "source": [
    "# Display unique counts to compare the dimensionality reduction\n",
    "print(f\"Original ZIP Code uniqueness: {X_train_processed['Zip Code'].nunique()}\")\n",
    "print(f\"Simplified ZIP Code uniqueness: {X_train_processed['Zip_Code_Simplified'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation retains regional information while reducing the feature dimensionality, which can be beneficial for model interpretability and efficiency. The original Zip Code column has been removed to avoid redundancy. For this motive we will delete also the Zip Code, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_processed = X_train_processed.drop(columns=['Zip Code'])\n",
    "# # Apply to the val set\n",
    "# X_val_processed = X_val_processed.drop(columns=['Zip Code'])\n",
    "# # Apply to the test set\n",
    "# df_test_processed = df_test_processed.drop(columns=['Zip Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11. Carrier Type Merged <a class=\"anchor\" id=\"section_3_11\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "Since there are several categories under \"Special Fund\" with very few occurrences, combining them into a single category can reduce noise in the data and make the feature more manageable for the model.\n",
    "\n",
    "After merging, we observe the following distribution of Carrier Type Merged values in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrier Type Merged\n",
      "PRIVATE         199662\n",
      "SELF PUBLIC      85264\n",
      "SIF              77456\n",
      "SELF PRIVATE     36835\n",
      "UNKNOWN           1212\n",
      "SPECIAL FUND       727\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a new feature that merges all 'Special Fund' categories into a single category for train, validation, and test sets\n",
    "for dataset in [X_train_processed, X_val_processed, df_test_processed]:\n",
    "    dataset['Carrier Type Merged'] = dataset['Carrier Type'].replace({\n",
    "        'SPECIAL FUND - UNKNOWN': 'SPECIAL FUND',\n",
    "        'SPECIAL FUND - POI CARRIER WCB MENANDS': 'SPECIAL FUND',\n",
    "        'SPECIAL FUND - CONS. COMM. (SECT. 25-A)': 'SPECIAL FUND'\n",
    "    })\n",
    "\n",
    "# Verifying the updated column for X_train_processed\n",
    "print(X_train_processed['Carrier Type Merged'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now let's delete Carrier Type from the train, val and test set\n",
    "# X_train_processed = X_train_processed.drop(columns=['Carrier Type'])\n",
    "# # Apply to the val set\n",
    "# X_val_processed = X_val_processed.drop(columns=['Carrier Type'])\n",
    "# # Apply to the test set\n",
    "# df_test_processed = df_test_processed.drop(columns=['Carrier Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12. Carrier_Name_Simplified <a class=\"anchor\" id=\"section_3_12\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "The 'Carrier Name' feature has high cardinality, with 1951 unique values. This level of uniqueness can complicate machine learning models, especially if some categories have very few instances. To simplify the analysis and potentially improve model performance, we will group carrier names with fewer than 500 occurrences under a single category called 'OTHER'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent Carrier Names with their counts:\n",
      "Carrier Name\n",
      "STATE INSURANCE FUND             77456\n",
      "POLICE, FIRE, SANITATION         15026\n",
      "AMERICAN ZURICH INSURANCE CO     12178\n",
      "CHARTER OAK FIRE INS CO          12021\n",
      "INDEMNITY INS. OF N AMERICA      10095\n",
      "SAFETY NATIONAL CASUALTY CORP     9778\n",
      "NEW HAMPSHIRE INSURANCE CO        8962\n",
      "LM INSURANCE CORP                 8557\n",
      "A I U INSURANCE COMPANY           7761\n",
      "INDEMNITY INSURANCE CO OF         6369\n",
      "NYC TRANSIT AUTHORITY             5826\n",
      "HARTFORD ACCIDENT & INDEMNITY     5311\n",
      "NEW YORK BLACK CAR OPERATORS'     5080\n",
      "ARCH INDEMNITY INSURANCE CO.      4652\n",
      "AIU INSURANCE CO                  4507\n",
      "CNY OTHER THAN ED, HED WATER      4454\n",
      "HEALTH & HOSPITAL CORP.           3914\n",
      "ARCH INDEMNITY INSURANCE CO       3731\n",
      "PENNSYLVANIA MANUFACTURERS'       3396\n",
      "PUBLIC EMPLOYERS RISK MGMT.       3231\n",
      "ACE AMERICAN INSURANCE CO.        3171\n",
      "OLD REPUBLIC INSURANCE CO.        3046\n",
      "MEMIC INDEMNITY COMPANY           2908\n",
      "WAL-MART ASSOCIATES, INC.         2764\n",
      "COUNTY OF NASSAU                  2678\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the most frequent Carrier Names along with their counts\n",
    "most_frequent_carriers = X_train_processed['Carrier Name'].value_counts().head(25)  # Adjust the number if you need more\n",
    "print(\"Most frequent Carrier Names with their counts:\")\n",
    "print(most_frequent_carriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of 'Carrier_Name_Simplified' feature in X_train_processed:\n",
      "Carrier_Name_Simplified\n",
      "OTHER                             85446\n",
      "STATE INSURANCE FUND              77456\n",
      "POLICE, FIRE, SANITATION          15026\n",
      "AMERICAN ZURICH INSURANCE CO      12178\n",
      "CHARTER OAK FIRE INS CO           12021\n",
      "                                  ...  \n",
      "EVEREST NATIONAL INS COMPANY        519\n",
      "TRAVELERS INDEMNITY CO OF AMER      517\n",
      "VISITING NURSE SERVICE OF NY        513\n",
      "HARTFORD INSURANCE COMPANY          508\n",
      "HARTFORD CASUALTY INSURANCE CO      501\n",
      "Name: count, Length: 106, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a new feature called 'Carrier_Name_Simplified' based on 'Carrier Name' for train, validation, and test sets\n",
    "X_train_processed['Carrier_Name_Simplified'] = X_train_processed['Carrier Name']\n",
    "X_val_processed['Carrier_Name_Simplified'] = X_val_processed['Carrier Name']\n",
    "df_test_processed['Carrier_Name_Simplified'] = df_test_processed['Carrier Name']\n",
    "\n",
    "# Identify carrier names that occur fewer than 500 times in X_train_processed\n",
    "carrier_counts = X_train_processed['Carrier Name'].value_counts()\n",
    "carriers_to_replace = carrier_counts[carrier_counts < 500].index\n",
    "\n",
    "# Replace carrier names with fewer than 500 occurrences with 'OTHER' in all datasets using the identified carriers from X_train\n",
    "for dataset in [X_train_processed, X_val_processed, df_test_processed]:\n",
    "    dataset['Carrier_Name_Simplified'] = dataset['Carrier_Name_Simplified'].replace(carriers_to_replace, 'OTHER')\n",
    "\n",
    "# Print the counts of the simplified carrier names in X_train_processed to verify the result\n",
    "print(\"Counts of 'Carrier_Name_Simplified' feature in X_train_processed:\")\n",
    "print(X_train_processed['Carrier_Name_Simplified'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'Carrier Name': 1966\n",
      "Number of unique values in 'Carrier_Name_Simplified': 106\n"
     ]
    }
   ],
   "source": [
    "#print the number of unique values in the original 'Carrier Name' feature\n",
    "print(f\"Number of unique values in 'Carrier Name': {X_train_processed['Carrier Name'].nunique()}\")\n",
    "\n",
    "#print the number of unique values in the simplified 'Carrier_Name_Simplified' feature\n",
    "print(f\"Number of unique values in 'Carrier_Name_Simplified': {X_train_processed['Carrier_Name_Simplified'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the 'Carrier Name' column after creating 'Carrier_Name_Simplified'\n",
    "# X_train_processed = X_train_processed.drop(columns=['Carrier Name'])\n",
    "\n",
    "# # Apply to the val set\n",
    "# X_val_processed = X_val_processed.drop(columns=['Carrier Name'])\n",
    "\n",
    "# # Apply to the test set\n",
    "# df_test_processed = df_test_processed.drop(columns=['Carrier Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133752, 58457, 143202)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sum all nan values from train, val, test\n",
    "X_train_processed.isnull().sum().sum(), X_val_processed.isnull().sum().sum(), df_test_processed.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13. Body_Part_Category <a class=\"anchor\" id=\"section_3_13\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "The Body_Part_Category feature will group the WCIO_Part_of_Body_Code into broader categories. Based on the codes in your document, each range of codes represents a specific body part region (e.g., codes from 10 to 19 represent the head). We‚Äôll map these codes to corresponding regions like ‚ÄúHead,‚Äù ‚ÄúNeck,‚Äù etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of WCIO Part of Body codes to broader categories\n",
    "part_of_body_mapping = {\n",
    "    **dict.fromkeys(range(10, 20), 'Head'),\n",
    "    **dict.fromkeys(range(20, 30), 'Neck'),\n",
    "    **dict.fromkeys(range(30, 40), 'Upper Extremities'),\n",
    "    **dict.fromkeys(list(range(40, 50)) + list(range(60, 64)), 'Trunk'),\n",
    "    **dict.fromkeys(range(50, 60), 'Lower Extremities'),\n",
    "    **dict.fromkeys([64, 65, 66, 90, 91, 99], 'Multiple Body Parts'),\n",
    "    **dict.fromkeys([101], 'NonClassificable')\n",
    "\n",
    "}\n",
    "\n",
    "# Creating the Body_Part_Category column by mapping Part of Body codes to categories\n",
    "X_train_processed['Body_Part_Category'] = X_train_processed['WCIO Part Of Body Code'].map(part_of_body_mapping)\n",
    "\n",
    "#Apply to the val set\n",
    "X_val_processed['Body_Part_Category'] = X_train_processed['WCIO Part Of Body Code'].map(part_of_body_mapping)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Body_Part_Category'] = df_test_processed['WCIO Part Of Body Code'].map(part_of_body_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Body_Part_Category\n",
       "Upper Extremities      124656\n",
       "Lower Extremities       84322\n",
       "Trunk                   70778\n",
       "Head                    39893\n",
       "Multiple Body Parts     32530\n",
       "NonClassificable        29322\n",
       "Neck                     8398\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_processed['Body_Part_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in 'Body_Part_Category': 11257\n",
      "WCIO Part Of Body Code distribution for missing 'Body_Part_Category':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WCIO Part Of Body Code\n",
       "0.0    11257\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the number of missing values in 'Body_Part_Category'\n",
    "missing_body_part_count = X_train_processed['Body_Part_Category'].isna().sum()\n",
    "print(f\"Number of missing values in 'Body_Part_Category': {missing_body_part_count}\")\n",
    "\n",
    "# Filter rows where 'Body_Part_Category' is missing\n",
    "missing_body_part_rows = X_train_processed[X_train_processed['Body_Part_Category'].isna()]\n",
    "\n",
    "# Investigate the corresponding 'WCIO Part Of Body Code' values\n",
    "missing_body_part_wcio_codes = missing_body_part_rows['WCIO Part Of Body Code'].value_counts(dropna=False)\n",
    "print(\"WCIO Part Of Body Code distribution for missing 'Body_Part_Category':\")\n",
    "missing_body_part_wcio_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11257"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_processed['Body_Part_Category'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14. Injury_Nature_Category <a class=\"anchor\" id=\"section_3_14\"></a>\n",
    "The Body_Part_Category feature will group the WCIO_Part_of_Body_Code into broader categories. Based on the codes in the document, each range of codes represents a specific body part region (e.g., codes from 10 to 19 represent the head). We will map these codes to corresponding regions like \"Head,\" \"Neck,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 7.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 16.0,\n",
       " 19.0,\n",
       " 22.0,\n",
       " 25.0,\n",
       " 28.0,\n",
       " 30.0,\n",
       " 31.0,\n",
       " 32.0,\n",
       " 34.0,\n",
       " 36.0,\n",
       " 37.0,\n",
       " 38.0,\n",
       " 40.0,\n",
       " 41.0,\n",
       " 42.0,\n",
       " 43.0,\n",
       " 46.0,\n",
       " 47.0,\n",
       " 49.0,\n",
       " 52.0,\n",
       " 53.0,\n",
       " 54.0,\n",
       " 55.0,\n",
       " 58.0,\n",
       " 59.0,\n",
       " 60.0,\n",
       " 61.0,\n",
       " 62.0,\n",
       " 63.0,\n",
       " 64.0,\n",
       " 65.0,\n",
       " 66.0,\n",
       " 67.0,\n",
       " 68.0,\n",
       " 69.0,\n",
       " 70.0,\n",
       " 71.0,\n",
       " 72.0,\n",
       " 73.0,\n",
       " 74.0,\n",
       " 75.0,\n",
       " 76.0,\n",
       " 77.0,\n",
       " 78.0,\n",
       " 79.0,\n",
       " 80.0,\n",
       " 83.0,\n",
       " 90.0,\n",
       " 91.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted(X_train_processed['WCIO Nature of Injury Code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of WCIO Nature of Injury codes to broader categories\n",
    "nature_of_injury_mapping = {\n",
    "    **dict.fromkeys(range(1, 60), 'Specific Injury'),\n",
    "    **dict.fromkeys(range(60, 81), 'Occupational Disease or Cumulative Injury'),\n",
    "    **dict.fromkeys([83], 'COVID-19 Injury'),\n",
    "    **dict.fromkeys([90, 91], 'Multiple Injuries')\n",
    "}\n",
    "\n",
    "# Creating the Injury_Nature_Category column by mapping Nature of Injury codes to categories\n",
    "X_train_processed['Injury_Nature_Category'] = X_train_processed['WCIO Nature of Injury Code'].map(nature_of_injury_mapping)\n",
    "\n",
    "#Apply to the val set\n",
    "X_val_processed['Injury_Nature_Category'] = X_val_processed['WCIO Nature of Injury Code'].map(nature_of_injury_mapping)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Injury_Nature_Category'] = df_test_processed['WCIO Nature of Injury Code'].map(nature_of_injury_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Injury_Nature_Category\n",
       "Specific Injury                              353666\n",
       "COVID-19 Injury                               18028\n",
       "Occupational Disease or Cumulative Injury      9966\n",
       "Multiple Injuries                              9202\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_processed['Injury_Nature_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_processed['Injury_Nature_Category'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.15. Injury_Cause_Category <a class=\"anchor\" id=\"section_3_15\"></a>\n",
    "The Injury_Cause_Category feature will classify the WCIO_Cause_of_Injury_Code values into broader cause categories. For example, codes related to burns or scalds can be grouped together, as well as those for falls or motor vehicle accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of WCIO Cause of Injury codes to broader categories\n",
    "cause_of_injury_mapping = {\n",
    "    **dict.fromkeys(list(range(1, 10)) + [11, 14, 84], 'Burn or Scald'),\n",
    "    **dict.fromkeys([10, 12, 13, 20], 'Caught In, Under, or Between'),\n",
    "    **dict.fromkeys(list(range(15, 20)), 'Cut, Puncture, Scrape'),\n",
    "    **dict.fromkeys(list(range(25, 34)), 'Fall, Slip, or Trip'),\n",
    "    **dict.fromkeys(list(range(40, 51)), 'Motor Vehicle'),\n",
    "    **dict.fromkeys(list(range(52, 62)) + [97], 'Strain or Injury By'),\n",
    "    **dict.fromkeys(list(range(65, 71)), 'Striking Against or Stepping On'),\n",
    "    **dict.fromkeys(list(range(74, 82)) + [85, 86], 'Struck or Injured by'),\n",
    "    **dict.fromkeys(list(range(94, 96)), 'Rubbed or Abraded by'),\n",
    "    **dict.fromkeys(list(range(87, 94)) + [96, 98, 99, 82], 'Miscellaneous Causes'),\n",
    "    **dict.fromkeys([83], 'COVID-19 Injury')\n",
    "}\n",
    "\n",
    "\n",
    "# Creating the Injury_Cause_Category column by mapping Cause of Injury codes to categories\n",
    "X_train_processed['Injury_Cause_Category'] = X_train_processed['WCIO Cause of Injury Code'].map(cause_of_injury_mapping)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Injury_Cause_Category'] = X_val_processed['WCIO Cause of Injury Code'].map(cause_of_injury_mapping)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Injury_Cause_Category'] = df_test_processed['WCIO Cause of Injury Code'].map(cause_of_injury_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Injury_Cause_Category\n",
       "Strain or Injury By                103853\n",
       "Fall, Slip, or Trip                 83895\n",
       "Struck or Injured by                70277\n",
       "Miscellaneous Causes                33947\n",
       "Cut, Puncture, Scrape               28247\n",
       "COVID-19 Injury                     17547\n",
       "Motor Vehicle                       16811\n",
       "Striking Against or Stepping On     13598\n",
       "Caught In, Under, or Between        13462\n",
       "Burn or Scald                        8547\n",
       "Rubbed or Abraded by                  689\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_processed['Injury_Cause_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_processed['Injury_Cause_Category'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have created new categorical features (Injury_Nature_Category, Body_Part_Category, Injury_Cause_Category) that provide a more meaningful representation of the original codes, it makes sense to remove the original code features. Keeping them would add redundancy, decrease interpretability, and unnecessarily increase the dimensionality of the dataset, potentially affecting model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removing the code features from train, validation, and test datasets\n",
    "# X_train_processed = X_train_processed.drop(columns=[\n",
    "#     'WCIO Cause of Injury Code',\n",
    "#     'WCIO Nature of Injury Code',\n",
    "#     'WCIO Part Of Body Code'\n",
    "# ])\n",
    "\n",
    "# X_val_processed = X_val_processed.drop(columns=[\n",
    "#     'WCIO Cause of Injury Code',\n",
    "#     'WCIO Nature of Injury Code',\n",
    "#     'WCIO Part Of Body Code'\n",
    "# ])\n",
    "\n",
    "# df_test_processed = df_test_processed.drop(columns=[\n",
    "#     'WCIO Cause of Injury Code',\n",
    "#     'WCIO Nature of Injury Code',\n",
    "#     'WCIO Part Of Body Code'\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.16. Risk of Each Job <a class=\"anchor\" id=\"section_3_16\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk = [11, 21, 23, 31, 32, 33, 48, 49]\n",
    "\n",
    "medium_risk = [22, 42, 44, 45, 56, 62, 71, 72, 81, 92]\n",
    "\n",
    "low_risk = [51, 52, 53, 54, 55, 61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Industry Code Industry Risk\n",
      "0           11.0     High Risk\n",
      "1           31.0     High Risk\n",
      "2           33.0     High Risk\n",
      "3           31.0     High Risk\n",
      "4           62.0   Medium Risk\n",
      "   Industry Code Industry Risk\n",
      "0           48.0     High Risk\n",
      "1           45.0   Medium Risk\n",
      "2           56.0   Medium Risk\n",
      "3           48.0     High Risk\n",
      "4           55.0      Low Risk\n"
     ]
    }
   ],
   "source": [
    "# Define a function to assign risk levels based on the industry code\n",
    "def assign_risk(industry_code):\n",
    "    if industry_code in high_risk:\n",
    "        return 'High Risk'\n",
    "    elif industry_code in medium_risk:\n",
    "        return 'Medium Risk'\n",
    "    elif industry_code in low_risk:\n",
    "        return 'Low Risk'\n",
    "    else:\n",
    "        return 'Unknown Risk'\n",
    "\n",
    "# Apply the function to create the 'Industry Risk' column for train and test datasets\n",
    "X_train_processed['Industry Risk'] = X_train_processed['Industry Code'].apply(assign_risk)\n",
    "X_val_processed['Industry Risk'] = X_val_processed['Industry Code'].apply(assign_risk)\n",
    "df_test_processed['Industry Risk'] = df_test['Industry Code'].apply(assign_risk)\n",
    "\n",
    "# Display a preview to verify\n",
    "print(X_train_processed[['Industry Code', 'Industry Risk']].head())\n",
    "print(df_test_processed[['Industry Code', 'Industry Risk']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALTERNATIVA POSS√çVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique industry descriptions\n",
    "#unique_industries = X_train_processed['Industry Code Description'].unique()\n",
    "#print(f\"Unique Industry Descriptions: {len(unique_industries)}\")\n",
    "#print(unique_industries[:10])  # Display the first 10 industry descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Industry Code Description' and calculate the frequency of claims\n",
    "#industry_injury_counts = X_train_processed.groupby('Industry Code Description')['Claim Injury Type'].count()\n",
    "\n",
    "# Normalize the injury frequencies to assign risk scores (1 = Low, 2 = Medium, 3 = High)\n",
    "#min_count = industry_injury_counts.min()\n",
    "#max_count = industry_injury_counts.max()\n",
    "#industry_injury_normalized = (industry_injury_counts - min_count) / (max_count - min_count)\n",
    "\n",
    "# Assign risk levels based on normalized frequencies\n",
    "#industry_risk_levels = industry_injury_normalized.apply(lambda x: 1 if x < 0.33 else (2 if x < 0.66 else 3))\n",
    "\n",
    "# Create a mapping dictionary\n",
    "#industry_risk_mapping = industry_risk_levels.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new \"Job Risk Level\" column to the dataset\n",
    "#X_train_processed['Job Risk Level'] = X_train_processed['Industry Code Description'].map(industry_risk_mapping)\n",
    "#X_val_processed['Job Risk Level'] = X_val_processed['Industry Code Description'].map(industry_risk_mapping)\n",
    "#df_test_processed['Job Risk Level'] = df_test_processed['Industry Code Description'].map(industry_risk_mapping)\n",
    "\n",
    "# Verify the new column\n",
    "#print(X_train_processed[['Industry Code Description', 'Job Risk Level']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.17. Relation between Salary and Dependents <a class=\"anchor\" id=\"section_3_17\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "The variable `Salary_Per_Dependent` denotes the average salary allocated per dependent in a household. This metric may provide valuable insights into the financial responsibilities faced by individuals and families, as well as their potential correlation with the frequency and severity of injury claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature: dividing the salary (Average Weekly Wage) by the number of dependents\n",
    "X_train_processed['Salary_Per_Dependent'] = X_train_processed['Average Weekly Wage'] / (X_train_processed['Number of Dependents'] + 1)\n",
    "\n",
    "# Apply the same transformation to the validation set\n",
    "X_val_processed['Salary_Per_Dependent'] = X_val_processed['Average Weekly Wage'] / (X_val_processed['Number of Dependents'] + 1)\n",
    "\n",
    "# Apply the same transformation to the test set\n",
    "df_test_processed['Salary_Per_Dependent'] = df_test_processed['Average Weekly Wage'] / (df_test_processed['Number of Dependents'] +1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Encoding <a class=\"anchor\" id=\"chapter5\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save Dataset for Modelling <a class=\"anchor\" id=\"chapter5\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
