{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<h3 align=\"center\">Machine Learning - Project</h3>**\n",
    "## **<h3 align=\"center\">3. Feature Engineering & Encoding</h3>**\n",
    "### **<h3 align=\"center\">Group 30 - Project</h3>**\n",
    "\n",
    "\n",
    "### Group Members\n",
    "| Name              | Email                        | Student ID |\n",
    "|-------------------|------------------------------|------------|\n",
    "| Alexandra Pinto   | 20211599@novaims.unl.pt      | 20211599   |\n",
    "| Gon√ßalo Peres     | 20211625@novaims.unl.pt      | 20211625   |\n",
    "| Leonor Mira       | 20240658@novaims.unl.pt      | 20240658   |\n",
    "| Miguel Nat√°rio    | 20240498@novaims.unl.pt      | 20240498   |\n",
    "| Nuno Bernardino   | 20211546@novaims.unl.pt      | 20211546   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### **3. Feature Engineering & Encoding Notebook**\n",
    "**Description:**  \n",
    "This notebook builds upon the preprocessed dataset from the Preprocessing & Cleaning notebook to prepare features for hierarchical classification. Key steps include:  \n",
    "- **Feature Engineering:** Create or transform features to enhance predictive power, including interaction terms, date-based calculations, and aggregations.  \n",
    "- **Encoding Categorical Variables:** Apply encoding techniques suited to the cardinality and nature of categorical variables, such as ordinal encoding, one-hot encoding, or frequency encoding.  \n",
    "- **Tailored Feature Preparation:** Prepare separate datasets for each level of hierarchical classification, ensuring optimal feature sets for Level 1 (binary classification) and Level 2 (binary and multi-class classification).  \n",
    "- **Output:** Save the feature-engineered datasets (in CSV or Pickle format) for modeling in subsequent notebooks.  \n",
    "\n",
    "This notebook ensures the dataset is optimally prepared for hierarchical classification, balancing feature relevance and computational efficiency.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"toc\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "* [1. Import the Libraries](#chapter1)\n",
    "* [2. Import the Datasets](#chapter2)       \n",
    "* [3. Feature Engineering](#chapter3)\n",
    "    * [3.1. Carrier-District Interaction](#section_3_1)\n",
    "    * [3.2. Income Category](#section_3_2)\n",
    "    * [3.3. Days_To_First_Hearing](#section_3_3)\n",
    "    * [3.4. Accident Quarter](#section_3_4)\n",
    "    * [3.5. Accident Year](#section_3_5)\n",
    "    * [3.6. Accident on Day and Weekend](#section_3_6)\n",
    "    * [3.7. Age Group](#section_3_7)\n",
    "    * [3.8. Time from Assembly Date to C-2 Filing](#section_3_8)\n",
    "    * [3.9. Time from Accident to C-2 Filing](#section_3_9)\n",
    "    * [3.10. Zip_Code_Simplified](#section_3_10)\n",
    "    * [3.11. Carrier Type Merged](#section_3_11)\n",
    "    * [3.12. Carrier_Name_Simplified](#section_3_12)\n",
    "    * [3.13. Body_Part_Category](#section_3_13)\n",
    "    * [3.14. Injury_Nature_Category](#section_3_14)\n",
    "    * [3.15. Injury_Cause_Category](#section_3_15)\n",
    "    * [3.16. Risk of Each Job](#section_3_16)\n",
    "    * [3.17. Relation between Salary and Dependents](#section_3_17)\n",
    "* [4. Encoding](#chapter4)\n",
    "* [5. Save Dataset for Modelling](#chapter5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# 1. Import the Libraries üìö<a class=\"anchor\" id=\"chapter1\"></a>\n",
    "\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "In this section we will imported the needed libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import zipfile\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "# --- Scikit-Learn Modules for Data Partitioning and Preprocessing ---\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Warnings ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Import functions from utils\n",
    "# from utils import analyze_numerical_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and Prepare Datasets üìÅ<a class=\"anchor\" id=\"chapter2\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "Now, we will load the dataset prepared in **Notebook 2: Preprocessing & Cleaning**, where we addressed key inconsistencies such as missing values and outliers. This preprocessed dataset serves as the foundation for the feature engineering steps in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                 Accident Date  Age at Injury                  Assembly Date  \\\n",
       " Claim Identifier                                                               \n",
       " 6099734             2022-09-22             67  2022-10-03 00:00:00.000000000   \n",
       " 5796182             2021-08-21             30  2021-08-24 00:00:00.000000000   \n",
       " 6128237             2022-11-03             58  2022-11-08 00:00:00.000000000   \n",
       " 5394501             2019-07-27             55  2020-01-03 00:00:00.000000000   \n",
       " 5452763             2020-03-11             25  2020-03-13 00:00:00.000000000   \n",
       " \n",
       "                   Attorney/Representative  Average Weekly Wage  Birth Year  \\\n",
       " Claim Identifier                                                             \n",
       " 6099734                               1.0                  839        1955   \n",
       " 5796182                               0.0                  150        1991   \n",
       " 6128237                               0.0                 2261        1964   \n",
       " 5394501                               0.0                 1227        1964   \n",
       " 5452763                               0.0                 1061        1995   \n",
       " \n",
       "                     C-2 Date    C-3 Date                   Carrier Name  \\\n",
       " Claim Identifier                                                          \n",
       " 6099734           2022-10-01  2022-10-28    INDEMNITY INS. OF N AMERICA   \n",
       " 5796182           2021-08-24           0  SAFETY NATIONAL CASUALTY CORP   \n",
       " 6128237           2022-11-08           0   AMERICAN ZURICH INSURANCE CO   \n",
       " 5394501           2020-01-15  2019-12-20  NEW YORK BLACK CAR OPERATORS'   \n",
       " 5452763           2020-03-13           0           STATE INSURANCE FUND   \n",
       " \n",
       "                   Carrier Type  ... IME-4 Count  Industry Code  \\\n",
       " Claim Identifier                ...                              \n",
       " 6099734                PRIVATE  ...         4.0           44.0   \n",
       " 5796182                PRIVATE  ...         0.0           44.0   \n",
       " 6128237                PRIVATE  ...         0.0           22.0   \n",
       " 5394501           SELF PRIVATE  ...         0.0           48.0   \n",
       " 5452763                    SIF  ...         0.0           61.0   \n",
       " \n",
       "                  Medical Fee Region WCIO Cause of Injury Code  \\\n",
       " Claim Identifier                                                \n",
       " 6099734                          IV                      32.0   \n",
       " 5796182                         III                      12.0   \n",
       " 6128237                           I                      19.0   \n",
       " 5394501                          IV                      45.0   \n",
       " 5452763                          II                      74.0   \n",
       " \n",
       "                  WCIO Nature of Injury Code  WCIO Part Of Body Code  Zip Code  \\\n",
       " Claim Identifier                                                                \n",
       " 6099734                                49.0                    53.0   11550.0   \n",
       " 5796182                                10.0                    56.0   10963.0   \n",
       " 6128237                                43.0                    41.0   13808.0   \n",
       " 5394501                                49.0                    90.0   10040.0   \n",
       " 5452763                                52.0                    34.0   12306.0   \n",
       " \n",
       "                  Agreement Reached  Number of Dependents  Accident Year  \n",
       " Claim Identifier                                                         \n",
       " 6099734                        0.0                   0.0           2022  \n",
       " 5796182                        0.0                   3.0           2021  \n",
       " 6128237                        0.0                   1.0           2022  \n",
       " 5394501                        0.0                   1.0           2019  \n",
       " 5452763                        0.0                   0.0           2020  \n",
       " \n",
       " [5 rows x 25 columns],\n",
       "                  Accident Date  Age at Injury Assembly Date  \\\n",
       " Claim Identifier                                              \n",
       " 5730729             2021-05-18             52    2021-05-25   \n",
       " 6038049             2022-04-09             28    2022-07-15   \n",
       " 5887035             2021-11-18             61    2021-12-28   \n",
       " 5953832             2022-01-21             52    2022-03-28   \n",
       " 5756573             2021-06-23             61    2021-06-29   \n",
       " \n",
       "                   Attorney/Representative  Average Weekly Wage  Birth Year  \\\n",
       " Claim Identifier                                                             \n",
       " 5730729                               1.0                 1681        1969   \n",
       " 6038049                               0.0                  763        1994   \n",
       " 5887035                               0.0                 1021        1960   \n",
       " 5953832                               0.0                 1042        1970   \n",
       " 5756573                               1.0                 1265        1960   \n",
       " \n",
       "                     C-2 Date    C-3 Date                 Carrier Name  \\\n",
       " Claim Identifier                                                        \n",
       " 5730729           2021-05-25  2021-06-04         STATE INSURANCE FUND   \n",
       " 6038049           2022-07-15           0         STATE INSURANCE FUND   \n",
       " 5887035           2021-12-28           0  INDEMNITY INS. OF N AMERICA   \n",
       " 5953832           2022-03-28           0             AIU INSURANCE CO   \n",
       " 5756573           2021-06-29  2021-06-29    INDEMNITY INSURANCE CO OF   \n",
       " \n",
       "                  Carrier Type  ... IME-4 Count  Industry Code  \\\n",
       " Claim Identifier               ...                              \n",
       " 5730729                   SIF  ...         1.0           92.0   \n",
       " 6038049                   SIF  ...         0.0           72.0   \n",
       " 5887035               PRIVATE  ...         0.0           31.0   \n",
       " 5953832               PRIVATE  ...         0.0           32.0   \n",
       " 5756573               PRIVATE  ...         7.0           31.0   \n",
       " \n",
       "                  Medical Fee Region WCIO Cause of Injury Code  \\\n",
       " Claim Identifier                                                \n",
       " 5730729                          IV                      29.0   \n",
       " 6038049                          IV                      68.0   \n",
       " 5887035                           I                      60.0   \n",
       " 5953832                         III                      60.0   \n",
       " 5756573                          IV                      58.0   \n",
       " \n",
       "                  WCIO Nature of Injury Code  WCIO Part Of Body Code  Zip Code  \\\n",
       " Claim Identifier                                                                \n",
       " 5730729                                52.0                    56.0   11229.0   \n",
       " 6038049                                 7.0                    11.0   11230.0   \n",
       " 5887035                                49.0                    42.0   13367.0   \n",
       " 5953832                                36.0                    38.0   10901.0   \n",
       " 5756573                                52.0                    42.0   11236.0   \n",
       " \n",
       "                  Agreement Reached  Number of Dependents  Accident Year  \n",
       " Claim Identifier                                                         \n",
       " 5730729                        0.0                   0.0           2021  \n",
       " 6038049                        0.0                   0.0           2022  \n",
       " 5887035                        0.0                   2.0           2021  \n",
       " 5953832                        0.0                   2.0           2022  \n",
       " 5756573                        0.0                   2.0           2021  \n",
       " \n",
       " [5 rows x 25 columns],\n",
       "                  Accident Date  Age at Injury Assembly Date  \\\n",
       " Claim Identifier                                              \n",
       " 6165911             2022-12-24             19    2023-01-02   \n",
       " 6166141             2022-11-20             19    2023-01-02   \n",
       " 6165907             2022-12-26             59    2023-01-02   \n",
       " 6166047             2022-12-28             42    2023-01-02   \n",
       " 6166102             2022-12-20             25    2023-01-02   \n",
       " \n",
       "                   Attorney/Representative  Average Weekly Wage  Birth Year  \\\n",
       " Claim Identifier                                                             \n",
       " 6165911                                 0                 1227        2003   \n",
       " 6166141                                 0                  807        2003   \n",
       " 6165907                                 0                  934        1963   \n",
       " 6166047                                 0                 1227        1980   \n",
       " 6166102                                 0                 1222        1997   \n",
       " \n",
       "                     C-2 Date C-3 Date                 Carrier Name  \\\n",
       " Claim Identifier                                                     \n",
       " 6165911           2023-01-02        0    INDEMNITY INSURANCE CO OF   \n",
       " 6166141           2023-01-02        0      A I U INSURANCE COMPANY   \n",
       " 6165907           2022-12-31        0    AMGUARD INSURANCE COMPANY   \n",
       " 6166047           2023-01-02        0  INDEMNITY INS. OF N AMERICA   \n",
       " 6166102           2022-12-31        0   NEW HAMPSHIRE INSURANCE CO   \n",
       " \n",
       "                  Carrier Type  ... Gender  IME-4 Count Industry Code  \\\n",
       " Claim Identifier               ...                                     \n",
       " 6165911               PRIVATE  ...      M          0.0          48.0   \n",
       " 6166141               PRIVATE  ...      F          0.0          45.0   \n",
       " 6165907               PRIVATE  ...      F          0.0          56.0   \n",
       " 6166047               PRIVATE  ...      F          0.0          48.0   \n",
       " 6166102               PRIVATE  ...      M          0.0          55.0   \n",
       " \n",
       "                  Medical Fee Region WCIO Cause of Injury Code  \\\n",
       " Claim Identifier                                                \n",
       " 6165911                          IV                      31.0   \n",
       " 6166141                          IV                      75.0   \n",
       " 6165907                         III                      68.0   \n",
       " 6166047                          IV                      25.0   \n",
       " 6166102                          IV                      79.0   \n",
       " \n",
       "                   WCIO Nature of Injury Code  WCIO Part Of Body Code Zip Code  \\\n",
       " Claim Identifier                                                                \n",
       " 6165911                                 10.0                    54.0  10466.0   \n",
       " 6166141                                 10.0                    10.0  11691.0   \n",
       " 6165907                                 49.0                    62.0  10604.0   \n",
       " 6166047                                 10.0                    53.0  11411.0   \n",
       " 6166102                                 40.0                    37.0  11212.0   \n",
       " \n",
       "                   Number of Dependents  Accident Year  \n",
       " Claim Identifier                                       \n",
       " 6165911                              1           2022  \n",
       " 6166141                              1           2022  \n",
       " 6165907                              0           2022  \n",
       " 6166047                              6           2022  \n",
       " 6166102                              5           2022  \n",
       " \n",
       " [5 rows x 24 columns])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets saved from Notebook 2\n",
    "X_train = pd.read_csv(\"../processed_datasets/X_train_preprocessed.csv\", index_col=\"Claim Identifier\")\n",
    "X_val = pd.read_csv(\"../processed_datasets/X_val_preprocessed.csv\", index_col=\"Claim Identifier\")\n",
    "df_test = pd.read_csv(\"../processed_datasets/df_test_preprocessed.csv\", index_col=\"Claim Identifier\")\n",
    "y_train = np.load(\"../processed_datasets/y_train_preprocessed.npy\", allow_pickle=True)\n",
    "y_val =  np.load(\"../processed_datasets/y_train_preprocessed.npy\", allow_pickle=True)\n",
    "\n",
    "# Verify the datasets are loaded successfully\n",
    "X_train.head(), X_val.head(), df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, ..., 3, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accident Date                   0\n",
       "Age at Injury                   0\n",
       "Assembly Date                   0\n",
       "Attorney/Representative         0\n",
       "Average Weekly Wage             0\n",
       "Birth Year                      0\n",
       "C-2 Date                        0\n",
       "C-3 Date                        0\n",
       "Carrier Name                    0\n",
       "Carrier Type                    0\n",
       "County of Injury                0\n",
       "COVID-19 Indicator              0\n",
       "District Name                   0\n",
       "First Hearing Date              0\n",
       "Gender                          0\n",
       "IME-4 Count                     0\n",
       "Industry Code                   0\n",
       "Medical Fee Region              0\n",
       "WCIO Cause of Injury Code       0\n",
       "WCIO Nature of Injury Code      0\n",
       "WCIO Part Of Body Code          0\n",
       "Zip Code                        0\n",
       "Agreement Reached               0\n",
       "Number of Dependents            0\n",
       "Accident Year                   0\n",
       "Carrier_District_Interaction    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "\n",
    "Feature engineering is the process of preparing data for machine learning models by transforming raw data into meaningful features that enhance model performance. In this section, we create, select, and modify variables to capture significant patterns within the data, making it more informative and useful for the model‚Äôs learning process. Through these transformations, we aim to improve the model‚Äôs accuracy and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Carrier-District Interaction <a class=\"anchor\" id=\"section_3_1\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "Combining **Carrier Type** with **District Name** may reveal regional preferences for certain insurance carriers, which could be useful in understanding regional biases or regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature by combining Carrier Type and District Name\n",
    "X_train['Carrier_District_Interaction'] = X_train['Carrier Type'] + \"_\" + X_train['District Name']\n",
    "\n",
    "# Apply to the val X_val\n",
    "X_val['Carrier_District_Interaction'] = X_val['Carrier Type'] + \"_\" + X_val['District Name']\n",
    "\n",
    "# Apply to the test set\n",
    "df_test['Carrier_District_Interaction'] = df_test['Carrier Type'] + \"_\" + df_test['District Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Income Category  <a class=\"anchor\" id=\"section_3_2\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "Creating categories for **Average Weekly Wage** can simplify the continuous nature of income into meaningful segments such as Low, Average, and High, which could help the model understand different socioeconomic statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25     876.0\n",
      "0.50    1211.0\n",
      "0.75    1481.0\n",
      "0.90    1681.0\n",
      "Name: Average Weekly Wage, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate key percentiles\n",
    "percentiles = X_train['Average Weekly Wage'].quantile([0.25, 0.5, 0.75, 0.9])\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the bins and labels for categorizing income based on percentiles\n",
    "income_bins = [0, 876.0, 1211.0, 1481.0, 1681.0, float('inf')]  # float('inf') allows us to set an open-ended range\n",
    "income_labels = ['Low Income', 'Lower-Middle Income', 'Middle Income', 'Upper-Middle Income', 'High Income']\n",
    "\n",
    "# Creating the new feature for income categories for the train set\n",
    "X_train['Income_Category'] = pd.cut(X_train['Average Weekly Wage'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val['Income_Category'] = pd.cut(X_val['Average Weekly Wage'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test['Income_Category'] = pd.cut(df_test['Average Weekly Wage'], bins=income_bins, labels=income_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating this categorical feature, we drop the original Average Weekly Wage column since it‚Äôs now represented by Income_Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the 'Average Weekly Wage' column as it's represented by 'Income_Category'\n",
    "# X_train_processed = X_train_processed.drop(columns=['Average Weekly Wage'])\n",
    "# X_val_processed = X_val_processed.drop(columns=['Average Weekly Wage'])\n",
    "# df_test_processed = df_test_processed.drop(columns=['Average Weekly Wage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Days_To_First_Hearing  <a class=\"anchor\" id=\"section_3_3\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "The feature **Days_To_First_Hearing** was created to capture the number of days between the Accident Date and the First Hearing Date. If a First Hearing Date is available, the feature represents the time elapsed, which can help the model understand the speed of the claim process. If the First Hearing Date is missing, it is represented as 0, indicating that a hearing has not occurred yet. This approach provides more nuanced information than simply indicating whether the hearing occurred or not, allowing the model to learn from both the presence and timing of the first hearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Hearing Date column after conversion:\n",
      "Claim Identifier\n",
      "6099734   2023-02-28\n",
      "5796182          NaT\n",
      "6128237          NaT\n",
      "5394501   2020-03-20\n",
      "5452763          NaT\n",
      "Name: First Hearing Date, dtype: datetime64[ns]\n",
      "C-2 Date column after conversion:\n",
      "Claim Identifier\n",
      "6099734   2022-10-01\n",
      "5796182   2021-08-24\n",
      "6128237   2022-11-08\n",
      "5394501   2020-01-15\n",
      "5452763   2020-03-13\n",
      "Name: C-2 Date, dtype: datetime64[ns]\n",
      "C-3 Date column after conversion:\n",
      "Claim Identifier\n",
      "6099734   2022-10-28\n",
      "5796182          NaT\n",
      "6128237          NaT\n",
      "5394501   2019-12-20\n",
      "5452763          NaT\n",
      "Name: C-3 Date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# List of columns to convert to datetime\n",
    "date_columns = ['First Hearing Date', 'C-2 Date', 'C-3 Date']\n",
    "\n",
    "# Replace 0 with NaT and convert to datetime\n",
    "for col in date_columns:\n",
    "    if col in X_train.columns:  # Check if the column exists in the DataFrame\n",
    "        X_train[col] = pd.to_datetime(X_train[col].replace(0, pd.NaT), errors='coerce')\n",
    "    if col in X_val.columns:\n",
    "        X_val[col] = pd.to_datetime(X_val[col].replace(0, pd.NaT), errors='coerce')\n",
    "    if col in df_test.columns:\n",
    "        df_test[col] = pd.to_datetime(df_test[col].replace(0, pd.NaT), errors='coerce')\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"First Hearing Date column after conversion:\")\n",
    "print(X_train['First Hearing Date'].head())\n",
    "print(\"C-2 Date column after conversion:\")\n",
    "print(X_train['C-2 Date'].head())\n",
    "print(\"C-3 Date column after conversion:\")\n",
    "print(X_train['C-3 Date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days_To_First_Hearing in Train Set:\n",
      "count    459219.000000\n",
      "mean         88.993785\n",
      "std         270.600201\n",
      "min          -5.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%          72.000000\n",
      "max       16373.000000\n",
      "Name: Days_To_First_Hearing, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Convert date columns to datetime\n",
    "date_columns = ['First Hearing Date', 'Accident Date']\n",
    "for col in date_columns:\n",
    "    X_train[col] = pd.to_datetime(X_train[col], errors='coerce')\n",
    "    X_val[col] = pd.to_datetime(X_val[col], errors='coerce')\n",
    "    df_test[col] = pd.to_datetime(df_test[col], errors='coerce')\n",
    "\n",
    "# Function to calculate days to first hearing\n",
    "def calculate_hearing_days(row):\n",
    "    if pd.notna(row['First Hearing Date']) and pd.notna(row['Accident Date']):\n",
    "        return (row['First Hearing Date'] - row['Accident Date']).days\n",
    "    return 0  # If no hearing date exists, return 0\n",
    "\n",
    "# Apply the function to create the new feature\n",
    "X_train['Days_To_First_Hearing'] = X_train.apply(calculate_hearing_days, axis=1)\n",
    "X_val['Days_To_First_Hearing'] = X_val.apply(calculate_hearing_days, axis=1)\n",
    "df_test['Days_To_First_Hearing'] = df_test.apply(calculate_hearing_days, axis=1)\n",
    "\n",
    "# Verify the new feature\n",
    "print(\"Days_To_First_Hearing in Train Set:\")\n",
    "print(X_train['Days_To_First_Hearing'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating this binary feature, we can drop the original First Hearing Date column from the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop First Hearing Date from the train, val, and test sets\n",
    "X_train_processed = X_train.drop(columns=['First Hearing Date'])\n",
    "X_val_processed = X_val.drop(columns=['First Hearing Date'])\n",
    "df_test_processed = df_test.drop(columns=['First Hearing Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Accident Quarter  <a class=\"anchor\" id=\"section_3_4\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "Temporal data can often influence outcomes. Extracting the quarter of the accident (e.g., 1st, 2nd, etc.) helps the model capture seasonal patterns that may impact accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the quarter of the Accident Date\n",
    "X_train_processed['Accident_Quarter'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce').dt.quarter\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Accident_Quarter'] = pd.to_datetime(X_val_processed['Accident Date'], errors='coerce').dt.quarter\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Accident_Quarter'] = pd.to_datetime(df_test_processed['Accident Date'], errors='coerce').dt.quarter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Accident Year <a class=\"anchor\" id=\"section_3_5\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "The year can help the model understand seasonal or yearly effects, like accident patterns during different times of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the year from the Accident Date\n",
    "X_train_processed['Accident_Year'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce').dt.year\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Accident_Year'] = pd.to_datetime(X_val_processed['Accident Date'], errors='coerce').dt.year\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Accident_Year'] = pd.to_datetime(df_test_processed['Accident Date'], errors='coerce').dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Accident on Day and Weekend <a class=\"anchor\" id=\"section_3_6\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "The day of the accident could be significant, as weekends might have different risk factors compared to weekdays. We will extract the day of the week and create a feature to indicate if the accident occurred on a weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the day of the week and creating a feature to indicate if the accident occurred on a weekend\n",
    "X_train_processed['Accident Day'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce').dt.dayofweek\n",
    "X_train_processed['Accident on Weekend'] = X_train_processed['Accident Day'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Accident Day'] = pd.to_datetime(X_val_processed['Accident Date'], errors='coerce').dt.dayofweek\n",
    "X_val_processed['Accident on Weekend'] = X_val_processed['Accident Day'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Accident Day'] = pd.to_datetime(df_test_processed['Accident Date'], errors='coerce').dt.dayofweek\n",
    "df_test_processed['Accident on Weekend'] = df_test_processed['Accident Day'].apply(lambda x: 1 if x >= 5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Age Group <a class=\"anchor\" id=\"section_3_7\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "Grouping ages can help simplify the model‚Äôs understanding of different age demographics (e.g., Youth, Young Adult, Middle Age, Senior). This could potentially improve model interpretability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    459219.000000\n",
       "mean         42.884970\n",
       "std          12.952508\n",
       "min           5.000000\n",
       "25%          33.000000\n",
       "50%          42.000000\n",
       "75%          53.000000\n",
       "max          82.000000\n",
       "Name: Age at Injury, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display unique values in 'Age at Injury' to understand the range\n",
    "X_train_processed['Age at Injury'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins and labels for age groups\n",
    "age_bins = [0, 25, 45, 65, float('inf')]\n",
    "age_labels = ['Youth', 'Young Adult', 'Middle Age', 'Senior']\n",
    "\n",
    "# Creating a new feature for age groups\n",
    "X_train_processed['Age Group'] = pd.cut(X_train_processed['Age at Injury'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Age Group'] = pd.cut(X_val_processed['Age at Injury'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Age Group'] = pd.cut(df_test_processed['Age at Injury'], bins=age_bins, labels=age_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Promptness_category <a class=\"anchor\" id=\"section_3_8\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "The `promptness_category` feature categorizes the time taken between key events in the claims process, specifically measuring the difference between the `Accident Date` and the `Assembly Date`. This feature quantifies the speed or delay in assembling the claim and provides insight into how promptly claims are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def categorize_promptness(df, date1_col, date2_col, new_col_name):\n",
    "    \"\"\"\n",
    "    Calculate and categorize promptness between two date columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to process.\n",
    "    - date1_col: The column representing the first date (e.g., Assembly Date).\n",
    "    - date2_col: The column representing the second date (e.g., Accident Date).\n",
    "    - new_col_name: The name of the new categorical column for promptness.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with new categorized promptness column.\n",
    "    \"\"\"\n",
    "    # Ensure the date columns are datetime\n",
    "    df[date1_col] = pd.to_datetime(df[date1_col], errors='coerce')\n",
    "    df[date2_col] = pd.to_datetime(df[date2_col], errors='coerce')\n",
    "\n",
    "    # Calculate the difference in days\n",
    "    df['Days_Difference'] = (df[date1_col] - df[date2_col]).dt.days\n",
    "\n",
    "    # Assign categories based on conditions\n",
    "    def assign_category(row):\n",
    "        if pd.isna(row[date1_col]) or pd.isna(row[date2_col]) or row['Days_Difference'] <= 0:\n",
    "            return 'Form Not Received'\n",
    "        elif row['Days_Difference'] <= 7:\n",
    "            return 'Until 1 week'\n",
    "        elif row['Days_Difference'] <= 14:\n",
    "            return 'Between 1 and 2 weeks'\n",
    "        elif row['Days_Difference'] <= 30:\n",
    "            return 'Between 2 weeks and 1 month'\n",
    "        elif row['Days_Difference'] <= 90:\n",
    "            return '1 to 3 months'\n",
    "        elif row['Days_Difference'] <= 180:\n",
    "            return '3 to 6 months'\n",
    "        elif row['Days_Difference'] <= 365:\n",
    "            return '6 months to 1 year'\n",
    "        else:\n",
    "            return 'More than 1 year'\n",
    "\n",
    "    # Apply the function to assign categories\n",
    "    df[new_col_name] = df.apply(assign_category, axis=1)\n",
    "\n",
    "    # Drop the intermediate column\n",
    "    df.drop(columns=['Days_Difference'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to X_train_processed\n",
    "X_train_processed = categorize_promptness(X_train_processed, 'Assembly Date', 'Accident Date', 'promptness_category')\n",
    "\n",
    "# Apply the function to X_val_processed\n",
    "X_val_processed = categorize_promptness(X_val_processed, 'Assembly Date', 'Accident Date', 'promptness_category')\n",
    "\n",
    "# Apply the function to df_test_processed\n",
    "df_test_processed = categorize_promptness(df_test_processed, 'Assembly Date', 'Accident Date', 'promptness_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Until 1 week                   182309\n",
       "Between 1 and 2 weeks           97525\n",
       "Between 2 weeks and 1 month     77102\n",
       "1 to 3 months                   58174\n",
       "3 to 6 months                   17709\n",
       "More than 1 year                12507\n",
       "6 months to 1 year              10845\n",
       "Form Not Received                3048\n",
       "Name: promptness_category, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display value counts for the new column\n",
    "X_train_processed['promptness_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These categories allow us to observe the promptness in claim processing, with the majority falling within Until 1 week, indicating a generally swift assembly of claims. However, a significant portion extends beyond a month, with a small subset taking more than a year. This feature can provide insights into patterns of delays or rapid processing, possibly indicating areas for improvement in claim management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9. promptness_C2_category <a class=\"anchor\" id=\"section_3_9\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "The \"promptness_C2_category\" feature tracks the time taken to register the C-2 Date (the receipt of the employer's report of work-related injury/illness) after the Accident Date. It evaluates employers' promptness in reporting accidents, offering insights into compliance and potential administrative delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where 'C-2 Date' is earlier than 'Accident Date': 0\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'C-2 Date' and 'Accident Date' are datetime\n",
    "X_train_processed['C-2 Date'] = pd.to_datetime(X_train_processed['C-2 Date'], errors='coerce')\n",
    "X_train_processed['Accident Date'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce')\n",
    "\n",
    "# Count the number of rows where 'C-2 Date' is earlier than 'Accident Date'\n",
    "num_negative_values = (X_train_processed['C-2 Date'] < X_train_processed['Accident Date']).sum()\n",
    "\n",
    "# Print the number of rows with this condition\n",
    "print(f\"Number of rows where 'C-2 Date' is earlier than 'Accident Date': {num_negative_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these cases, where the C-2 Date is earlier than the Accident Date, we identify them as incorrect or inconsistent entries. This issue may have been introduced during the imputation of missing values. To resolve this, we will set the affected C-2 Date values to NaT (Not a Time), ensuring the data remains clean and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'C-2 Date' values set to NaT: 0\n"
     ]
    }
   ],
   "source": [
    "# Identify rows where 'C-2 Date' is earlier than 'Accident Date'\n",
    "invalid_c2_date_mask = X_train_processed['C-2 Date'] < X_train_processed['Accident Date']\n",
    "\n",
    "# Set 'C-2 Date' to NaT for the identified rows\n",
    "X_train_processed.loc[invalid_c2_date_mask, 'C-2 Date'] = pd.NaT\n",
    "\n",
    "# Verify the changes\n",
    "num_invalid_c2_dates = invalid_c2_date_mask.sum()\n",
    "print(f\"Number of 'C-2 Date' values set to NaT: {num_invalid_c2_dates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to X_train_processed\n",
    "X_train_processed = categorize_promptness(X_train_processed, 'C-2 Date', 'Accident Date', 'promptness_C2_category')\n",
    "\n",
    "# Apply the function to X_val_processed\n",
    "X_val_processed = categorize_promptness(X_val_processed, 'C-2 Date', 'Accident Date', 'promptness_C2_category')\n",
    "\n",
    "# Apply the function to df_test_processed\n",
    "df_test_processed = categorize_promptness(df_test_processed, 'C-2 Date', 'Accident Date', 'promptness_C2_category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Until 1 week                   183304\n",
       "Between 1 and 2 weeks           86992\n",
       "Between 2 weeks and 1 month     68979\n",
       "1 to 3 months                   56602\n",
       "3 to 6 months                   21417\n",
       "Form Not Received               15631\n",
       "6 months to 1 year              14305\n",
       "More than 1 year                11989\n",
       "Name: promptness_C2_category, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display value counts for the new column\n",
    "X_train_processed['promptness_C2_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. promptness_C3_category <a class=\"anchor\" id=\"section_3_10\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "The \"promptness_C3_category\" feature tracks the time taken to register the C-3 Date (the receipt of the employer's report of work-related injury/illness) after the Accident Date. It evaluates employers' promptness in reporting accidents, offering insights into compliance and potential administrative delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to X_train_processed\n",
    "X_train_processed = categorize_promptness(X_train_processed, 'C-3 Date', 'Accident Date', 'promptness_C3_category')\n",
    "\n",
    "# Apply the function to X_val_processed\n",
    "X_val_processed = categorize_promptness(X_val_processed, 'C-3 Date', 'Accident Date', 'promptness_C3_category')\n",
    "\n",
    "# Apply the function to df_test_processed\n",
    "df_test_processed = categorize_promptness(df_test_processed, 'C-3 Date', 'Accident Date', 'promptness_C3_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Form Not Received              311530\n",
       "1 to 3 months                   36320\n",
       "Between 2 weeks and 1 month     35159\n",
       "Between 1 and 2 weeks           22154\n",
       "Until 1 week                    21016\n",
       "3 to 6 months                   14384\n",
       "6 months to 1 year              10074\n",
       "More than 1 year                 8582\n",
       "Name: promptness_C3_category, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display value counts for the new column\n",
    "X_train_processed['promptness_C3_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating new features based on the existing date columns, we will remove the original date features to avoid redundancy and simplify the dataset. We believe that the impact of these date features is adequately captured in the newly engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = X_train_processed.drop(columns=['Accident Date', 'Assembly Date', 'C-2 Date','C-3 Date'])\n",
    "\n",
    "#Apply to the val set\n",
    "X_val_processed = X_val_processed.drop(columns=['Accident Date','Assembly Date', 'C-2 Date','C-3 Date'])\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed = df_test_processed.drop(columns=['Accident Date', 'Assembly Date', 'C-2 Date', 'C-3 Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. Zip_Code_Simplified <a class=\"anchor\" id=\"section_3_10\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "To reduce the dimensionality of the Zip Code feature, we will create a new feature called Zip_Code_Simplified. This feature will group all zip codes that appear less than 2,000 times in the training dataset into a category labeled as 'Other'. By doing this, we effectively reduce the number of unique zip codes, simplifying the model while retaining the most significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent Zip Codes with their counts:\n",
      "11236.0    4144\n",
      "11717.0    4061\n",
      "11434.0    3887\n",
      "11550.0    3133\n",
      "10467.0    2969\n",
      "10940.0    2635\n",
      "10701.0    2320\n",
      "10029.0    2211\n",
      "14224.0    2125\n",
      "11706.0    1995\n",
      "10314.0    1978\n",
      "11207.0    1854\n",
      "14609.0    1853\n",
      "11368.0    1837\n",
      "11208.0    1815\n",
      "12550.0    1780\n",
      "11212.0    1757\n",
      "11226.0    1735\n",
      "12601.0    1719\n",
      "11234.0    1612\n",
      "10466.0    1598\n",
      "11203.0    1583\n",
      "10462.0    1522\n",
      "11385.0    1502\n",
      "10456.0    1490\n",
      "Name: Zip Code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the most frequent Carrier Names along with their counts\n",
    "most_frequent_zipcode = X_train_processed['Zip Code'].value_counts().head(25)  # Adjust the number if you need more\n",
    "print(\"Most frequent Zip Codes with their counts:\")\n",
    "print(most_frequent_zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of 'Zip_Code_Simplified' feature in X_train_processed:\n",
      "OTHER      350340\n",
      "11236.0      4144\n",
      "11717.0      4061\n",
      "11434.0      3887\n",
      "11550.0      3133\n",
      "            ...  \n",
      "14150.0      1011\n",
      "13760.0      1009\n",
      "11691.0      1005\n",
      "10306.0      1004\n",
      "10460.0      1002\n",
      "Name: Zip_Code_Simplified, Length: 71, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a new feature called 'Zip_Code_Simplified' based on 'Zip Code' for train, validation, and test sets\n",
    "X_train_processed['Zip_Code_Simplified'] = X_train_processed['Zip Code']\n",
    "X_val_processed['Zip_Code_Simplified'] = X_val_processed['Zip Code']\n",
    "df_test_processed['Zip_Code_Simplified'] = df_test_processed['Zip Code']\n",
    "\n",
    "# Identify carrier names that occur fewer than 1000 times in X_train_processed\n",
    "zipcode_counts = X_train_processed['Zip Code'].value_counts()\n",
    "zipcode_to_replace = zipcode_counts[zipcode_counts < 1000].index\n",
    "\n",
    "# Replace carrier names with fewer than 1000 occurrences with 'OTHER' in all datasets using the identified carriers from X_train\n",
    "for dataset in [X_train_processed, X_val_processed, df_test_processed]:\n",
    "    dataset['Zip_Code_Simplified'] = dataset['Zip_Code_Simplified'].replace(zipcode_to_replace, 'OTHER')\n",
    "\n",
    "# Print the counts of the simplified carrier names in X_train_processed to verify the result\n",
    "print(\"Counts of 'Zip_Code_Simplified' feature in X_train_processed:\")\n",
    "print(X_train_processed['Zip_Code_Simplified'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ZIP Code uniqueness: 11442\n",
      "Simplified ZIP Code uniqueness: 71\n"
     ]
    }
   ],
   "source": [
    "# Display unique counts to compare the dimensionality reduction\n",
    "print(f\"Original ZIP Code uniqueness: {X_train_processed['Zip Code'].nunique()}\")\n",
    "print(f\"Simplified ZIP Code uniqueness: {X_train_processed['Zip_Code_Simplified'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation retains regional information while reducing the feature dimensionality, which can be beneficial for model interpretability and efficiency. The original Zip Code column has been removed to avoid redundancy. For this motive we will delete also the Zip Code, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_processed = X_train_processed.drop(columns=['Zip Code'])\n",
    "# # Apply to the val set\n",
    "# X_val_processed = X_val_processed.drop(columns=['Zip Code'])\n",
    "# # Apply to the test set\n",
    "# df_test_processed = df_test_processed.drop(columns=['Zip Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11. Carrier Type Merged <a class=\"anchor\" id=\"section_3_11\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "\n",
    "Since there are several categories under \"Special Fund\" with very few occurrences, combining them into a single category can reduce noise in the data and make the feature more manageable for the model.\n",
    "\n",
    "After merging, we observe the following distribution of Carrier Type Merged values in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIVATE         228492\n",
      "SELF PUBLIC      97456\n",
      "SIF              88862\n",
      "SELF PRIVATE     42169\n",
      "UNKNOWN           1407\n",
      "SPECIAL FUND       833\n",
      "Name: Carrier Type Merged, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a new feature that merges all 'Special Fund' categories into a single category for train, validation, and test sets\n",
    "for dataset in [X_train_processed, X_val_processed, df_test_processed]:\n",
    "    dataset['Carrier Type Merged'] = dataset['Carrier Type'].replace({\n",
    "        'SPECIAL FUND - UNKNOWN': 'SPECIAL FUND',\n",
    "        'SPECIAL FUND - POI CARRIER WCB MENANDS': 'SPECIAL FUND',\n",
    "        'SPECIAL FUND - CONS. COMM. (SECT. 25-A)': 'SPECIAL FUND'\n",
    "    })\n",
    "\n",
    "# Verifying the updated column for X_train_processed\n",
    "print(X_train_processed['Carrier Type Merged'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now let's delete Carrier Type from the train, val and test set\n",
    "# X_train_processed = X_train_processed.drop(columns=['Carrier Type'])\n",
    "# # Apply to the val set\n",
    "# X_val_processed = X_val_processed.drop(columns=['Carrier Type'])\n",
    "# # Apply to the test set\n",
    "# df_test_processed = df_test_processed.drop(columns=['Carrier Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12. Carrier_Name_Simplified <a class=\"anchor\" id=\"section_3_12\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "The 'Carrier Name' feature has high cardinality, with 1951 unique values. This level of uniqueness can complicate machine learning models, especially if some categories have very few instances. To simplify the analysis and potentially improve model performance, we will group carrier names with fewer than 500 occurrences under a single category called 'OTHER'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent Carrier Names with their counts:\n",
      "STATE INSURANCE FUND             88862\n",
      "POLICE, FIRE, SANITATION         17247\n",
      "AMERICAN ZURICH INSURANCE CO     14012\n",
      "CHARTER OAK FIRE INS CO          13697\n",
      "INDEMNITY INS. OF N AMERICA      11544\n",
      "SAFETY NATIONAL CASUALTY CORP    11163\n",
      "NEW HAMPSHIRE INSURANCE CO       10237\n",
      "LM INSURANCE CORP                 9830\n",
      "A I U INSURANCE COMPANY           8907\n",
      "INDEMNITY INSURANCE CO OF         7246\n",
      "NYC TRANSIT AUTHORITY             6603\n",
      "HARTFORD ACCIDENT & INDEMNITY     6039\n",
      "NEW YORK BLACK CAR OPERATORS'     5841\n",
      "ARCH INDEMNITY INSURANCE CO.      5321\n",
      "AIU INSURANCE CO                  5181\n",
      "CNY OTHER THAN ED, HED WATER      5161\n",
      "HEALTH & HOSPITAL CORP.           4478\n",
      "ARCH INDEMNITY INSURANCE CO       4281\n",
      "PENNSYLVANIA MANUFACTURERS'       3899\n",
      "PUBLIC EMPLOYERS RISK MGMT.       3656\n",
      "ACE AMERICAN INSURANCE CO.        3623\n",
      "OLD REPUBLIC INSURANCE CO.        3487\n",
      "MEMIC INDEMNITY COMPANY           3295\n",
      "WAL-MART ASSOCIATES, INC.         3158\n",
      "COUNTY OF NASSAU                  3046\n",
      "Name: Carrier Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the most frequent Carrier Names along with their counts\n",
    "most_frequent_carriers = X_train_processed['Carrier Name'].value_counts().head(25)  # Adjust the number if you need more\n",
    "print(\"Most frequent Carrier Names with their counts:\")\n",
    "print(most_frequent_carriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of 'Carrier_Name_Simplified' feature in X_train_processed:\n",
      "OTHER                            93013\n",
      "STATE INSURANCE FUND             88862\n",
      "POLICE, FIRE, SANITATION         17247\n",
      "AMERICAN ZURICH INSURANCE CO     14012\n",
      "CHARTER OAK FIRE INS CO          13697\n",
      "                                 ...  \n",
      "TOWN OF OYSTER BAY                 519\n",
      "NY LUMBERMENS INS. TRUST FUND      515\n",
      "AMTRUST INS CO OF KANSAS INC       512\n",
      "YONKERS, CITY OF                   510\n",
      "HEALTH & HOSPITALS CORP. CNY       505\n",
      "Name: Carrier_Name_Simplified, Length: 115, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a new feature called 'Carrier_Name_Simplified' based on 'Carrier Name' for train, validation, and test sets\n",
    "X_train_processed['Carrier_Name_Simplified'] = X_train_processed['Carrier Name']\n",
    "X_val_processed['Carrier_Name_Simplified'] = X_val_processed['Carrier Name']\n",
    "df_test_processed['Carrier_Name_Simplified'] = df_test_processed['Carrier Name']\n",
    "\n",
    "# Identify carrier names that occur fewer than 500 times in X_train_processed\n",
    "carrier_counts = X_train_processed['Carrier Name'].value_counts()\n",
    "carriers_to_replace = carrier_counts[carrier_counts < 500].index\n",
    "\n",
    "# Replace carrier names with fewer than 500 occurrences with 'OTHER' in all datasets using the identified carriers from X_train\n",
    "for dataset in [X_train_processed, X_val_processed, df_test_processed]:\n",
    "    dataset['Carrier_Name_Simplified'] = dataset['Carrier_Name_Simplified'].replace(carriers_to_replace, 'OTHER')\n",
    "\n",
    "# Print the counts of the simplified carrier names in X_train_processed to verify the result\n",
    "print(\"Counts of 'Carrier_Name_Simplified' feature in X_train_processed:\")\n",
    "print(X_train_processed['Carrier_Name_Simplified'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'Carrier Name': 1996\n",
      "Number of unique values in 'Carrier_Name_Simplified': 115\n"
     ]
    }
   ],
   "source": [
    "#print the number of unique values in the original 'Carrier Name' feature\n",
    "print(f\"Number of unique values in 'Carrier Name': {X_train_processed['Carrier Name'].nunique()}\")\n",
    "\n",
    "#print the number of unique values in the simplified 'Carrier_Name_Simplified' feature\n",
    "print(f\"Number of unique values in 'Carrier_Name_Simplified': {X_train_processed['Carrier_Name_Simplified'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the 'Carrier Name' column after creating 'Carrier_Name_Simplified'\n",
    "# X_train_processed = X_train_processed.drop(columns=['Carrier Name'])\n",
    "\n",
    "# # Apply to the val set\n",
    "# X_val_processed = X_val_processed.drop(columns=['Carrier Name'])\n",
    "\n",
    "# # Apply to the test set\n",
    "# df_test_processed = df_test_processed.drop(columns=['Carrier Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum all nan values from train, val, test\n",
    "X_train_processed.isnull().sum().sum(), X_val_processed.isnull().sum().sum(), df_test_processed.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13. Body_Part_Category <a class=\"anchor\" id=\"section_3_13\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "The Body_Part_Category feature will group the WCIO_Part_of_Body_Code into broader categories. Based on the codes in your document, each range of codes represents a specific body part region (e.g., codes from 10 to 19 represent the head). We‚Äôll map these codes to corresponding regions like ‚ÄúHead,‚Äù ‚ÄúNeck,‚Äù etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of WCIO Part of Body codes to broader categories\n",
    "part_of_body_mapping = {\n",
    "    **dict.fromkeys(range(10, 20), 'Head'),\n",
    "    **dict.fromkeys(range(20, 30), 'Neck'),\n",
    "    **dict.fromkeys(range(30, 40), 'Upper Extremities'),\n",
    "    **dict.fromkeys(list(range(40, 50)) + list(range(60, 64)), 'Trunk'),\n",
    "    **dict.fromkeys(range(50, 60), 'Lower Extremities'),\n",
    "    **dict.fromkeys([64, 65, 66, 90, 91, 99], 'Multiple Body Parts'),\n",
    "    **dict.fromkeys([101], 'NonClassificable'),\n",
    "    0: 'No_Code'  # Explicitly add 'No_Code' for values of 0\n",
    "}\n",
    "\n",
    "# Apply mapping to the training set\n",
    "X_train_processed['Body_Part_Category'] = X_train_processed['WCIO Part Of Body Code'].map(part_of_body_mapping)\n",
    "\n",
    "# Apply mapping to the validation set\n",
    "X_val_processed['Body_Part_Category'] = X_val_processed['WCIO Part Of Body Code'].map(part_of_body_mapping)\n",
    "\n",
    "# Apply mapping to the test set\n",
    "df_test_processed['Body_Part_Category'] = df_test_processed['WCIO Part Of Body Code'].map(part_of_body_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Upper Extremities      142559\n",
       "Lower Extremities       96343\n",
       "Trunk                   80866\n",
       "Head                    45491\n",
       "Multiple Body Parts     37241\n",
       "NonClassificable        33562\n",
       "No_Code                 13564\n",
       "Neck                     9593\n",
       "Name: Body_Part_Category, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed['Body_Part_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed['Body_Part_Category'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14. Injury_Nature_Category <a class=\"anchor\" id=\"section_3_14\"></a>\n",
    "The Body_Part_Category feature will group the WCIO_Part_of_Body_Code into broader categories. Based on the codes in the document, each range of codes represents a specific body part region (e.g., codes from 10 to 19 represent the head). We will map these codes to corresponding regions like \"Head,\" \"Neck,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of WCIO Nature of Injury codes to broader categories\n",
    "nature_of_injury_mapping = {\n",
    "    **dict.fromkeys(range(1, 60), 'Specific Injury'),\n",
    "    **dict.fromkeys(range(60, 81), 'Occupational Disease or Cumulative Injury'),\n",
    "    **dict.fromkeys([83], 'COVID-19 Injury'),\n",
    "    **dict.fromkeys([90, 91], 'Multiple Injuries'),\n",
    "    0: 'No_Code'  # Explicitly add 'No_Code' for values of 0\n",
    "}\n",
    "\n",
    "# Creating the Injury_Nature_Category column by mapping Nature of Injury codes to categories\n",
    "X_train_processed['Injury_Nature_Category'] = X_train_processed['WCIO Nature of Injury Code'].map(nature_of_injury_mapping)\n",
    "\n",
    "#Apply to the val set\n",
    "X_val_processed['Injury_Nature_Category'] = X_val_processed['WCIO Nature of Injury Code'].map(nature_of_injury_mapping)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Injury_Nature_Category'] = df_test_processed['WCIO Nature of Injury Code'].map(nature_of_injury_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Specific Injury                              404174\n",
       "COVID-19 Injury                               20663\n",
       "No_Code                                       12463\n",
       "Occupational Disease or Cumulative Injury     11439\n",
       "Multiple Injuries                             10480\n",
       "Name: Injury_Nature_Category, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed['Injury_Nature_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed['Injury_Nature_Category'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.15. Injury_Cause_Category <a class=\"anchor\" id=\"section_3_15\"></a>\n",
    "The Injury_Cause_Category feature will classify the WCIO_Cause_of_Injury_Code values into broader cause categories. For example, codes related to burns or scalds can be grouped together, as well as those for falls or motor vehicle accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of WCIO Cause of Injury codes to broader categories\n",
    "cause_of_injury_mapping = {\n",
    "    **dict.fromkeys(list(range(1, 10)) + [11, 14, 84], 'Burn or Scald'),\n",
    "    **dict.fromkeys([10, 12, 13, 20], 'Caught In, Under, or Between'),\n",
    "    **dict.fromkeys(list(range(15, 20)), 'Cut, Puncture, Scrape'),\n",
    "    **dict.fromkeys(list(range(25, 34)), 'Fall, Slip, or Trip'),\n",
    "    **dict.fromkeys(list(range(40, 51)), 'Motor Vehicle'),\n",
    "    **dict.fromkeys(list(range(52, 62)) + [97], 'Strain or Injury By'),\n",
    "    **dict.fromkeys(list(range(65, 71)), 'Striking Against or Stepping On'),\n",
    "    **dict.fromkeys(list(range(74, 82)) + [85, 86], 'Struck or Injured by'),\n",
    "    **dict.fromkeys(list(range(94, 96)), 'Rubbed or Abraded by'),\n",
    "    **dict.fromkeys(list(range(87, 94)) + [96, 98, 99, 82], 'Miscellaneous Causes'),\n",
    "    **dict.fromkeys([83], 'COVID-19 Injury'),\n",
    "    0: 'No_Code'  # Explicitly add 'No_Code' for values of 0\n",
    "}\n",
    "\n",
    "\n",
    "# Creating the Injury_Cause_Category column by mapping Cause of Injury codes to categories\n",
    "X_train_processed['Injury_Cause_Category'] = X_train_processed['WCIO Cause of Injury Code'].map(cause_of_injury_mapping)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Injury_Cause_Category'] = X_val_processed['WCIO Cause of Injury Code'].map(cause_of_injury_mapping)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Injury_Cause_Category'] = df_test_processed['WCIO Cause of Injury Code'].map(cause_of_injury_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Strain or Injury By                118515\n",
       "Fall, Slip, or Trip                 95955\n",
       "Struck or Injured by                80324\n",
       "Miscellaneous Causes                38713\n",
       "Cut, Puncture, Scrape               32369\n",
       "COVID-19 Injury                     20098\n",
       "Motor Vehicle                       19259\n",
       "Striking Against or Stepping On     15526\n",
       "Caught In, Under, or Between        15402\n",
       "No_Code                             12448\n",
       "Burn or Scald                        9833\n",
       "Rubbed or Abraded by                  777\n",
       "Name: Injury_Cause_Category, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed['Injury_Cause_Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have created new categorical features (Injury_Nature_Category, Body_Part_Category, Injury_Cause_Category) that provide a more meaningful representation of the original codes, it makes sense to remove the original code features. Keeping them would add redundancy, decrease interpretability, and unnecessarily increase the dimensionality of the dataset, potentially affecting model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removing the code features from train, validation, and test datasets\n",
    "# X_train_processed = X_train_processed.drop(columns=[\n",
    "#     'WCIO Cause of Injury Code',\n",
    "#     'WCIO Nature of Injury Code',\n",
    "#     'WCIO Part Of Body Code'\n",
    "# ])\n",
    "\n",
    "# X_val_processed = X_val_processed.drop(columns=[\n",
    "#     'WCIO Cause of Injury Code',\n",
    "#     'WCIO Nature of Injury Code',\n",
    "#     'WCIO Part Of Body Code'\n",
    "# ])\n",
    "\n",
    "# df_test_processed = df_test_processed.drop(columns=[\n",
    "#     'WCIO Cause of Injury Code',\n",
    "#     'WCIO Nature of Injury Code',\n",
    "#     'WCIO Part Of Body Code'\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.16. Risk of Each Job <a class=\"anchor\" id=\"section_3_16\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk = [11, 21, 23, 31, 32, 33, 48, 49]\n",
    "\n",
    "medium_risk = [22, 42, 44, 45, 56, 62, 71, 72, 81, 92]\n",
    "\n",
    "low_risk = [51, 52, 53, 54, 55, 61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Industry Code Industry Risk\n",
      "Claim Identifier                             \n",
      "6099734                    44.0   Medium Risk\n",
      "5796182                    44.0   Medium Risk\n",
      "6128237                    22.0   Medium Risk\n",
      "5394501                    48.0     High Risk\n",
      "5452763                    61.0      Low Risk\n",
      "                  Industry Code Industry Risk\n",
      "Claim Identifier                             \n",
      "6165911                    48.0     High Risk\n",
      "6166141                    45.0   Medium Risk\n",
      "6165907                    56.0   Medium Risk\n",
      "6166047                    48.0     High Risk\n",
      "6166102                    55.0      Low Risk\n"
     ]
    }
   ],
   "source": [
    "# Define a function to assign risk levels based on the industry code\n",
    "def assign_risk(industry_code):\n",
    "    if industry_code in high_risk:\n",
    "        return 'High Risk'\n",
    "    elif industry_code in medium_risk:\n",
    "        return 'Medium Risk'\n",
    "    elif industry_code in low_risk:\n",
    "        return 'Low Risk'\n",
    "    else:\n",
    "        return 'Unknown Risk'\n",
    "\n",
    "# Apply the function to create the 'Industry Risk' column for train and test datasets\n",
    "X_train_processed['Industry Risk'] = X_train_processed['Industry Code'].apply(assign_risk)\n",
    "X_val_processed['Industry Risk'] = X_val_processed['Industry Code'].apply(assign_risk)\n",
    "df_test_processed['Industry Risk'] = df_test['Industry Code'].apply(assign_risk)\n",
    "\n",
    "# Display a preview to verify\n",
    "print(X_train_processed[['Industry Code', 'Industry Risk']].head())\n",
    "print(df_test_processed[['Industry Code', 'Industry Risk']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALTERNATIVA POSS√çVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique industry descriptions\n",
    "#unique_industries = X_train_processed['Industry Code Description'].unique()\n",
    "#print(f\"Unique Industry Descriptions: {len(unique_industries)}\")\n",
    "#print(unique_industries[:10])  # Display the first 10 industry descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Industry Code Description' and calculate the frequency of claims\n",
    "#industry_injury_counts = X_train_processed.groupby('Industry Code Description')['Claim Injury Type'].count()\n",
    "\n",
    "# Normalize the injury frequencies to assign risk scores (1 = Low, 2 = Medium, 3 = High)\n",
    "#min_count = industry_injury_counts.min()\n",
    "#max_count = industry_injury_counts.max()\n",
    "#industry_injury_normalized = (industry_injury_counts - min_count) / (max_count - min_count)\n",
    "\n",
    "# Assign risk levels based on normalized frequencies\n",
    "#industry_risk_levels = industry_injury_normalized.apply(lambda x: 1 if x < 0.33 else (2 if x < 0.66 else 3))\n",
    "\n",
    "# Create a mapping dictionary\n",
    "#industry_risk_mapping = industry_risk_levels.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new \"Job Risk Level\" column to the dataset\n",
    "#X_train_processed['Job Risk Level'] = X_train_processed['Industry Code Description'].map(industry_risk_mapping)\n",
    "#X_val_processed['Job Risk Level'] = X_val_processed['Industry Code Description'].map(industry_risk_mapping)\n",
    "#df_test_processed['Job Risk Level'] = df_test_processed['Industry Code Description'].map(industry_risk_mapping)\n",
    "\n",
    "# Verify the new column\n",
    "#print(X_train_processed[['Industry Code Description', 'Job Risk Level']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.17. Relation between Salary and Dependents <a class=\"anchor\" id=\"section_3_17\"></a>\n",
    "[Back to 3. Feature Engineering ](#chapter3)<br>\n",
    "\n",
    "The variable `Salary_Per_Dependent` denotes the average salary allocated per dependent in a household. This metric may provide valuable insights into the financial responsibilities faced by individuals and families, as well as their potential correlation with the frequency and severity of injury claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature: dividing the salary (Average Weekly Wage) by the number of dependents\n",
    "X_train_processed['Salary_Per_Dependent'] = X_train_processed['Average Weekly Wage'] / (X_train_processed['Number of Dependents'] + 1)\n",
    "\n",
    "# Apply the same transformation to the validation set\n",
    "X_val_processed['Salary_Per_Dependent'] = X_val_processed['Average Weekly Wage'] / (X_val_processed['Number of Dependents'] + 1)\n",
    "\n",
    "# Apply the same transformation to the test set\n",
    "df_test_processed['Salary_Per_Dependent'] = df_test_processed['Average Weekly Wage'] / (df_test_processed['Number of Dependents'] +1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    459219.000000\n",
       "mean        457.819737\n",
       "std        1984.076454\n",
       "min           3.285714\n",
       "25%         194.666667\n",
       "50%         301.250000\n",
       "75%         557.000000\n",
       "max      707019.750000\n",
       "Name: Salary_Per_Dependent, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed['Salary_Per_Dependent'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Encoding <a class=\"anchor\" id=\"chapter5\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save Dataset for Modelling <a class=\"anchor\" id=\"chapter5\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully for Feature Selection & Modelling:\n",
      "Training features saved to: ../processed_datasets\\X_train_model.csv\n",
      "Validation features saved to: ../processed_datasets\\X_val_model.csv\n",
      "Test data saved to: ../processed_datasets\\df_test_model.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define output directory for the next notebook\n",
    "output_folder = \"../processed_datasets\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Paths for saving datasets\n",
    "X_train_path = os.path.join(output_folder, \"X_train_model.csv\")\n",
    "X_val_path = os.path.join(output_folder, \"X_val_model.csv\")\n",
    "df_test_path = os.path.join(output_folder, \"df_test_model.csv\")\n",
    "\n",
    "# Save X datasets to CSV\n",
    "X_train_processed.to_csv(X_train_path, index=True)\n",
    "X_val_processed.to_csv(X_val_path, index=True)\n",
    "df_test_processed.to_csv(df_test_path, index=True)\n",
    "\n",
    "\n",
    "# Confirmation messages\n",
    "print(\"Datasets saved successfully for Feature Selection & Modelling:\")\n",
    "print(f\"Training features saved to: {X_train_path}\")\n",
    "print(f\"Validation features saved to: {X_val_path}\")\n",
    "print(f\"Test data saved to: {df_test_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
