{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<h3 align=\"center\">Machine Learning - Project</h3>**\n",
    "## **<h3 align=\"center\">3. Level 2 Binary Classification</h3>**\n",
    "### **<h3 align=\"center\">Group 30 - Project</h3>**\n",
    "\n",
    "\n",
    "### Group Members\n",
    "| Name              | Email                        | Student ID |\n",
    "|-------------------|------------------------------|------------|\n",
    "| Alexandra Pinto   | 20211599@novaims.unl.pt      | 20211599   |\n",
    "| Gon√ßalo Peres     | 20211625@novaims.unl.pt      | 20211625   |\n",
    "| Leonor Mira       | 20240658@novaims.unl.pt      | 20240658   |\n",
    "| Miguel Nat√°rio    | 20240498@novaims.unl.pt      | 20240498   |\n",
    "| Nuno Bernardino   | 20211546@novaims.unl.pt      | 20211546    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **3. Level 2 Binary Classification Notebook**\n",
    "**Description:**\n",
    "This notebook focuses on the **Level 2 Binary Classification model**, which distinguishes between the two most common classes identified in Level 1:\n",
    "- **2 - NON-COMP**\n",
    "- **4 - TEMPORARY**\n",
    "\n",
    "Key steps include:\n",
    "- Loading the subset of **‚ÄúCommon‚Äù** cases from Level 1 predictions.\n",
    "- **Feature selection:** Tailor feature preprocessing and selection for this binary classification task.\n",
    "- **Model training:** Train and evaluate a binary classification model to distinguish between the two classes.\n",
    "- **Evaluation:** Use metrics like accuracy, precision, recall, and F1-score to measure performance.\n",
    "- **Output:** Save predictions for integration in the final notebook.\n",
    "\n",
    "This notebook refines the classification of cases within the most common classes, contributing to the pipeline's accuracy.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [1. Import the Libraries](#chapter1)\n",
    "* [2. Load and Prepare Datasets](#chapter2)\n",
    "* [3. Setting the Target](#chapter3)\n",
    "* [4. Feature Selection](#chapter4)\n",
    "    * [Scaling the Data](#section_4_1)  \n",
    "    * [Numerical Features](#section_4_2) \n",
    "    * [Categorical Features](#section_4_3) \n",
    "    * [Final Features](#section_4_3)\n",
    "* [5. Modelling](#chapter5)\n",
    "* [6. Loading the results](#chapter6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the Libraries üìö<a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import zipfile\n",
    "\n",
    "\n",
    "# --- Scikit-Learn Modules for Data Partitioning and Preprocessing ---\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# --- Feature Selection Methods ---\n",
    "# Filter Methods\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2, SelectKBest\n",
    "\n",
    "# Wrapper Methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Embedded Methods\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# --- Warnings ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#selecionar apenas as fun√ß√£o que vamos usar neste :)\n",
    "from utils import plot_importance, cor_heatmap, find_optimal_features_with_rfe, compare_rf_feature_importances,compare_feature_importances, select_high_score_features_chi2_no_model,select_high_score_features_MIC, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and Prepare Datasets üìÅ<a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Selection <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: Filtrar apenas as inst√¢ncias com classes 2 e 4 no conjunto de treino\n",
    "train_majority = X_train_processed[X_train_processed['claim injury type'].isin([2, 4])]\n",
    "val_majority = X_val_processed[X_val_processed['claim injury type'].isin([2, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying descriptive statistics for categorical features in the training dataset\n",
    "train_majority.describe(include='O').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the distribution of Income_Category\n",
    "train_majority['claim injury type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_majority.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setting the Target <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necess√°rias\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Passo 1: Filtrar apenas as inst√¢ncias com classes 2 e 4 no conjunto de treino\n",
    "train_majority = X_train_processed[X_train_processed['claim injury type'].isin([2, 4])]\n",
    "val_majority = X_val_processed[X_val_processed['claim injury type'].isin([2, 4])]\n",
    "\n",
    "# Separar as features (X) e o target (y) no conjunto de treino e valida√ß√£o\n",
    "X_train_bin = train_majority.drop(columns=['claim injury type'])  # Features de treino\n",
    "y_train_bin = train_majority['claim injury type']                # Target de treino\n",
    "\n",
    "X_val_bin = val_majority.drop(columns=['claim injury type'])      # Features de valida√ß√£o\n",
    "y_val_bin = val_majority['claim injury type']                    # Target de valida√ß√£o\n",
    "\n",
    "# Passo 2: Criar e treinar o modelo bin√°rio\n",
    "binary_model = RandomForestClassifier(random_state=42)\n",
    "binary_model.fit(X_train_bin, y_train_bin)\n",
    "\n",
    "# Avaliar o modelo na valida√ß√£o\n",
    "accuracy = binary_model.score(X_val_bin, y_val_bin)\n",
    "print(f\"Accuracy on validation set: {accuracy:.2f}\")\n",
    "\n",
    "# Passo 3: Aplicar o modelo ao conjunto de teste para as linhas da classe maiorit√°ria\n",
    "# Selecionar as linhas da classe \"majorit√°ria\" no conjunto de teste\n",
    "test_majority = df_test_processed[df_test_processed['class_category'] == 'majority']\n",
    "\n",
    "# Remover colunas desnecess√°rias para obter as features\n",
    "X_test_majority = test_majority.drop(columns=['class_category'])\n",
    "\n",
    "# Fazer previs√µes\n",
    "predictions = binary_model.predict(X_test_majority)\n",
    "\n",
    "# Adicionar as previs√µes ao dataset original\n",
    "df_test_processed.loc[test_majority.index, 'predicted_claim_injury_type'] = predictions\n",
    "\n",
    "# (Opcional) Verificar as previs√µes feitas\n",
    "print(df_test_processed['predicted_claim_injury_type'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
