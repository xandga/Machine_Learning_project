{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<h3 align=\"center\">Machine Learning - Project</h3>**\n",
    "## **<h3 align=\"center\">1. Preprocessing and Feature Engineering</h3>**\n",
    "### **<h3 align=\"center\">Group 30 - Project</h3>**\n",
    "\n",
    "\n",
    "### Group Members\n",
    "| Name              | Email                        | Student ID |\n",
    "|-------------------|------------------------------|------------|\n",
    "| Alexandra Pinto   | 20211599@novaims.unl.pt      | 20211599   |\n",
    "| Gonçalo Peres     | 20211625@novaims.unl.pt      | 20211625   |\n",
    "| Leonor Mira       | 20240658@novaims.unl.pt      | 20240658   |\n",
    "| Miguel Natário    | 20240498@novaims.unl.pt      | 20240498   |\n",
    "| Nuno Bernardino   | 2021546@novaims.unl.pt       | 2021546    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **1. Preprocessing and Feature Engineering Notebook**\n",
    "**Description:**\n",
    "In this notebook, as the name suggests, we will focus on **understanding and preparing the dataset** for the hierarchical classification pipeline. Key steps include:\n",
    "- **Exploratory Data Analysis (EDA):** Analyze the dataset for anomalies, missing values, and inconsistencies.\n",
    "- **Preprocessing:** Handle data cleaning, encoding, and scaling of features to create a baseline dataset.\n",
    "- **Feature Engineering:** Create new features that could enhance the predictive performance across all levels.\n",
    "- **Output:** Save the **preprocessed dataset** in a structured format (e.g., CSV or Pickle) for use in the subsequent steps:\n",
    "  - **Feature selection tailored to the target variable for each hierarchical model.**\n",
    "  - **Modeling and training each level of the hierarchical pipeline.**\n",
    "\n",
    "This notebook sets the foundation for the entire project, ensuring the data is clean, consistent, and enriched for downstream tasks. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [1. Import the Libraries](#chapter1)\n",
    "* [2. Import the Datasets](#chapter2)\n",
    "* [3. Explore the Dataset ](#chapter3)\n",
    "    * [3.1. Basic Statistics](#section_3_1)\n",
    "    * [3.2. Inconsistencies](#section_3_2)\n",
    "        * [3.2.1. Non Null Target](#sub_section_3_2_1)\n",
    "        * [3.2.2. Checking Combinations of Code](#sub_section_3_2_2)\n",
    "        * [3.2.3. Handling Average Weekly Wage Inconsistencies](#sub_section_3_2_3)\n",
    "        * [3.2.4. Handling Birth Year Inconsistencies](#sub_section_3_2_4)\n",
    "        * [3.2.5. Age at Injury vs. Birth Year](#sub_section_3_2_5)\n",
    "        * [3.2.6. Age at Injury](#sub_section_3_2_6)\n",
    "        * [3.2.7. First Hearing Date vs. Accident Date](#sub_section_3_2_7)\n",
    "        * [3.2.8. C2 Date vs. C3 Date vs. Accident Date](#sub_section_3_2_8)\n",
    "        * [3.2.9. Assembly Date vs. Accident Date](#sub_section_3_2_9)\n",
    "        * [3.2.10. Handling ZIP Code Format](#sub_section_3_2_10)\n",
    "        * [3.2.11. Gender Feature](#sub_section_3_2_11)\n",
    "        * [3.2.12. Overview of Inconsistencies](#sub_section_3_2_12)\n",
    "    * [3.3. Visualization](#section_3_3)  \n",
    "        * [3.3.1. Basic Plots](#sub_section_3_3_1)\n",
    "        * [3.3.2. Multivariate Analysis](#sub_section_3_3_2)        \n",
    "* [4. Train Test Split](#chapter4)\n",
    "* [5. Preprocessing the Data](#chapter5)\n",
    "    * [5.1. Outliers](#section_5_1) \n",
    "    * [5.2. Missing Values](#section_5_2)  \n",
    "    * [5.3. Categorical Features](#section_5_3) \n",
    "* [6. Feature Engineering](#chapter6)\n",
    "    * [6.1. Carrier-District Interaction](#section_6_1)\n",
    "    * [6.2. Income Category](#section_6_2)\n",
    "    * [6.3. Days_To_First_Hearing](#section_6_3)\n",
    "    * [6.4. Accident Quarter](#section_6_4)\n",
    "    * [6.5. Accident Year](#section_6_5)\n",
    "    * [6.6. Accident on Day and Weekend](#section_6_6)\n",
    "    * [6.7. Age Group](#section_6_7)\n",
    "    * [6.8. Time from Assembly Date to C-2 Filing](#section_6_8)\n",
    "    * [6.9. Time from Accident to C-2 Filing](#section_6_9)\n",
    "    * [6.10. Zip_Code_Simplified](#section_6_10)\n",
    "    * [6.11. Carrier Type Merged](#section_6_11)\n",
    "    * [6.12. Carrier_Name_Simplified](#section_6_12)\n",
    "    * [6.13. Body_Part_Category](#section_6_13)\n",
    "    * [6.14. Injury_Nature_Category](#section_6_14)\n",
    "    * [6.15. Injury_Cause_Category](#section_6_15)\n",
    "    * [6.16. Risk of Each Job](#section_6_16)\n",
    "* [7. Feature Selection](#chapter7)\n",
    "    * [Scaling the Data](#section_7_1)  \n",
    "    * [Numerical Features](#section_7_2) \n",
    "    * [Categorical Features](#section_7_3) \n",
    "    * [Final Features](#section_7_3)\n",
    "* [8. Modeling](#chapter8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1. Import the Libraries 📚<a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"update\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip update scikit-learn imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Invalid requirement: 'scikit-learn=1.1.3'\n",
      "Hint: = is not a valid operator. Did you mean == ?\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn=1.1.3 imbalanced-learn=0.10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'conda-forge'\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -c conda-forge imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in /Users/peresgoncalo/Library/Python/3.9/lib/python/site-packages (0.12.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/peresgoncalo/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/peresgoncalo/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/peresgoncalo/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/peresgoncalo/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/peresgoncalo/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn) (1.5.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import zipfile\n",
    "\n",
    "# --- Scikit-Learn Modules for Data Partitioning and Preprocessing ---\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- Feature Selection Methods ---\n",
    "# Filter Methods\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2, SelectKBest\n",
    "\n",
    "# Wrapper Methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Embedded Methods\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# --- Machine Learning Models ---\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Warnings ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Imbalanced Data Handling ---\n",
    "from imblearn.over_sampling import SMOTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and Prepare Datasets 📁<a class=\"anchor\" id=\"chapter2\"></a>\n",
    "Before importing the datasets, we unzip the data file to make it accessible for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "zip_path = r'../Data.zip'  # Adjust based on actual location\n",
    "extract_to_path = '../project_data'\n",
    "\n",
    "# Extract files\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_path)\n",
    "\n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed with loading the datasets.\n",
    "\n",
    "In the following cell, we import the train and test datasets. The `Claim Identifier` column is set as the index for both datasets to ensure unique identification of claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'/Users/peresgoncalo/Documents/Mestrado/C.U./ML/Project/project_data/train_data.csv', index_col='Claim Identifier')\n",
    "df_test = pd.read_csv(r'/Users/peresgoncalo/Documents/Mestrado/C.U./ML/Project/project_data/test_data.csv',index_col='Claim Identifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the imports really worked, we will use `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident Date</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Alternative Dispute Resolution</th>\n",
       "      <th>Assembly Date</th>\n",
       "      <th>Attorney/Representative</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>C-2 Date</th>\n",
       "      <th>C-3 Date</th>\n",
       "      <th>Carrier Name</th>\n",
       "      <th>...</th>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <th>WCIO Cause of Injury Description</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Description</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>WCIO Part Of Body Description</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Agreement Reached</th>\n",
       "      <th>WCB Decision</th>\n",
       "      <th>Number of Dependents</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5393875</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>31.0</td>\n",
       "      <td>N</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>N</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW HAMPSHIRE INSURANCE CO</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>FROM LIQUID OR GREASE SPILLS</td>\n",
       "      <td>10.0</td>\n",
       "      <td>CONTUSION</td>\n",
       "      <td>62.0</td>\n",
       "      <td>BUTTOCKS</td>\n",
       "      <td>13662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not Work Related</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393091</th>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>46.0</td>\n",
       "      <td>N</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Y</td>\n",
       "      <td>1745.93</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>ZURICH AMERICAN INSURANCE CO</td>\n",
       "      <td>...</td>\n",
       "      <td>97.0</td>\n",
       "      <td>REPETITIVE MOTION</td>\n",
       "      <td>49.0</td>\n",
       "      <td>SPRAIN OR TEAR</td>\n",
       "      <td>38.0</td>\n",
       "      <td>SHOULDER(S)</td>\n",
       "      <td>14569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not Work Related</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Accident Date  Age at Injury Alternative Dispute Resolution  \\\n",
       "Claim Identifier                                                               \n",
       "5393875             2019-12-30           31.0                              N   \n",
       "5393091             2019-08-30           46.0                              N   \n",
       "\n",
       "                 Assembly Date Attorney/Representative  Average Weekly Wage  \\\n",
       "Claim Identifier                                                              \n",
       "5393875             2020-01-01                       N                 0.00   \n",
       "5393091             2020-01-01                       Y              1745.93   \n",
       "\n",
       "                  Birth Year    C-2 Date    C-3 Date  \\\n",
       "Claim Identifier                                       \n",
       "5393875               1988.0  2019-12-31         NaN   \n",
       "5393091               1973.0  2020-01-01  2020-01-14   \n",
       "\n",
       "                                  Carrier Name  ... WCIO Cause of Injury Code  \\\n",
       "Claim Identifier                                ...                             \n",
       "5393875             NEW HAMPSHIRE INSURANCE CO  ...                      27.0   \n",
       "5393091           ZURICH AMERICAN INSURANCE CO  ...                      97.0   \n",
       "\n",
       "                 WCIO Cause of Injury Description WCIO Nature of Injury Code  \\\n",
       "Claim Identifier                                                               \n",
       "5393875              FROM LIQUID OR GREASE SPILLS                       10.0   \n",
       "5393091                         REPETITIVE MOTION                       49.0   \n",
       "\n",
       "                 WCIO Nature of Injury Description WCIO Part Of Body Code  \\\n",
       "Claim Identifier                                                            \n",
       "5393875                                  CONTUSION                   62.0   \n",
       "5393091                             SPRAIN OR TEAR                   38.0   \n",
       "\n",
       "                 WCIO Part Of Body Description Zip Code  Agreement Reached  \\\n",
       "Claim Identifier                                                             \n",
       "5393875                               BUTTOCKS    13662                0.0   \n",
       "5393091                            SHOULDER(S)    14569                1.0   \n",
       "\n",
       "                      WCB Decision Number of Dependents  \n",
       "Claim Identifier                                         \n",
       "5393875           Not Work Related                  1.0  \n",
       "5393091           Not Work Related                  4.0  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident Date</th>\n",
       "      <th>Age at Injury</th>\n",
       "      <th>Alternative Dispute Resolution</th>\n",
       "      <th>Assembly Date</th>\n",
       "      <th>Attorney/Representative</th>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>C-2 Date</th>\n",
       "      <th>C-3 Date</th>\n",
       "      <th>Carrier Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical Fee Region</th>\n",
       "      <th>OIICS Nature of Injury Description</th>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <th>WCIO Cause of Injury Description</th>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <th>WCIO Nature of Injury Description</th>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <th>WCIO Part Of Body Description</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Number of Dependents</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6165911</th>\n",
       "      <td>2022-12-24</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INDEMNITY INSURANCE CO OF</td>\n",
       "      <td>...</td>\n",
       "      <td>IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>FALL, SLIP OR TRIP, NOC</td>\n",
       "      <td>10.0</td>\n",
       "      <td>CONTUSION</td>\n",
       "      <td>54.0</td>\n",
       "      <td>LOWER LEG</td>\n",
       "      <td>10466</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166141</th>\n",
       "      <td>2022-11-20</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A I U INSURANCE COMPANY</td>\n",
       "      <td>...</td>\n",
       "      <td>IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>FALLING OR FLYING OBJECT</td>\n",
       "      <td>10.0</td>\n",
       "      <td>CONTUSION</td>\n",
       "      <td>10.0</td>\n",
       "      <td>MULTIPLE HEAD INJURY</td>\n",
       "      <td>11691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Accident Date  Age at Injury Alternative Dispute Resolution  \\\n",
       "Claim Identifier                                                               \n",
       "6165911             2022-12-24             19                              N   \n",
       "6166141             2022-11-20             19                              N   \n",
       "\n",
       "                 Assembly Date Attorney/Representative  Average Weekly Wage  \\\n",
       "Claim Identifier                                                              \n",
       "6165911             2023-01-02                       N                  NaN   \n",
       "6166141             2023-01-02                       N                  NaN   \n",
       "\n",
       "                  Birth Year    C-2 Date C-3 Date               Carrier Name  \\\n",
       "Claim Identifier                                                               \n",
       "6165911               2003.0  2023-01-02      NaN  INDEMNITY INSURANCE CO OF   \n",
       "6166141               2003.0  2023-01-02      NaN    A I U INSURANCE COMPANY   \n",
       "\n",
       "                  ... Medical Fee Region OIICS Nature of Injury Description  \\\n",
       "Claim Identifier  ...                                                         \n",
       "6165911           ...                 IV                                NaN   \n",
       "6166141           ...                 IV                                NaN   \n",
       "\n",
       "                 WCIO Cause of Injury Code WCIO Cause of Injury Description  \\\n",
       "Claim Identifier                                                              \n",
       "6165911                               31.0          FALL, SLIP OR TRIP, NOC   \n",
       "6166141                               75.0         FALLING OR FLYING OBJECT   \n",
       "\n",
       "                 WCIO Nature of Injury Code WCIO Nature of Injury Description  \\\n",
       "Claim Identifier                                                                \n",
       "6165911                                10.0                         CONTUSION   \n",
       "6166141                                10.0                         CONTUSION   \n",
       "\n",
       "                  WCIO Part Of Body Code  WCIO Part Of Body Description  \\\n",
       "Claim Identifier                                                          \n",
       "6165911                             54.0                      LOWER LEG   \n",
       "6166141                             10.0           MULTIPLE HEAD INJURY   \n",
       "\n",
       "                 Zip Code Number of Dependents  \n",
       "Claim Identifier                                \n",
       "6165911             10466                    1  \n",
       "6166141             11691                    1  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3. Explore the Dataset <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "In this chapter, we will perform initial exploratory steps to understand the structure and basic characteristics of our dataset.\n",
    "\n",
    "\n",
    "## 3.1. Basic Statistics 📊 <a class=\"anchor\" id=\"section_3_1\"></a>\n",
    "\n",
    "The `info()` method provides a summary of the dataframe, including column types, non-null counts, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 593471 entries, 5393875 to 818961390\n",
      "Data columns (total 32 columns):\n",
      " #   Column                              Non-Null Count   Dtype  \n",
      "---  ------                              --------------   -----  \n",
      " 0   Accident Date                       570337 non-null  object \n",
      " 1   Age at Injury                       574026 non-null  float64\n",
      " 2   Alternative Dispute Resolution      574026 non-null  object \n",
      " 3   Assembly Date                       593471 non-null  object \n",
      " 4   Attorney/Representative             574026 non-null  object \n",
      " 5   Average Weekly Wage                 545375 non-null  float64\n",
      " 6   Birth Year                          544948 non-null  float64\n",
      " 7   C-2 Date                            559466 non-null  object \n",
      " 8   C-3 Date                            187245 non-null  object \n",
      " 9   Carrier Name                        574026 non-null  object \n",
      " 10  Carrier Type                        574026 non-null  object \n",
      " 11  Claim Injury Type                   574026 non-null  object \n",
      " 12  County of Injury                    574026 non-null  object \n",
      " 13  COVID-19 Indicator                  574026 non-null  object \n",
      " 14  District Name                       574026 non-null  object \n",
      " 15  First Hearing Date                  150798 non-null  object \n",
      " 16  Gender                              574026 non-null  object \n",
      " 17  IME-4 Count                         132803 non-null  float64\n",
      " 18  Industry Code                       564068 non-null  float64\n",
      " 19  Industry Code Description           564068 non-null  object \n",
      " 20  Medical Fee Region                  574026 non-null  object \n",
      " 21  OIICS Nature of Injury Description  0 non-null       float64\n",
      " 22  WCIO Cause of Injury Code           558386 non-null  float64\n",
      " 23  WCIO Cause of Injury Description    558386 non-null  object \n",
      " 24  WCIO Nature of Injury Code          558369 non-null  float64\n",
      " 25  WCIO Nature of Injury Description   558369 non-null  object \n",
      " 26  WCIO Part Of Body Code              556944 non-null  float64\n",
      " 27  WCIO Part Of Body Description       556944 non-null  object \n",
      " 28  Zip Code                            545389 non-null  object \n",
      " 29  Agreement Reached                   574026 non-null  float64\n",
      " 30  WCB Decision                        574026 non-null  object \n",
      " 31  Number of Dependents                574026 non-null  float64\n",
      "dtypes: float64(11), object(21)\n",
      "memory usage: 149.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info() #1223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Some conclusions from the output above:*\n",
    "\n",
    "- Some data types need to be changed, such as:\n",
    "  - `'Accident Date'`, `'Assembly Date'`, `'C-2 Date'`, `'C-3 Date'`, and `'First Hearing Date'` can be converted to datetime.\n",
    "  - `'Industry Code'`, `'Zip Code'`, `'WCIO Part of Body Code'`, `'WCIO Cause of Injury Code'`, and `'WCIO Nature of Injury Code'` should be integers instead of floats. We will handle missing values before converting them.\n",
    "  - Similarly, `'Birth Year'` and `'Age at Injury'` will be converted after addressing missing values.\n",
    "- We have some missing values (which will be handled in Section 4.1).\n",
    "- The `'OIICS Nature of Injury Description'` column contains only missing values, so we can remove it from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime format\n",
    "date_columns = ['Accident Date', 'Assembly Date', 'C-2 Date', 'C-3 Date', 'First Hearing Date']\n",
    "for col in date_columns:\n",
    "    df_train[col] = pd.to_datetime(df_train[col], errors='coerce')\n",
    "    df_test[col] = pd.to_datetime(df_test[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column with only missing values in train and test datasets\n",
    "df_train.drop(columns=['OIICS Nature of Injury Description'], inplace=True)\n",
    "df_test.drop(columns=['OIICS Nature of Injury Description'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 593471 entries, 5393875 to 818961390\n",
      "Data columns (total 31 columns):\n",
      " #   Column                             Non-Null Count   Dtype         \n",
      "---  ------                             --------------   -----         \n",
      " 0   Accident Date                      570337 non-null  datetime64[ns]\n",
      " 1   Age at Injury                      574026 non-null  float64       \n",
      " 2   Alternative Dispute Resolution     574026 non-null  object        \n",
      " 3   Assembly Date                      593471 non-null  datetime64[ns]\n",
      " 4   Attorney/Representative            574026 non-null  object        \n",
      " 5   Average Weekly Wage                545375 non-null  float64       \n",
      " 6   Birth Year                         544948 non-null  float64       \n",
      " 7   C-2 Date                           559466 non-null  datetime64[ns]\n",
      " 8   C-3 Date                           187245 non-null  datetime64[ns]\n",
      " 9   Carrier Name                       574026 non-null  object        \n",
      " 10  Carrier Type                       574026 non-null  object        \n",
      " 11  Claim Injury Type                  574026 non-null  object        \n",
      " 12  County of Injury                   574026 non-null  object        \n",
      " 13  COVID-19 Indicator                 574026 non-null  object        \n",
      " 14  District Name                      574026 non-null  object        \n",
      " 15  First Hearing Date                 150798 non-null  datetime64[ns]\n",
      " 16  Gender                             574026 non-null  object        \n",
      " 17  IME-4 Count                        132803 non-null  float64       \n",
      " 18  Industry Code                      564068 non-null  float64       \n",
      " 19  Industry Code Description          564068 non-null  object        \n",
      " 20  Medical Fee Region                 574026 non-null  object        \n",
      " 21  WCIO Cause of Injury Code          558386 non-null  float64       \n",
      " 22  WCIO Cause of Injury Description   558386 non-null  object        \n",
      " 23  WCIO Nature of Injury Code         558369 non-null  float64       \n",
      " 24  WCIO Nature of Injury Description  558369 non-null  object        \n",
      " 25  WCIO Part Of Body Code             556944 non-null  float64       \n",
      " 26  WCIO Part Of Body Description      556944 non-null  object        \n",
      " 27  Zip Code                           545389 non-null  object        \n",
      " 28  Agreement Reached                  574026 non-null  float64       \n",
      " 29  WCB Decision                       574026 non-null  object        \n",
      " 30  Number of Dependents               574026 non-null  float64       \n",
      "dtypes: datetime64[ns](5), float64(10), object(16)\n",
      "memory usage: 144.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Summary:**\n",
    "- Key column types:\n",
    "  - Datetime columns (5): `'Accident Date'`, `'Assembly Date'`, `'C-2 Date'`, `'C-3 Date'`, and `'First Hearing Date'`.\n",
    "  - Float columns (4): Including `'Average Weekly Wage'`, `'Age at Injury'`, etc.\n",
    "  - Object columns (16): Including `'Carrier Name'`, `'District Name'`, etc.\n",
    "- Missing values are present in several columns, which will require further handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convert numerical columns stored as objects to float (if necessary)\n",
    "# df_train['Average Weekly Wage'] = pd.to_numeric(df_train['Average Weekly Wage'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` method provides a summary of the dataset's numerical columns, showing statistics such as mean, minimum, maximum, and standard deviation. This helps us identify potential outliers, unusual values, and data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accident Date</th>\n",
       "      <td>570337</td>\n",
       "      <td>2021-04-28 21:00:27.892982784</td>\n",
       "      <td>1961-09-06 00:00:00</td>\n",
       "      <td>2020-09-14 00:00:00</td>\n",
       "      <td>2021-06-27 00:00:00</td>\n",
       "      <td>2022-03-21 00:00:00</td>\n",
       "      <td>2023-09-29 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age at Injury</th>\n",
       "      <td>574026.0</td>\n",
       "      <td>42.11427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>14.256432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assembly Date</th>\n",
       "      <td>593471</td>\n",
       "      <td>2021-07-19 03:25:38.260841728</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>2020-10-26 00:00:00</td>\n",
       "      <td>2021-07-29 00:00:00</td>\n",
       "      <td>2022-04-19 00:00:00</td>\n",
       "      <td>2022-12-31 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Weekly Wage</th>\n",
       "      <td>545375.0</td>\n",
       "      <td>491.088321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>2828079.0</td>\n",
       "      <td>6092.91812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Birth Year</th>\n",
       "      <td>544948.0</td>\n",
       "      <td>1886.767604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>414.644423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C-2 Date</th>\n",
       "      <td>559466</td>\n",
       "      <td>2021-07-16 20:11:19.865443328</td>\n",
       "      <td>1996-01-12 00:00:00</td>\n",
       "      <td>2020-11-06 00:00:00</td>\n",
       "      <td>2021-08-09 00:00:00</td>\n",
       "      <td>2022-04-26 00:00:00</td>\n",
       "      <td>2024-06-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C-3 Date</th>\n",
       "      <td>187245</td>\n",
       "      <td>2021-07-26 21:31:28.449891840</td>\n",
       "      <td>1992-11-13 00:00:00</td>\n",
       "      <td>2020-10-27 00:00:00</td>\n",
       "      <td>2021-07-21 00:00:00</td>\n",
       "      <td>2022-04-20 00:00:00</td>\n",
       "      <td>2024-05-31 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Hearing Date</th>\n",
       "      <td>150798</td>\n",
       "      <td>2022-03-19 05:07:38.218278400</td>\n",
       "      <td>2020-01-30 00:00:00</td>\n",
       "      <td>2021-06-01 00:00:00</td>\n",
       "      <td>2022-03-09 00:00:00</td>\n",
       "      <td>2023-01-11 00:00:00</td>\n",
       "      <td>2024-06-07 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IME-4 Count</th>\n",
       "      <td>132803.0</td>\n",
       "      <td>3.207337</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.832303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Industry Code</th>\n",
       "      <td>564068.0</td>\n",
       "      <td>58.645305</td>\n",
       "      <td>11.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>19.644175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WCIO Cause of Injury Code</th>\n",
       "      <td>558386.0</td>\n",
       "      <td>54.381143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>25.874281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WCIO Nature of Injury Code</th>\n",
       "      <td>558369.0</td>\n",
       "      <td>41.013839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>22.207521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WCIO Part Of Body Code</th>\n",
       "      <td>556944.0</td>\n",
       "      <td>39.738146</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>22.36594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agreement Reached</th>\n",
       "      <td>574026.0</td>\n",
       "      <td>0.046665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Dependents</th>\n",
       "      <td>574026.0</td>\n",
       "      <td>3.006559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.000801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               count                           mean  \\\n",
       "Accident Date                 570337  2021-04-28 21:00:27.892982784   \n",
       "Age at Injury               574026.0                       42.11427   \n",
       "Assembly Date                 593471  2021-07-19 03:25:38.260841728   \n",
       "Average Weekly Wage         545375.0                     491.088321   \n",
       "Birth Year                  544948.0                    1886.767604   \n",
       "C-2 Date                      559466  2021-07-16 20:11:19.865443328   \n",
       "C-3 Date                      187245  2021-07-26 21:31:28.449891840   \n",
       "First Hearing Date            150798  2022-03-19 05:07:38.218278400   \n",
       "IME-4 Count                 132803.0                       3.207337   \n",
       "Industry Code               564068.0                      58.645305   \n",
       "WCIO Cause of Injury Code   558386.0                      54.381143   \n",
       "WCIO Nature of Injury Code  558369.0                      41.013839   \n",
       "WCIO Part Of Body Code      556944.0                      39.738146   \n",
       "Agreement Reached           574026.0                       0.046665   \n",
       "Number of Dependents        574026.0                       3.006559   \n",
       "\n",
       "                                            min                  25%  \\\n",
       "Accident Date               1961-09-06 00:00:00  2020-09-14 00:00:00   \n",
       "Age at Injury                               0.0                 31.0   \n",
       "Assembly Date               2020-01-01 00:00:00  2020-10-26 00:00:00   \n",
       "Average Weekly Wage                         0.0                  0.0   \n",
       "Birth Year                                  0.0               1965.0   \n",
       "C-2 Date                    1996-01-12 00:00:00  2020-11-06 00:00:00   \n",
       "C-3 Date                    1992-11-13 00:00:00  2020-10-27 00:00:00   \n",
       "First Hearing Date          2020-01-30 00:00:00  2021-06-01 00:00:00   \n",
       "IME-4 Count                                 1.0                  1.0   \n",
       "Industry Code                              11.0                 45.0   \n",
       "WCIO Cause of Injury Code                   1.0                 31.0   \n",
       "WCIO Nature of Injury Code                  1.0                 16.0   \n",
       "WCIO Part Of Body Code                     -9.0                 33.0   \n",
       "Agreement Reached                           0.0                  0.0   \n",
       "Number of Dependents                        0.0                  1.0   \n",
       "\n",
       "                                            50%                  75%  \\\n",
       "Accident Date               2021-06-27 00:00:00  2022-03-21 00:00:00   \n",
       "Age at Injury                              42.0                 54.0   \n",
       "Assembly Date               2021-07-29 00:00:00  2022-04-19 00:00:00   \n",
       "Average Weekly Wage                         0.0                841.0   \n",
       "Birth Year                               1977.0               1989.0   \n",
       "C-2 Date                    2021-08-09 00:00:00  2022-04-26 00:00:00   \n",
       "C-3 Date                    2021-07-21 00:00:00  2022-04-20 00:00:00   \n",
       "First Hearing Date          2022-03-09 00:00:00  2023-01-11 00:00:00   \n",
       "IME-4 Count                                 2.0                  4.0   \n",
       "Industry Code                              61.0                 71.0   \n",
       "WCIO Cause of Injury Code                  56.0                 75.0   \n",
       "WCIO Nature of Injury Code                 49.0                 52.0   \n",
       "WCIO Part Of Body Code                     38.0                 53.0   \n",
       "Agreement Reached                           0.0                  0.0   \n",
       "Number of Dependents                        3.0                  5.0   \n",
       "\n",
       "                                            max         std  \n",
       "Accident Date               2023-09-29 00:00:00         NaN  \n",
       "Age at Injury                             117.0   14.256432  \n",
       "Assembly Date               2022-12-31 00:00:00         NaN  \n",
       "Average Weekly Wage                   2828079.0  6092.91812  \n",
       "Birth Year                               2018.0  414.644423  \n",
       "C-2 Date                    2024-06-01 00:00:00         NaN  \n",
       "C-3 Date                    2024-05-31 00:00:00         NaN  \n",
       "First Hearing Date          2024-06-07 00:00:00         NaN  \n",
       "IME-4 Count                                73.0    2.832303  \n",
       "Industry Code                              92.0   19.644175  \n",
       "WCIO Cause of Injury Code                  99.0   25.874281  \n",
       "WCIO Nature of Injury Code                 91.0   22.207521  \n",
       "WCIO Part Of Body Code                     99.0    22.36594  \n",
       "Agreement Reached                           1.0    0.210921  \n",
       "Number of Dependents                        6.0    2.000801  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- An unusual minimum value of `-9` is observed in the `WCIO Part of Body Code` feature. We will investigate this further.\n",
    "- Some columns have `NaN` for certain statistics, indicating potential missing or undefined values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WCIO Part Of Body Description\n",
       "MULTIPLE    42011\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate rows where 'WCIO Part Of Body Code' is -9 to understand the corresponding description\n",
    "df_train[df_train['WCIO Part Of Body Code'] == -9]['WCIO Part Of Body Description'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `-9` value represents a specific category (like \"Unknown\" or \"Multiple\"), we might choose to treat it accordingly in our analysis. Otherwise, it may need further cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WCIO Part Of Body Code\n",
       "-9.0    42011\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if Multiple is associated with other different code\n",
    "df_train[df_train['WCIO Part Of Body Description'] == 'MULTIPLE']['WCIO Part Of Body Code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified a negative value (`-9`) in the `WCIO Part of Body Code` column and confirmed that it is not a valid code. Since all occurrences of `-9` correspond to the \"Multiple\" category in the description, we decided to replace it with a new code, `101`, to retain the information. We also changed the description from \"Multiple\" to \"Nonclassifiable\" to reflect this adjustment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negative code (-9) with 101 and update description to 'Nonclassifiable'\n",
    "df_train.loc[df_train['WCIO Part Of Body Code'] < 0, 'WCIO Part Of Body Code'] = 101\n",
    "df_train.loc[df_train['WCIO Part Of Body Code'] == 101, 'WCIO Part Of Body Description'] = 'Nonclassifiable'\n",
    "\n",
    "# Apply the same transformation to the test dataset\n",
    "df_test.loc[df_test['WCIO Part Of Body Code'] < 0, 'WCIO Part Of Body Code'] = 101\n",
    "df_test.loc[df_test['WCIO Part Of Body Code'] == 101, 'WCIO Part Of Body Description'] = 'Nonclassifiable'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We chose `101` as the new code for `WCIO Part of Body Code` values of `-9` to ensure they are treated as a distinct category in further analysis, without conflicting with existing valid codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alternative Dispute Resolution</th>\n",
       "      <td>574026</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>571412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attorney/Representative</th>\n",
       "      <td>574026</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>392291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carrier Name</th>\n",
       "      <td>574026</td>\n",
       "      <td>2046</td>\n",
       "      <td>STATE INSURANCE FUND</td>\n",
       "      <td>111144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carrier Type</th>\n",
       "      <td>574026</td>\n",
       "      <td>8</td>\n",
       "      <td>1A. PRIVATE</td>\n",
       "      <td>285368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Injury Type</th>\n",
       "      <td>574026</td>\n",
       "      <td>8</td>\n",
       "      <td>2. NON-COMP</td>\n",
       "      <td>291078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>County of Injury</th>\n",
       "      <td>574026</td>\n",
       "      <td>63</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>60430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COVID-19 Indicator</th>\n",
       "      <td>574026</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>546505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>District Name</th>\n",
       "      <td>574026</td>\n",
       "      <td>8</td>\n",
       "      <td>NYC</td>\n",
       "      <td>270779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>574026</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>335218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Industry Code Description</th>\n",
       "      <td>564068</td>\n",
       "      <td>20</td>\n",
       "      <td>HEALTH CARE AND SOCIAL ASSISTANCE</td>\n",
       "      <td>114339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medical Fee Region</th>\n",
       "      <td>574026</td>\n",
       "      <td>5</td>\n",
       "      <td>IV</td>\n",
       "      <td>265981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WCIO Cause of Injury Description</th>\n",
       "      <td>558386</td>\n",
       "      <td>74</td>\n",
       "      <td>LIFTING</td>\n",
       "      <td>46610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WCIO Nature of Injury Description</th>\n",
       "      <td>558369</td>\n",
       "      <td>56</td>\n",
       "      <td>STRAIN OR TEAR</td>\n",
       "      <td>153373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WCIO Part Of Body Description</th>\n",
       "      <td>556944</td>\n",
       "      <td>54</td>\n",
       "      <td>LOWER BACK AREA</td>\n",
       "      <td>51862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zip Code</th>\n",
       "      <td>545389</td>\n",
       "      <td>10060</td>\n",
       "      <td>11236</td>\n",
       "      <td>3302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WCB Decision</th>\n",
       "      <td>574026</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Work Related</td>\n",
       "      <td>574026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    count unique  \\\n",
       "Alternative Dispute Resolution     574026      3   \n",
       "Attorney/Representative            574026      2   \n",
       "Carrier Name                       574026   2046   \n",
       "Carrier Type                       574026      8   \n",
       "Claim Injury Type                  574026      8   \n",
       "County of Injury                   574026     63   \n",
       "COVID-19 Indicator                 574026      2   \n",
       "District Name                      574026      8   \n",
       "Gender                             574026      4   \n",
       "Industry Code Description          564068     20   \n",
       "Medical Fee Region                 574026      5   \n",
       "WCIO Cause of Injury Description   558386     74   \n",
       "WCIO Nature of Injury Description  558369     56   \n",
       "WCIO Part Of Body Description      556944     54   \n",
       "Zip Code                           545389  10060   \n",
       "WCB Decision                       574026      1   \n",
       "\n",
       "                                                                 top    freq  \n",
       "Alternative Dispute Resolution                                     N  571412  \n",
       "Attorney/Representative                                            N  392291  \n",
       "Carrier Name                                    STATE INSURANCE FUND  111144  \n",
       "Carrier Type                                             1A. PRIVATE  285368  \n",
       "Claim Injury Type                                        2. NON-COMP  291078  \n",
       "County of Injury                                             SUFFOLK   60430  \n",
       "COVID-19 Indicator                                                 N  546505  \n",
       "District Name                                                    NYC  270779  \n",
       "Gender                                                             M  335218  \n",
       "Industry Code Description          HEALTH CARE AND SOCIAL ASSISTANCE  114339  \n",
       "Medical Fee Region                                                IV  265981  \n",
       "WCIO Cause of Injury Description                             LIFTING   46610  \n",
       "WCIO Nature of Injury Description                     STRAIN OR TEAR  153373  \n",
       "WCIO Part Of Body Description                        LOWER BACK AREA   51862  \n",
       "Zip Code                                                       11236    3302  \n",
       "WCB Decision                                        Not Work Related  574026  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe(include=['O']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `describe()` method for object-type features, we gained insights into categorical data. Key observations:\n",
    "\n",
    "- `Claim Injury Type` and `Carrier Type` have numeric values that could be recoded or binned if necessary.\n",
    "- Some features, like `COVID-19 Indicator` and `WCB Decision`, have only two unique values, indicating they could be converted to binary format.\n",
    "- `WCB Decision` contains only a single unique value across all records, suggesting it might not add value to the analysis and warrants further investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WCB Decision\n",
       "Not Work Related    574026\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of unique values in 'WCB Decision' to assess its utility\n",
    "df_train['WCB Decision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(19445)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['WCB Decision'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `WCB Decision` contains only a single unique value, we will choose to drop this column in the data cleaning process as it does not provide any variability for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Accident Date', 'Age at Injury', 'Alternative Dispute Resolution',\n",
       "       'Assembly Date', 'Attorney/Representative', 'Average Weekly Wage',\n",
       "       'Birth Year', 'C-2 Date', 'C-3 Date', 'Carrier Name', 'Carrier Type',\n",
       "       'County of Injury', 'COVID-19 Indicator', 'District Name',\n",
       "       'First Hearing Date', 'Gender', 'IME-4 Count', 'Industry Code',\n",
       "       'Industry Code Description', 'Medical Fee Region',\n",
       "       'WCIO Cause of Injury Code', 'WCIO Cause of Injury Description',\n",
       "       'WCIO Nature of Injury Code', 'WCIO Nature of Injury Description',\n",
       "       'WCIO Part Of Body Code', 'WCIO Part Of Body Description', 'Zip Code',\n",
       "       'Number of Dependents'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['WCB Decision'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's clean the **text variables**.\n",
    "\n",
    "Now, we’ll clean the text variables by removing numeric prefixes from `Carrier Type` and `Claim Injury Type` to make the values more interpretable and consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Carrier Type\n",
       "1A. PRIVATE                                    285368\n",
       "3A. SELF PUBLIC                                121920\n",
       "2A. SIF                                        111144\n",
       "4A. SELF PRIVATE                                52788\n",
       "UNKNOWN                                          1774\n",
       "5D. SPECIAL FUND - UNKNOWN                       1023\n",
       "5C. SPECIAL FUND - POI CARRIER WCB MENANDS          5\n",
       "5A. SPECIAL FUND - CONS. COMM. (SECT. 25-A)         4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Carrier Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Claim Injury Type\n",
       "2. NON-COMP        291078\n",
       "4. TEMPORARY       148507\n",
       "3. MED ONLY         68906\n",
       "5. PPD SCH LOSS     48280\n",
       "1. CANCELLED        12477\n",
       "6. PPD NSL           4211\n",
       "8. DEATH              470\n",
       "7. PTD                 97\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Claim Injury Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numeric prefix from 'Carrier Type' in df_train\n",
    "df_train['Carrier Type'] = df_train['Carrier Type'].apply(lambda x: x.split('.', 1)[1].strip() if pd.notna(x) and x.lower() != 'unknown' else x)\n",
    "\n",
    "# Apply the same transformation to the test set\n",
    "df_test['Carrier Type'] = df_test['Carrier Type'].apply(lambda x: x.split('.', 1)[1].strip() if pd.notna(x) and x.lower() != 'unknown' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Changes:**\n",
    "- Removed numeric prefixes from `Carrier Type` for clarity.\n",
    "- Both variables now contain only descriptive text, making them easier to interpret in further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check for the **binary ones**.\n",
    "Let's examine the binary variables in our dataset to ensure they are clean and consistent. For variables with unexpected values, such as `Alternative Dispute Resolution`, we will decide on the best way to handle them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alternative Dispute Resolution\n",
       "N    571412\n",
       "Y      2609\n",
       "U         5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Alternative Dispute Resolution'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only 5 rows where `Alternative Dispute Resolution` is `'U'`, we have two options: either delete these rows or assign `'NaN'` to these values. \n",
    "\n",
    "In this case, we’ll assign as NaN values, to avoid deleting rows in the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'U' with NaN in 'Alternative Dispute Resolution' column for training set\n",
    "df_train['Alternative Dispute Resolution'] = df_train['Alternative Dispute Resolution'].replace('U', np.nan)\n",
    "\n",
    "# Apply the same change to the test set\n",
    "df_test['Alternative Dispute Resolution'] = df_test['Alternative Dispute Resolution'].replace('U', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attorney/Representative\n",
       "N    392291\n",
       "Y    181735\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Attorney/Representative'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COVID-19 Indicator\n",
       "N    546505\n",
       "Y     27521\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['COVID-19 Indicator'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To standardize our binary columns, we will map `N` to `0` and `Y` to `1` in columns such as `Attorney/Representative`, `COVID-19 Indicator`, and `Alternative Dispute Resolution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map binary columns to 0 and 1 in training data\n",
    "binary_columns = ['Attorney/Representative', 'COVID-19 Indicator', 'Alternative Dispute Resolution']\n",
    "for col in binary_columns:\n",
    "    df_train[col] = df_train[col].map({'N': 0, 'Y': 1})\n",
    "    \n",
    "    # Apply the same transformation to the test set\n",
    "    df_test[col] = df_test[col].map({'N': 0, 'Y': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `var()` method allows us to check if any numerical variables are univariate (variance is equal to 0). Columns with zero variance provide no predictive power and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age at Injury                     2.032459e+02\n",
       "Alternative Dispute Resolution    4.524480e-03\n",
       "Attorney/Representative           2.163638e-01\n",
       "Average Weekly Wage               3.712365e+07\n",
       "Birth Year                        1.719300e+05\n",
       "COVID-19 Indicator                4.564529e-02\n",
       "IME-4 Count                       8.021942e+00\n",
       "Industry Code                     3.858936e+02\n",
       "WCIO Cause of Injury Code         6.694784e+02\n",
       "WCIO Nature of Injury Code        4.931740e+02\n",
       "WCIO Part Of Body Code            5.353025e+02\n",
       "Agreement Reached                 4.448758e-02\n",
       "Number of Dependents              4.003206e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only numerical columns for variance check\n",
    "numerical_cols = df_train.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate variance of each numerical column\n",
    "numerical_cols.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove rows that have missing values in our target variable, `Claim Injury Type`, as they are not useful for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the target column ('Claim Injury Type')\n",
    "df_train = df_train.dropna(subset=['Claim Injury Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Variables Summary**\n",
    "\n",
    "In this section, we performed the following steps to clean and standardize the binary variables in our dataset:\n",
    "\n",
    "1. **Identification and Mapping of Binary Columns**:\n",
    "   - We identified `Attorney/Representative`, `COVID-19 Indicator`, and `Alternative Dispute Resolution` as binary columns with values of `N` and `Y`.\n",
    "   - To standardize these columns, we mapped `N` to `0` and `Y` to `1` for consistency and easier processing in future analyses.\n",
    "\n",
    "2. **Handling Irregular Values in `Alternative Dispute Resolution`**:\n",
    "   - The `Alternative Dispute Resolution` column contained five rows with the value `'U'`, which did not fit the binary `N`/`Y` format.\n",
    "   - Given the small number of occurrences, we chose to remove these rows from both the training and test datasets to maintain binary consistency.\n",
    "\n",
    "3. **Variance Check for Numerical Columns**:\n",
    "   - We calculated the variance of all numerical columns to identify any that were univariate (variance equal to 0).\n",
    "   - Columns with zero variance provide no predictive value and could be candidates for removal in future steps.\n",
    "\n",
    "4. **Handling Missing Values in Target Variable**:\n",
    "   - We removed rows with missing values in the target variable, `Claim Injury Type`, as these rows would not contribute to the model’s training and evaluation.\n",
    "\n",
    "These cleaning steps ensure that our binary variables are consistent and prepared for further analysis, while unnecessary data rows are removed to maintain data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Inconsistencies <a class=\"anchor\" id=\"section_3_2\"></a>\n",
    "In this section, we will identify and address data inconsistencies to enhance the quality of our dataset for analysis. Specifically, we will ensure that the target variable, `Claim Injury Type`, has no missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df_train to work on without altering the original data\n",
    "df = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Non Null Target <a class=\"anchor\" id=\"sub_section_3_2_1\"></a>\n",
    "\n",
    "Since `Claim Injury Type` is our target variable, we need to ensure there are no missing values in this column, as they could affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Claim Injury Type'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows `0` missing values in `Claim Injury Type`, confirming that our target variable is complete.\n",
    "\n",
    "Although no missing values were found, we include a line to drop rows with NaN in `Claim Injury Type` as a precaution in case of future updates to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in 'Claim Injury Type' (if any exist)\n",
    "df = df.dropna(subset=['Claim Injury Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### 3.2.2. Checking Combinations of Code and Description <a class=\"anchor\" id=\"sub_section_3_2_2\"></a>\n",
    "We need to verify that each `Code` corresponds uniquely to its respective `Description`. To achieve this, we created a function that takes the DataFrame, code column, and description column as inputs. The function counts the number of unique code-description combinations and compares it to the individual counts of the code and description columns. It also checks if the total unique combinations match the counts of each column, identifying any inconsistencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### função ja está no utils\n",
    "from utils import check_code_description_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This function helps ensure data consistency by verifying that each code uniquely maps to a single description. It’s particularly useful for categorical data where one-to-one mapping between two columns is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Cause of Injury Code and  Description\n",
    "\n",
    "We need to verify that each `Code` uniquely corresponds to its respective `Description`. This ensures consistency and avoids ambiguities in our dataset. To achieve this, we created a function that checks the number of unique code-description combinations and compares it to the individual counts of each column. If the counts do not match, it indicates potential inconsistencies, which we explore further to understand the cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique combinations of WCIO Cause of Injury Code and WCIO Cause of Injury Description: 78\n",
      "Total unique WCIO Cause of Injury Code values: 77\n",
      "Total unique WCIO Cause of Injury Description values: 74\n",
      "There is a discrepancy between the number of unique combinations and the total counts of WCIO Cause of Injury Code and WCIO Cause of Injury Description.\n"
     ]
    }
   ],
   "source": [
    "# Call the function for different Code and Description columns\n",
    "check_code_description_combinations(df, 'WCIO Cause of Injury Code', 'WCIO Cause of Injury Description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that there is a discrepancy between the total number of unique combinations of `WCIO Cause of Injury Code` and `WCIO Cause of Injury Description` and the individual counts of each column. This discrepancy suggests that some descriptions may be linked to multiple codes, which we investigate further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description 'OBJECT BEING LIFTED OR HANDLED' is linked to 3 different codes.\n",
      "\n",
      "\n",
      "                  WCIO Cause of Injury Code WCIO Cause of Injury Description\n",
      "Claim Identifier                                                            \n",
      "5393889                                79.0   OBJECT BEING LIFTED OR HANDLED\n",
      "5393948                                17.0   OBJECT BEING LIFTED OR HANDLED\n",
      "5393880                                66.0   OBJECT BEING LIFTED OR HANDLED\n",
      "Description 'REPETITIVE MOTION' is linked to 2 different codes.\n",
      "\n",
      "\n",
      "                  WCIO Cause of Injury Code WCIO Cause of Injury Description\n",
      "Claim Identifier                                                            \n",
      "5393091                                97.0                REPETITIVE MOTION\n",
      "5398012                                94.0                REPETITIVE MOTION\n"
     ]
    }
   ],
   "source": [
    "# Loop through each unique description and check if it maps to multiple codes\n",
    "for description, group in df.groupby('WCIO Cause of Injury Description'):\n",
    "    unique_codes = group['WCIO Cause of Injury Code'].nunique()\n",
    "    \n",
    "    # If the description is linked to more than one code, print details\n",
    "    if unique_codes > 1:\n",
    "        print(f\"Description '{description}' is linked to {unique_codes} different codes.\")\n",
    "        print(\"\\n\")\n",
    "        print(group[['WCIO Cause of Injury Code', 'WCIO Cause of Injury Description']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "Our analysis confirmed that certain injury descriptions legitimately map to multiple codes. This structure aligns with the original reference data and indicates that the variations are intentional. We can proceed with confidence that our `Code` and `Description` columns are consistent with each other.<br>\n",
    "Let's do the same for the others Code and Description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Nature of Injury Code and Description\n",
    "\n",
    "We will verify if each `WCIO Nature of Injury Code` corresponds uniquely to its respective `WCIO Nature of Injury Description`. This check helps us ensure consistency in the code-description relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique combinations of WCIO Nature of Injury Code and WCIO Nature of Injury Description: 57\n",
      "Total unique WCIO Nature of Injury Code values: 56\n",
      "Total unique WCIO Nature of Injury Description values: 56\n",
      "There is a discrepancy between the number of unique combinations and the total counts of WCIO Nature of Injury Code and WCIO Nature of Injury Description.\n"
     ]
    }
   ],
   "source": [
    "check_code_description_combinations(df, 'WCIO Nature of Injury Code', 'WCIO Nature of Injury Description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates a discrepancy between the unique combinations of `WCIO Nature of Injury Code` and `WCIO Nature of Injury Description`. This discrepancy likely arises due to `NaN` values, which can create additional unique combinations.\n",
    "\n",
    "Since `NaN` values do not provide meaningful information, they do not impact the integrity of the code-description relationship for analysis purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any descriptions are linked to multiple codes\n",
    "for description, group in df.groupby('WCIO Nature of Injury Description'):\n",
    "    unique_codes = group['WCIO Nature of Injury Code'].nunique()\n",
    "    \n",
    "    # If a description is linked to more than one code, print the details\n",
    "    if unique_codes > 1:\n",
    "        print(f\"Description '{description}' is linked to {unique_codes} different codes.\")\n",
    "        print(\"\\n\")\n",
    "        print(group[['WCIO Nature of Injury Code', 'WCIO Nature of Injury Description']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The loop did not return any output because all the discrepancies involve `NaN` values. No descriptions were found to be linked to multiple codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** This analysis confirms that the `WCIO Nature of Injury Code` and `WCIO Nature of Injury Description` pairs are consistent, with the discrepancy attributed solely to `NaN` values. We can proceed with confidence that this column pair is accurately mapped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Part Of Body Code and Description\n",
    "\n",
    "We will check if each `WCIO Part Of Body Code` uniquely corresponds to a single `WCIO Part Of Body Description`. This is essential to ensure that the descriptions consistently match their respective codes without ambiguity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique combinations of WCIO Part Of Body Code and WCIO Part Of Body Description: 58\n",
      "Total unique WCIO Part Of Body Code values: 57\n",
      "Total unique WCIO Part Of Body Description values: 54\n",
      "There is a discrepancy between the number of unique combinations and the total counts of WCIO Part Of Body Code and WCIO Part Of Body Description.\n"
     ]
    }
   ],
   "source": [
    "check_code_description_combinations(df_train, 'WCIO Part Of Body Code', 'WCIO Part Of Body Description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows a discrepancy between the total unique combinations of `WCIO Part Of Body Code` and `WCIO Part Of Body Description` and the individual counts of each column. Further investigation reveals that certain descriptions, such as `'DISC'`, `'SOFT TISSUE'`, and `'SPINAL CORD'`, are associated with multiple codes. This may indicate a broader categorization where each code represents specific contexts within the same general description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description 'DISC' is linked to 2 different codes.\n",
      "\n",
      "\n",
      "                  WCIO Part Of Body Code WCIO Part Of Body Description\n",
      "Claim Identifier                                                      \n",
      "5393781                             43.0                          DISC\n",
      "5394136                             22.0                          DISC\n",
      "Description 'SOFT TISSUE' is linked to 2 different codes.\n",
      "\n",
      "\n",
      "                  WCIO Part Of Body Code WCIO Part Of Body Description\n",
      "Claim Identifier                                                      \n",
      "5393980                             18.0                   SOFT TISSUE\n",
      "5394179                             25.0                   SOFT TISSUE\n",
      "Description 'SPINAL CORD' is linked to 2 different codes.\n",
      "\n",
      "\n",
      "                  WCIO Part Of Body Code WCIO Part Of Body Description\n",
      "Claim Identifier                                                      \n",
      "5393898                             23.0                   SPINAL CORD\n",
      "5397921                             47.0                   SPINAL CORD\n"
     ]
    }
   ],
   "source": [
    "for description, group in df.groupby('WCIO Part Of Body Description'):\n",
    "    unique_codes = group['WCIO Part Of Body Code'].nunique()\n",
    "    if unique_codes > 1:\n",
    "        print(f\"Description '{description}' is linked to {unique_codes} different codes.\")\n",
    "        print(\"\\n\")\n",
    "        print(group[['WCIO Part Of Body Code', 'WCIO Part Of Body Description']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions `'DISC'`, `'SOFT TISSUE'`, and `'SPINAL CORD'` are linked to multiple codes, as shown above. Upon reviewing the original reference table, we confirmed that this is correct. Each code corresponds to a specific part of the body or injury type, explaining these variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop description columns from both training and test datasets\n",
    "df = df.drop(['WCIO Cause of Injury Description', 'WCIO Nature of Injury Description', 'WCIO Part Of Body Description', 'Industry Code Description'], axis=1)\n",
    "\n",
    "# Apply the same drop to the test set\n",
    "df_test = df_test.drop(['WCIO Cause of Injury Description', 'WCIO Nature of Injury Description', 'WCIO Part Of Body Description', 'Industry Code Description'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.  Handling Average Weekly Wage Inconsistencies <a class=\"anchor\" id=\"sub_section_3_2_3\"></a>\n",
    "\n",
    "The `Average Weekly Wage` variable is expected to have positive values, as it represents wages related to job insurances. Negative values would be illogical, and zero values may indicate missing or placeholder data that requires further investigation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check if there are any negative values in the `Average Weekly Wage` column, as these would be logically incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative wages: 0\n"
     ]
    }
   ],
   "source": [
    "# Average Weekly Wage: Check for negative or extremely high wages\n",
    "wage_neg = df[df['Average Weekly Wage'] < 0]\n",
    "\n",
    "print(f\"Number of negative wages: {wage_neg.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative wages: 0\n"
     ]
    }
   ],
   "source": [
    "# Average Weekly Wage: Check for negative or extremely high wages\n",
    "wage_neg = df_test[df_test['Average Weekly Wage'] < 0]\n",
    "\n",
    "print(f\"Number of negative wages: {wage_neg.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check for values equal to zero. While zero wages may not make sense in this context, they might represent missing data or placeholders that require further attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero wages: 335450\n"
     ]
    }
   ],
   "source": [
    "# Check for zero values in 'Average Weekly Wage', which may indicate placeholders or missing data\n",
    "wage_null = df[df['Average Weekly Wage'] == 0]\n",
    "print(f\"Number of zero wages: {wage_null.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero wages: 316549\n"
     ]
    }
   ],
   "source": [
    "# Check for zero values in 'Average Weekly Wage', which may indicate placeholders or missing data\n",
    "wage_null = df_test[df_test['Average Weekly Wage'] == 0]\n",
    "print(f\"Number of zero wages: {wage_null.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387975, 24)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a significant number of entries with zero wages (335,446 records). Given the nature of this dataset, zero wages are likely placeholders or missing values rather than actual wage data. We may consider imputing these values based on other relevant columns or filtering them out, depending on their impact on the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Handling Birth Year Inconsistencies <a class=\"anchor\" id=\"sub_section_3_2_4\"></a>\n",
    "\n",
    "For the `Birth Year` variable, we expect realistic values. This means:\n",
    "- Birth years should not be earlier than 1900, as records before this would be highly unusual.\n",
    "- Birth years should not be beyond a certain limit (e.g., 2006) to account for minimum working ages.\n",
    "\n",
    "Additionally, any entries with a birth year of `0` will be considered missing, as it is impossible for a person to have a birth year of zero.\n",
    "\n",
    "First, we replace all instances of `0` in the `Birth Year` column with `NaN`, treating them as missing values. This helps us handle these entries more effectively in later data processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0 values in 'Birth Year' with NaN using np.where\n",
    "df['Birth Year'] = np.where(df['Birth Year'] == 0, np.nan, df['Birth Year'])\n",
    "\n",
    "#Apply to the test set\n",
    "df_test['Birth Year'] = np.where(df_test['Birth Year'] == 0, np.nan, df_test['Birth Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check for entries with unrealistic birth years (those before 1940 or after 2006). These likely represent erroneous entries, as they fall outside the expected range for working individuals in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unrealistic birth years: 874\n",
      "Number of unrealistic birth years: 984\n"
     ]
    }
   ],
   "source": [
    "# Identify unrealistic Birth Year entries (e.g., before 1940 or after 2006)\n",
    "birth_year= df[(df['Birth Year'] < 1940) | (df['Birth Year'] > 2006)]\n",
    "print(f\"Number of unrealistic birth years: {birth_year.shape[0]}\")\n",
    "\n",
    "# Identify unrealistic Birth Year entries (e.g., before 1940 or after 2006)\n",
    "birth_year_test= df_test[(df_test['Birth Year'] < 1940) | (df_test['Birth Year'] > 2006)]\n",
    "print(f\"Number of unrealistic birth years: {birth_year_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 874 entries with unrealistic birth years, which we will edit from the dataset to maintain data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of dropping, replace unrealistic Birth Year entries with NaN\n",
    "df.loc[birth_year.index, 'Birth Year'] = np.nan\n",
    "df_test.loc[birth_year_test.index, 'Birth Year'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** By handling missing and unrealistic values in the `Birth Year` column, we ensure that only plausible data remains, improving the reliability of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5. Age at Injury vs. Birth Year <a class=\"anchor\" id=\"sub_section_3_2_5\"></a>\n",
    "\n",
    "To ensure data consistency, we need to verify that the `Age at Injury` aligns with the calculated age based on `Birth Year` and `Accident Date`. This check helps identify any discrepancies, and we will correct the `Age at Injury` when it appears inconsistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the age based on `Birth Year` and `Accident Date` and compare it with the provided `Age at Injury`. Any differences between these two values indicate potential data entry errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of age discrepancies: 0\n"
     ]
    }
   ],
   "source": [
    "# Extract Accident Year from Accident Date\n",
    "df['Accident Year'] = pd.to_datetime(df['Accident Date'], errors='coerce').dt.year\n",
    "df_test['Accident Year'] = pd.to_datetime(df_test['Accident Date'], errors='coerce').dt.year\n",
    "\n",
    "tolerance = 1  # Allowable discrepancy in years\n",
    "age_discrepancies = df[\n",
    "    (df['Birth Year'].notnull()) & \n",
    "    (df['Accident Year'].notnull()) & \n",
    "    (abs((df['Accident Year'] - df['Birth Year']) - df['Age at Injury']) > tolerance)\n",
    "]\n",
    "\n",
    "print(f\"Number of age discrepancies: {age_discrepancies.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of age discrepancies: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of rows with age discrepancies\n",
    "percentage_discrepancies = (age_discrepancies.shape[0] / df.shape[0]) * 100\n",
    "\n",
    "print(f\"Percentage of age discrepancies: {percentage_discrepancies:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the age at injury by finding the difference between accident year and birth year\n",
    "df['Age at Injury'] = df['Accident Year'] - df['Birth Year']\n",
    "\n",
    "# Apply the same calculation to the test set\n",
    "df_test['Age at Injury'] = df_test['Accident Year'] - df_test['Birth Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking `Age at Injury`, we check for any remaining missing values or unrealistic ages to ensure the accuracy of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.  8.  9. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28.\n",
      " 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46.\n",
      " 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64.\n",
      " 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82.\n",
      " nan]\n"
     ]
    }
   ],
   "source": [
    "# Get unique values of 'Age at Injury' and sort them in ascending order\n",
    "unique_ages = df['Age at Injury'].unique()\n",
    "sorted_unique_ages = np.sort(unique_ages)\n",
    "\n",
    "# Print the result\n",
    "print(sorted_unique_ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9. 11. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28.\n",
      " 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46.\n",
      " 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64.\n",
      " 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82.\n",
      " 83. 84. nan]\n"
     ]
    }
   ],
   "source": [
    "# Get unique values of 'Age at Injury' and sort them in ascending order\n",
    "unique_ages = df_test['Age at Injury'].unique()\n",
    "sorted_unique_ages = np.sort(unique_ages)\n",
    "\n",
    "# Print the result\n",
    "print(sorted_unique_ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(58216)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age at Injury'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** After checking `Age at Injury`, we have verified there are no discrepancies with `Birth Year` and `Accident Date`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6. Age at Injury <a class=\"anchor\" id=\"sub_section_3_2_6\"></a>\n",
    "\n",
    "To maintain data quality, we need to identify and remove any unrealistic values in the `Age at Injury` column. Typically, we expect working ages to fall within a certain range, so we’ll treat ages below 16 and above 80 as potential outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify any entries where Age is 0 or greater than 100 (possible outliers)\n",
    "age_strange = df[(df['Age at Injury'] < 16) | (df['Age at Injury'] > 80)]\n",
    "print(f\"Number of age strange: {age_strange.shape[0]}\")\n",
    "\n",
    "# Identify any entries where Age is 0 or greater than 100 (possible outliers)\n",
    "age_strange_test = df_test[(df_test['Age at Injury'] < 16) | (df_test['Age at Injury'] > 80)]\n",
    "print(f\"Number of age strange: {age_strange_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that there are 212 entries with unrealistic `Age at Injury` values, which fall outside the expected range of 16 to 80 years. These entries are likely errors or outliers that could skew the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the findings, we have decided to replace to NaN these entries from the dataset to improve data reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of dropping, replace unrealistic Age at Injury entries with NaN\n",
    "df.loc[age_strange.index, 'Age at Injury'] = np.nan\n",
    "# Instead of dropping, replace unrealistic Age at Injury entries with NaN\n",
    "df_test.loc[age_strange_test.index, 'Age at Injury'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** By removing outliers in the `Age at Injury` column, we ensure that only realistic age values remain in the dataset, which will lead to more accurate analyses and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7. First Hearing Date vs. Accident Date <a class=\"anchor\" id=\"sub_section_3_2_7\"></a>\n",
    "\n",
    "To ensure consistency, we need to verify that the `First Hearing Date` occurs after the `Accident Date`. An invalid hearing date that precedes the accident date would be illogical and likely indicates data entry errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify any entries where `First Hearing Date` is earlier than `Accident Date`. These cases are considered inconsistent and will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify entries where First Hearing Date is earlier than Accident Date\n",
    "invalid_hearing_dates = df[(df['First Hearing Date'].notnull()) & \n",
    "                           (df['Accident Date'].notnull()) & \n",
    "                           (df['First Hearing Date'] < df['Accident Date'])]\n",
    "print(f\"Number of invalid hearing dates: {invalid_hearing_dates.shape[0]}\")\n",
    "\n",
    "# Identify entries where First Hearing Date is earlier than Accident Date\n",
    "invalid_hearing_dates_test = df_test[(df_test['First Hearing Date'].notnull()) & \n",
    "                           (df_test['Accident Date'].notnull()) & \n",
    "                           (df_test['First Hearing Date'] < df_test['Accident Date'])]\n",
    "print(f\"Number of invalid hearing dates: {invalid_hearing_dates_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that there are 74 entries with `First Hearing Date` preceding the `Accident Date`. We will edit these entries to maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of dropping, replace invalid First Hearing Date entries with NaN\n",
    "df.loc[invalid_hearing_dates.index, 'First Hearing Date'] = np.nan\n",
    "# Instead of dropping, replace invalid First Hearing Date entries with NaN\n",
    "df_test.loc[invalid_hearing_dates_test.index, 'First Hearing Date'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** By removing entries with invalid `First Hearing Date` values, we ensure that hearing events occur in a logical sequence after the related accident, improving the quality of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.8. C2 Date vs. C3 Date vs Accident Date <a class=\"anchor\" id=\"sub_section_3_2_8\"></a>\n",
    "For these dates, we established the following logical order:\n",
    "- **C-2 Date** should occur before **C-3 Date** (tem de ter 1 semana de diferença).\n",
    "- **C-2 Date** must also be after the **Accident Date** (tem de ser 2 anos depois).\n",
    "\n",
    "These validations are essential to ensure chronological consistency within the dataset.\n",
    "\n",
    "#### Step 1: Validate C-2 Date vs. C-3 Date\n",
    "We first checked if the `C-2 Date` precedes the `C-3 Date` for all entries where both dates are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2 must be before C3\n",
    "df_invalid_c2_c3 = df[(df['C-2 Date'].notnull()) & (df['C-3 Date'].notnull()) & \n",
    "                     (df['C-2 Date'] > df['C-3 Date'])]\n",
    "print(f\"Number of invalid C2 vs. C3 dates: {df_invalid_c2_c3.shape[0]}\")\n",
    "\n",
    "# C2 must be before C3\n",
    "df_invalid_c2_c3_test = df_test[(df_test['C-2 Date'].notnull()) & (df_test['C-3 Date'].notnull()) & \n",
    "                     (df_test['C-2 Date'] > df_test['C-3 Date'])]\n",
    "print(f\"Number of invalid C2 vs. C3 dates: {df_invalid_c2_c3_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: We found 75,816 entries with C-2 Date occurring after C-3 Date.\n",
    "Since this inconsistency is substantial, we decided not to delete these records but to flag this issue for further consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of rows with C-3 Date missing\n",
    "percentage = (df['C-3 Date'].isnull().sum() / df.shape[0]) * 100\n",
    "print(f\"Percentage of missing C-3 dates: {percentage:.2f}%\")\n",
    "\n",
    "# Calculate the percentage of rows with C-3 Date missing\n",
    "percentage_test = (df_test['C-3 Date'].isnull().sum() / df_test.shape[0]) * 100\n",
    "print(f\"Percentage of missing C-3 dates: {percentage_test:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: Approximately 67.36% of the C-3 Date values are missing.\n",
    "Due to the high proportion of missing values, we have decided to remove this column in the Missing Values treatment section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['C-3 Date'].isnull().sum()\n",
    "df_test['C-3 Date'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its very representative so we decided to not delete. Also the 'C-3 Date' has a lot of missing values, more than 68% so we will delete in Missing Values part. Also we will ignore this inconsistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2 must be after Accident Date\n",
    "df_invalid_c2_accident = df[\n",
    "    (df['C-2 Date'].notnull()) &\n",
    "    (df['Accident Date'].notnull()) &\n",
    "    (df['C-2 Date'] < df['Accident Date'])\n",
    "]\n",
    "print(f\"Number of invalid C2 vs. Accident dates: {df_invalid_c2_accident.shape[0]}\")\n",
    "\n",
    "# C2 must be after Accident Date\n",
    "df_invalid_c2_accident_test = df_test[\n",
    "    (df_test['C-2 Date'].notnull()) &\n",
    "    (df_test['Accident Date'].notnull()) &\n",
    "    (df_test['C-2 Date'] < df_test['Accident Date'])\n",
    "]\n",
    "print(f\"Number of invalid C2 vs. Accident dates: {df_invalid_c2_accident_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: We found 982 entries with C-2 Date occurring before Accident Date.\n",
    "We have decided to edit these invalid entries to maintain data consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of dropping, replace invalid C-2 Date entries with NaN\n",
    "df.loc[df_invalid_c2_accident.index, 'C-2 Date'] = np.nan\n",
    "\n",
    "# Instead of dropping, replace invalid C-2 Date entries with NaN\n",
    "df_test.loc[df_invalid_c2_accident_test.index, 'C-2 Date'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** In this section, we identified and addressed various date inconsistencies:\n",
    "\n",
    "C-2 Date vs. C-3 Date: Flagged but retained due to the high occurrence of inconsistencies.\n",
    "\n",
    "C-3 Date missingness: Will be removed in the missing values treatment phase.\n",
    "\n",
    "C-2 Date vs. Accident Date: Edited inconsistent records to maintain logical chronology.\n",
    "\n",
    "This approach ensures the dataset retains its logical and temporal coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.9. Assembly Date vs. Accident Date <a class=\"anchor\" id=\"sub_section_3_2_9\"></a>\n",
    "\n",
    "To ensure logical consistency in our dataset, we checked that the **Assembly Date** occurs after the **Accident Date**. It would not make sense for an assembly process related to the accident to take place before the accident itself.\n",
    "\n",
    "The code below identifies rows where **Assembly Date** is earlier than **Accident Date** and counts the number of such entries. If inconsistencies are found, we will remove these rows to maintain the chronological integrity of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify entries where Assembly Date is after Accident Date\n",
    "# Filter rows where both Assembly Date and Accident Date are not null\n",
    "# and Assembly Date is earlier than Accident Date\n",
    "invalid_assembly_dates = df[(df['Assembly Date'].notnull()) & (df['Accident Date'].notnull()) & \n",
    "                            (df['Assembly Date'] < df['Accident Date'])]\n",
    "\n",
    "# Print the number of identified invalid entries\n",
    "print(f\"Number of invalid assembly dates: {invalid_assembly_dates.shape[0]}\")\n",
    "\n",
    "# Identify entries where Assembly Date is after Accident Date\n",
    "# Filter rows where both Assembly Date and Accident Date are not null\n",
    "# and Assembly Date is earlier than Accident Date\n",
    "invalid_assembly_dates_test = df_test[(df_test['Assembly Date'].notnull()) & (df_test['Accident Date'].notnull()) & \n",
    "                            (df_test['Assembly Date'] < df_test['Accident Date'])]\n",
    "\n",
    "# Print the number of identified invalid entries\n",
    "print(f\"Number of invalid assembly dates: {invalid_assembly_dates_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we found 1407 entries with **Assembly Date** occurring before the **Accident Date**, we decided to edit these entries to maintain chronological accuracy within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of dropping, replace invalid Assembly Date entries with NaN\n",
    "df.loc[invalid_assembly_dates.index, 'Assembly Date'] = np.nan\n",
    "# Instead of dropping, replace invalid Assembly Date entries with NaN\n",
    "df_test.loc[invalid_assembly_dates_test.index, 'Assembly Date'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.10. Handling ZIP Code Format <a class=\"anchor\" id=\"sub_section_3_2_10\"></a>\n",
    "To ensure ZIP codes are in a valid format, let's first check if there are any ZIP codes containing letters, as ZIP codes should typically contain only numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Zip Code' column to string to ensure consistency\n",
    "df['Zip Code'] = df['Zip Code'].astype(str)\n",
    "\n",
    "# Identify ZIP codes that contain letters (A-Z) while ignoring NaN values\n",
    "zip_codes_with_letters = df[df['Zip Code'].notna() & df['Zip Code'].str.contains(r'[A-Za-z]')]\n",
    "\n",
    "# Print unique ZIP codes with letters (invalid format) for review\n",
    "print(\"ZIP codes that contain letters (invalid format):\")\n",
    "print(zip_codes_with_letters['Zip Code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_codes_with_letters['Zip Code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "By ensuring that ZIP codes are in a numeric format, we found that some entries contain letters, which makes them invalid ZIP codes. We printed a list of these invalid formats to identify the extent of the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate and print the percentage of ZIP codes that contain letters to understand how prevalent this issue is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of ZIP codes that contain letters\n",
    "percentage_with_letters = (zip_codes_with_letters.shape[0] / df.shape[0]) * 100\n",
    "\n",
    "# Print the percentage for analysis\n",
    "print(f\"Percentage of ZIP codes with letters: {percentage_with_letters:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated percentage shows the proportion of ZIP codes that contain invalid characters (letters). This helps in determining whether these records require further cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Zip Code'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same in df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Zip Code' column to string to ensure consistency\n",
    "df_test['Zip Code'] = df_test['Zip Code'].astype(str)\n",
    "\n",
    "# Identify ZIP codes that contain letters (A-Z) while ignoring NaN values\n",
    "zip_codes_with_letters_test = df_test[df_test['Zip Code'].notna() & df_test['Zip Code'].str.contains(r'[A-Za-z]')]\n",
    "\n",
    "# Print unique ZIP codes with letters (invalid format) for review\n",
    "print(\"ZIP codes that contain letters (invalid format):\")\n",
    "print(zip_codes_with_letters_test['Zip Code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_codes_with_letters_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of ZIP codes that contain letters\n",
    "percentage_with_letters_test = (zip_codes_with_letters_test.shape[0] / df_test.shape[0]) * 100\n",
    "\n",
    "# Print the percentage for analysis\n",
    "print(f\"Percentage of ZIP codes with letters: {percentage_with_letters_test:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, these ZIP codes are invalid, and there are also instances where 'nan' is written instead of a real NaN value. Therefore, we will replace these invalid entries with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex for pattern matching\n",
    "import re\n",
    "\n",
    "# Replace invalid ZIP codes that contain letters or are 'nan' with NaN for df\n",
    "df['Zip Code'] = df['Zip Code'].apply(lambda x: np.nan if pd.isna(x) or re.search(r'[A-Za-z]', str(x)) or str(x).lower() == 'nan' else x)\n",
    "\n",
    "# Do the same for df_test to maintain consistency in the test set\n",
    "df_test['Zip Code'] = df_test['Zip Code'].apply(lambda x: np.nan if pd.isna(x) or re.search(r'[A-Za-z]', str(x)) or str(x).lower() == 'nan' else x)\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Invalid ZIP codes containing letters or 'nan' have been replaced with NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Zip Code'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ensuring 5-Digit Format for ZIP Codes*\n",
    "\n",
    "If the length of a ZIP code is 4 digits, add a leading zero to make it 5 digits for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_zip_code\n",
    "\n",
    "# Apply the function to standardize ZIP codes in the dataset\n",
    "df['Zip Code'] = df['Zip Code'].apply(convert_zip_code)\n",
    "\n",
    "# Apply the function to the test set to ensure consistent formatting\n",
    "df_test['Zip Code'] = df_test['Zip Code'].apply(convert_zip_code)\n",
    "\n",
    "# Print some values to verify the changes in ZIP Code format\n",
    "print(df['Zip Code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After validating and cleaning the ZIP code format, we ensure all entries are in a consistent 5-digit format by adding a leading zero to any 4-digit codes. This step ensures uniformity across the dataset, which is crucial for subsequent analysis or modeling.\n",
    "\n",
    "These steps handle the ZIP code format issues and validate consistency across the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.11. Gender Feature <a class=\"anchor\" id=\"sub_section_3_2_11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified that there are valid entries for the Gender feature, but there are also a few inconsistencies that need to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the unique values and their counts for the Gender column\n",
    "df['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that only 45 entries are labeled as 'X' in the Gender column. Since 'U' typically represents individuals who prefer not to disclose their gender, we will replace 'X' with 'U'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'X' with 'U' in the Gender column\n",
    "df['Gender'] = df['Gender'].replace('X', 'U')\n",
    "\n",
    "# Apply to the test set as well\n",
    "df_test['Gender'] = df_test['Gender'].replace('X', 'U')\n",
    "\n",
    "# Print the number of unique values in Gender to verify\n",
    "print(\"Unique values in Gender after replacement:\")\n",
    "print(df['Gender'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process has helped to standardize the Gender feature values, which now contain only 'M', 'F', and 'U' as valid entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.12. Overview of Inconsistencies <a class=\"anchor\" id=\"sub_section_3_2_12\"></a>\n",
    "In this section, we will calculate the number of rows removed while handling missing values by comparing the original dataset (df_train) with the cleaned dataset (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the percentage of increased NaN values\n",
    "nan_increase = df.isna().sum().sum() - df_train.isna().sum().sum()  \n",
    "print(f\"Percentage of NaNs increased: {(nan_increase / df_train.size) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us an overview of how much data was removed due to handling inconsistencies, ensuring that the impact on the dataset is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Visualization <a class=\"anchor\" id=\"section_3_3\"></a>\n",
    "Now that we have treated the inconsistencies, we can use some basic visualizations like boxplots and histograms to better understand the distribution and potentially check for any other issues in our dataset.\n",
    "\n",
    "Here, we think that we should separate the numerical data into the following categories:\n",
    "\n",
    "- Continuous\n",
    "- Discrete\n",
    "- Categorical\n",
    "- Text features\n",
    "\n",
    "### 3.3.1. Basic Plots <a class=\"anchor\" id=\"sub_section_3_3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for all numerical columns\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots that reflect the different categories of features, such as categorical, binary, discrete.\n",
    "\n",
    "1. Continuous Columns: Plotting Histograms\n",
    "For continuous columns, a histogram can help visualize the distribution.\n",
    "\n",
    "2. Categorical Columns: Plotting Frequency Distributions\n",
    "For each categorical column, we can plot the frequency of the top values.\n",
    "\n",
    "3. Binary Columns: Plotting the Counts of Values (N vs Y)\n",
    "For binary columns, we will use a bar plot to show the counts of N and Y.\n",
    "\n",
    "4. Discrete Columns: Plotting the Distribution of Discrete Values\n",
    "For discrete columns, we can plot the frequency of each distinct value or group by bins if there are too many values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Continuous Variables** <br>\n",
    "\n",
    "In this section, we plot the pairplot for the continuous variable 'Average Weekly Wage' to examine its distribution and relationship with the target variable 'Claim Injury Type'. The pairplot allows us to visualize how different classes in the target variable behave in relation to 'Average Weekly Wage', and observe any patterns or separations between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the dataset to remove rows with 'Average Weekly Wage' equal to zero and extreme values\n",
    "filtered_df = df[(df['Average Weekly Wage'] > 0) & (df['Average Weekly Wage'] < df['Average Weekly Wage'].quantile(0.95))]\n",
    "\n",
    "# Set figure size and plot pairplot\n",
    "g = sns.pairplot(filtered_df, vars=['Average Weekly Wage'], hue='Claim Injury Type', palette='Set2', height=5, aspect=1.5)\n",
    "\n",
    "# Add a title to the pairplot\n",
    "g.fig.suptitle(\"Pairplot between continuous variable and target (filtered data)\", y=1.02)\n",
    "\n",
    "# Manually add legend outside of plot\n",
    "plt.legend(\n",
    "    labels=filtered_df['Claim Injury Type'].unique(),\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc='upper left',\n",
    "    fontsize='medium'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Increased figure size for better visibility\n",
    "sns.boxplot(data=filtered_df, x='Claim Injury Type', y='Average Weekly Wage', palette='Set2')\n",
    "plt.title(f'Distribution of Average Weekly Wage by Claim Injury Type')\n",
    "plt.xlabel('Claim Injury Type')  # Adding labels for better clarity\n",
    "plt.ylabel('Average Weekly Wage')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Categorical Variables** <br>\n",
    "\n",
    "Now we will create plots for the categorical variables. Since some categorical features have a high number of unique values, we will limit our analysis to the top 10 most frequent values within each categorical feature, with the target variable as the hue for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Columns: Plotting Frequency Distributions with Target as Hue\n",
    "categorical_columns = ['Carrier Type', 'Carrier Name', 'District Name', 'County of Injury', 'Gender', 'Medical Fee Region']\n",
    "\n",
    "# Loop through categorical columns and create separate figures\n",
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot using seaborn to include hue (target column)\n",
    "    ax = sns.countplot(data=df, x=column, hue='Claim Injury Type', palette='viridis', order=df[column].value_counts().iloc[:10].index)\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(f'Top 10 Most Frequent Values in {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add count labels above each bar\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha='center', va='bottom', fontsize=8, color='black')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Categorical Variables\n",
    "\n",
    "#### 1. Carrier Type\n",
    "\n",
    "The \"Carrier Type\" chart shows the distribution of claims across different carrier types with \"Claim Injury Type\" as the hue, indicating the frequency of each injury type within each carrier type. \n",
    "\n",
    "Observations:\n",
    "- **Private** carriers have the highest number of claims, with a significant number of \"NON-COMP\" (Non-compensable) injury claims. This is followed by \"TEMPORARY\" claims.\n",
    "- **Self-Public** and **Self-Private** carriers have notable numbers of \"NON-COMP\" and \"TEMPORARY\" injury types but at a much lower frequency than private carriers.\n",
    "- \"UNKNOWN\" carrier type has a small number of claims distributed across various injury types, indicating possible data quality issues or lack of information.\n",
    "  \n",
    "Conclusion:\n",
    "The private sector appears to handle the majority of claims, particularly for non-compensable injury types. Other carrier types have fewer claims, possibly due to the smaller scale or specific operational areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Carrier Name\n",
    "\n",
    "The \"Carrier Name\" chart highlights the top 10 carriers by the number of claims with \"Claim Injury Type\" as the hue. \n",
    "\n",
    "Observations:\n",
    "- **State Insurance Fund** is the leading carrier in terms of claim numbers, predominantly with \"NON-COMP\" and \"TEMPORARY\" injury claims.\n",
    "- Other carriers, like **Police, Fire, Sanitation** and **American Zurich Insurance Co**, also have a high frequency of claims, but with varying distributions across injury types.\n",
    "- There is a visible drop in claim numbers for carriers outside the top few, showing a long tail in the claim distribution.\n",
    "\n",
    "Conclusion:\n",
    "The State Insurance Fund handles a large volume of claims, potentially indicating its role as a major carrier in the region or its handling of a specific sector. Other carriers follow, but none reach the claim volume of the State Insurance Fund.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. District Name\n",
    "\n",
    "The \"District Name\" chart shows the distribution of claims by district, with \"Claim Injury Type\" displayed as the hue. \n",
    "\n",
    "Observations:\n",
    "- **NYC** (New York City) has the highest claim frequency by a significant margin, mostly for \"NON-COMP\" and \"TEMPORARY\" injury types.\n",
    "- **Albany** and **Hauppauge** follow, with much lower numbers but a similar distribution pattern across injury types.\n",
    "- Other districts like **Buffalo** and **Rochester** show diverse injury types but with a focus on \"NON-COMP\".\n",
    "\n",
    "Conclusion:\n",
    "NYC is a major hub for claims, possibly due to its population density or the concentration of industries. The distribution across other districts suggests that claim volumes vary greatly by region, with injury type distributions remaining relatively consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. County of Injury\n",
    "\n",
    "The \"County of Injury\" chart illustrates the top 10 counties by the number of claims, broken down by \"Claim Injury Type\".\n",
    "\n",
    "Observations:\n",
    "- **Suffolk** and **Queens** counties show the highest frequencies, particularly in \"NON-COMP\" and \"TEMPORARY\" injury types.\n",
    "- The **Bronx** and **Kings** counties also report substantial claims, with similar injury type distributions to the leading counties.\n",
    "- There is a steady decline in claim frequency as we move down the list of counties, with a more varied mix of injury types in counties with fewer claims.\n",
    "\n",
    "Conclusion:\n",
    "Certain counties like Suffolk and Queens lead in claim volumes, possibly due to larger populations or higher-risk industries. The injury type distribution remains largely similar across counties, highlighting regional patterns in claim types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gender\n",
    "\n",
    "The \"Gender\" chart shows the distribution of claims by gender, with \"Claim Injury Type\" as the hue.\n",
    "\n",
    "Observations:\n",
    "- **Males (M)** have the highest number of claims, especially for \"NON-COMP\" and \"TEMPORARY\" injury types, followed by \"MED ONLY\".\n",
    "- **Females (F)** also show significant claim numbers but at a notably lower volume than males, with a similar injury type distribution.\n",
    "- Claims labeled with **U** (unspecified) gender are few but still show a diverse mix of injury types.\n",
    "\n",
    "Conclusion:\n",
    "Males have a significantly higher number of claims than females, which might suggest higher-risk roles or industries. Unspecified gender claims are minimal but should be considered for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Medical Fee Region\n",
    "\n",
    "The \"Medical Fee Region\" chart illustrates the distribution of claims across different regions, with \"Claim Injury Type\" represented by the hue.\n",
    "\n",
    "Observations:\n",
    "- Regions **IV**, **I**, and **II** show the highest number of claims, with \"NON-COMP\" and \"TEMPORARY\" injury types being the most prevalent across these regions.\n",
    "- The **UK** (United Kingdom) region also has a significant number of claims, which may indicate involvement in international cases or coverage for individuals/entities with connections to the UK.\n",
    "- Region **III** has lower claim frequencies than IV, I, and II but still follows the same general pattern of injury types.\n",
    "- The distribution across regions highlights that claims are concentrated in a few key areas, with a substantial number of cases likely linked to industry activities or regional policies in these zones.\n",
    "\n",
    "Conclusion:\n",
    "Regions IV, I, and II are central hubs for claims, potentially due to high industry density or specific regulatory policies that influence claim volumes. The presence of claims in the UK region indicates international aspects in the dataset, suggesting cross-border coverage or claims related to international entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Discrete Variables** <br>\n",
    "Next, we will plot the discrete columns, focusing on the 15 most frequent values for each feature, with the target variable as the hue to enhance interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete Columns: Plotting Top 15 Most Frequent Values with Target as Hue\n",
    "discrete_columns = ['Number of Dependents', 'Age at Injury', 'IME-4 Count', 'Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code', 'Zip Code']\n",
    "\n",
    "# Loop through discrete columns and create separate figures\n",
    "for column in discrete_columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Select the top 10 most frequent values for the current column\n",
    "    top_10_values = df[column].value_counts().nlargest(10).index\n",
    "    \n",
    "    # Plot using seaborn to include hue (target column)\n",
    "    ax = sns.countplot(data=df[df[column].isin(top_10_values)], x=column, hue='Claim Injury Type', palette='viridis', order=top_10_values)\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(f'Top 10 Most Frequent Values in {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add count labels above each bar\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha='center', va='bottom', fontsize=8, color='black')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Discrete Variables\n",
    "\n",
    "#### 1. Number of Dependents\n",
    "- **Insight**: The majority of records have `0`, `1`, or `2` dependents, with a significantly higher frequency in the `NON-COMP` claim injury type.\n",
    "- **Distribution**: Higher counts of dependents are less common, with a steep drop after `3` dependents.\n",
    "- **Claim Injury Type Distribution**: `NON-COMP` and `TEMPORARY` types are more common among records with `0` to `3` dependents, while other claim types appear relatively less frequently across all dependent numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Age at Injury\n",
    "- **Insight**: Ages around `32`, `52`, and `35` have the highest frequency, suggesting certain age groups may be more prone to injury in this dataset.\n",
    "- **Claim Injury Type Trends**: Similar to other variables, the `NON-COMP` claim type dominates across all age groups, especially in the 30s and 50s.\n",
    "- **Observations**: This could indicate specific age ranges where the workforce may be more susceptible to injuries, possibly due to experience or physical demands related to the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. IME-4 Count\n",
    "- **Insight**: The `IME-4 Count` values are highly concentrated around `1`, `2`, and `3`, showing a lower count as the value increases.\n",
    "- **Claim Injury Type Distribution**: `TEMPORARY` and `PPD SCH LOSS` claim types are more common in cases with lower `IME-4 Count`, suggesting fewer medical evaluations for these injury types.\n",
    "- **Trend**: Higher `IME-4 Count` values appear less frequently, indicating that the number of independent medical exams is limited for most claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Industry Code\n",
    "- **Insight**: Codes `62.0`, `92.0`, and `61.0` are the most frequent, which may correspond to specific industries more prone to injuries.\n",
    "- **Claim Type Distribution**: `NON-COMP` claims are notably high across these industry codes, showing that certain industries may have different injury claim distributions.\n",
    "- **Observation**: Industries corresponding to these codes might have higher injury risks, warranting further investigation or preventive measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. WCIO Cause of Injury Code\n",
    "- **Insight**: Codes `56.0`, `74.0`, and `60.0` are the top causes of injuries, with varying distribution across claim types.\n",
    "- **Claim Type Trends**: `MED ONLY` and `NON-COMP` types are more prevalent in these injury causes, possibly indicating the nature and severity associated with each code.\n",
    "- **Observation**: Understanding which injury causes lead to `TEMPORARY` or `PPD` claims may help in assessing injury prevention efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. WCIO Nature of Injury Code\n",
    "- **Insight**: `52.0` and `10.0` codes dominate in injury nature, which may represent specific injury types (like sprains, fractures, etc.).\n",
    "- **Claim Type Distribution**: Consistently, `NON-COMP` is prevalent across these injury codes, though other types like `TEMPORARY` also appear frequently.\n",
    "- **Observation**: This variable provides insight into the types of injuries commonly associated with higher claim counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. WCIO Part of Body Code\n",
    "- **Insight**: Parts `42.0` and `53.0` are the most affected, suggesting particular body parts are more prone to workplace injuries.\n",
    "- **Claim Type Trends**: `TEMPORARY` claims are frequently associated with these body parts, possibly indicating less severe injuries that lead to temporary disability.\n",
    "- **Observation**: This can guide ergonomic adjustments or protective equipment focus to reduce injuries in these body parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Zip Code\n",
    "- **Insight**: Certain ZIP codes (`11236`, `10940`, etc.) have notably higher injury frequencies, which may point to geographic regions with higher claims.\n",
    "- **Claim Type Distribution**: Distribution across claim types varies, with `NON-COMP` frequently leading in many of these areas.\n",
    "- **Observation**: Regional analysis of injury claims can be insightful for targeted interventions or support in high-claim ZIP codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Binary Variables** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Columns: Plotting Counts of Values with Target as Hue\n",
    "binary_columns = ['Attorney/Representative', 'COVID-19 Indicator', 'Agreement Reached','Alternative Dispute Resolution']\n",
    "\n",
    "# Loop through binary columns and create separate figures\n",
    "for column in binary_columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    # Plot using seaborn to include hue (target column)\n",
    "    ax = sns.countplot(data=df, x=column, hue='Claim Injury Type', palette='viridis')\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(f'Value Counts in {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Add count labels above each bar\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha='center', va='bottom', fontsize=8, color='black')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Binary Variables\n",
    "\n",
    "#### Attorney/Representative\n",
    "\n",
    "- This chart displays the count of cases based on whether they had an Attorney or Representative involved (`1`) or not (`0`).\n",
    "- Cases without an attorney or representative are the majority, especially for the `NON-COMP` and `TEMPORARY` injury types.\n",
    "- The presence of an attorney or representative is more common among cases with `TEMPORARY` and `MED ONLY` injury types but significantly lower in number than cases without representation across all injury types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COVID-19 Indicator\n",
    "\n",
    "- This plot shows the distribution of cases based on whether they were associated with COVID-19 (`1`) or not (`0`).\n",
    "- Cases not related to COVID-19 dominate across all injury types, with a significant majority in `NON-COMP` and `TEMPORARY` injury types.\n",
    "- COVID-19-related cases are present but in much smaller numbers, primarily impacting `NON-COMP` and `TEMPORARY` injury types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agreement Reached\n",
    "\n",
    "- This chart illustrates whether an agreement was reached (`1`) or not (`0`) in claims across different injury types.\n",
    "- The majority of cases did not reach an agreement, with a high count in the `NON-COMP`, `TEMPORARY`, and `MED ONLY` categories.\n",
    "- Cases where an agreement was reached are relatively fewer but still show some presence, mainly in `TEMPORARY` and `MED ONLY` injury types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Dispute Resolution\n",
    "\n",
    "- This plot represents the use of alternative dispute resolution (ADR) in claims, indicating `1` if used and `0` otherwise.\n",
    "- ADR was not utilized in the majority of cases, with high counts for `NON-COMP` and `TEMPORARY` injury types.\n",
    "- For cases where ADR was employed, it is relatively more common in `TEMPORARY` and `MED ONLY` injury types, although the numbers are still low compared to cases without ADR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Multivariate Analyses <a class=\"anchor\" id=\"sub_section_3_3_2\"></a>\n",
    "In this section, we will conduct multivariate analyses with two primary objectives:\n",
    "- Understanding the correlation between different variables.\n",
    "- Examining the relationship between the features and the target variable (Claim Injury Type).\n",
    "\n",
    "In the following code, we will explore binary and categorical variables by visualizing how binary features impact different categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_count_for_binary_and_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## função ja está no utils\n",
    "\n",
    "# Filtering binary and categorical variables from your dataframe\n",
    "binary_vars = [col for col in df.columns if df[col].nunique() == 2]  # Binary variables\n",
    "\n",
    "# Define the columns to exclude\n",
    "exclude_columns = ['Zip Code', 'Carrier Name']\n",
    "\n",
    "# Get categorical variables excluding the specified columns\n",
    "categorical_vars = [col for col in df.columns if df[col].dtype == 'object' and col not in exclude_columns]\n",
    "\n",
    "# Generate count plots for binary and categorical variables\n",
    "plot_count_for_binary_and_categorical(df, binary_vars, categorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['Attorney/Representative', 'COVID-19 Indicator', 'Agreement Reached', 'Alternative Dispute Resolution']\n",
    "\n",
    "# Loop through binary columns and create a heatmap for each\n",
    "for column in binary_columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create a pivot table counting the occurrences of each combination\n",
    "    count_data = df.groupby([column, 'Claim Injury Type']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Calculate proportions\n",
    "    proportions = count_data.div(count_data.sum(axis=0), axis=1)\n",
    "    \n",
    "    # Plot heatmap of proportions\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(proportions, annot=True, fmt=\".2%\", cmap=\"viridis\", cbar=True, ax=ax2)\n",
    "    ax2.set_title(f'Heatmap of Proportions for {column} by Claim Injury Type')\n",
    "    ax2.set_xlabel('Claim Injury Type')\n",
    "    ax2.set_ylabel(column)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we observe that when the binary variable 'Alternative Dispute Resolution' is analyzed against categorical variables, it rarely has a value of 1, and is predominantly 0. This suggests that 'Alternative Dispute Resolution' has minimal or no impact on other variables. Therefore, we have decided to remove this variable from further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete Alternative Dispute Resolution\n",
    "df = df.drop(['Alternative Dispute Resolution'], axis=1)\n",
    "\n",
    "#Apply to the test set\n",
    "df_test = df_test.drop(['Alternative Dispute Resolution'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the contingency table\n",
    "contingency_table = pd.crosstab([df['COVID-19 Indicator'], df['Agreement Reached']], df['Claim Injury Type'])\n",
    "\n",
    "# Calculate proportions\n",
    "proportions = contingency_table.div(contingency_table.sum(axis=0), axis=1)\n",
    "\n",
    "# Plot the heatmap of proportions\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(proportions, annot=True, cmap=\"viridis\", fmt=\".2%\", cbar_kws={\"shrink\": .8})\n",
    "plt.title(\"Heatmap of Proportions between Binary Variables and Claim Injury Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap highlights the proportions of different claim injury types for each binary variable.\n",
    "\n",
    "Notable findings include:\n",
    "\n",
    "- High Proportions in No COVID-19 Cases: The \"0.0-0.0\" category, representing cases with no COVID-19 association and no agreement reached, shows high proportions across most claim injury types, suggesting a strong pattern in non-COVID-19 related claims.\n",
    "\n",
    "- Specific Injury Types and Agreement Reached: Categories like \"1.0-0.0\" (COVID-19 associated, no agreement) show significant proportions for serious claim types such as PTD and DEATH. This indicates that COVID-19 related claims with no agreement reached are more likely to result in severe outcomes.\n",
    "\n",
    "These observations suggest potential areas for focused interventions and adjustments in handling claims based on COVID-19 association and agreement status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['Attorney/Representative', 'COVID-19 Indicator', 'Agreement Reached']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine binary variables into a single column\n",
    "df['Binary Combination'] = df[binary_columns].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "# Calculate the contingency table\n",
    "contingency_table = pd.crosstab(df['Binary Combination'], df['Claim Injury Type'])\n",
    "\n",
    "# Calculate proportions\n",
    "proportions = contingency_table.div(contingency_table.sum(axis=0), axis=1)\n",
    "\n",
    "# Plot the heatmap of proportions\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(proportions, annot=True, cmap=\"viridis\", fmt=\".2%\", cbar_kws={\"shrink\": .8})\n",
    "plt.title(\"Heatmap of Proportions between Binary Variable Combinations and Claim Injury Type\")\n",
    "plt.xlabel(\"Claim Injury Type\")\n",
    "plt.ylabel(\"Combination of Binary Variables\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Columns \\ Binary Values | Attorney/ Representative | COVID-19 Indicator | Agreement Reached |\n",
    "|-------------------------|--------------------------|--------------------|-------------------|\n",
    "| 0                       | No                       | Not associated     | Needed WCB        | \n",
    "| 1                       | Yes                      | Associated         | Didn't need WCB   | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Impact of COVID-19 Indicator:\n",
    "\n",
    "The 1.0-0.0 combination (claims with an Attorney/Representative but no COVID-19 association) shows significantly high proportions for serious claim types such as PPD NSL (90.47%) and PPD SCH LOSS (88.03%). This suggests a strong pattern in non-COVID-19 related claims with attorney involvement.\n",
    "\n",
    "2. Agreement Reached:\n",
    "\n",
    "The 0.0-0.0-1.0 combination (claims without an Attorney/Representative, not associated with COVID-19, but with an agreement reached) generally shows very low proportions across all claim injury types. This indicates that agreements are less common in claims without attorney involvement.\n",
    "\n",
    "3. Attorney/Representative:\n",
    "\n",
    "Claims involving an Attorney/Representative (1.0) have higher proportions in serious claim types such as PPD NSL, PPD SCH LOSS, and PTD. This highlights the critical role of attorney involvement in more severe or complex claims.\n",
    "\n",
    "By focusing on the binary variables Attorney/Representative, COVID-19 Indicator, and Agreement Reached, the heatmap helps identify key patterns and relationships that can inform better decision-making and targeted interventions. This streamlined approach provides a clearer and more focused view of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete Binary Combination\n",
    "df = df.drop(['Binary Combination'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to adapt the code using only two categorical variables\n",
    "categorical_columns = ['Carrier Type', 'Gender']  # Choose a few columns to avoid high cardinality\n",
    "\n",
    "# Create a new column that combines the chosen categorical variables\n",
    "df['Category Combination'] = df[categorical_columns].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "# Create the contingency table between the combination of categorical variables and Claim Injury Type\n",
    "contingency_table = pd.crosstab(df['Category Combination'], df['Claim Injury Type'])\n",
    "\n",
    "# Calculate proportions\n",
    "proportions = contingency_table.div(contingency_table.sum(axis=0), axis=1)\n",
    "\n",
    "# Plot the heatmap of proportions\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(proportions, annot=True, cmap=\"viridis\", fmt=\".2%\", cbar_kws={\"shrink\": .8})\n",
    "plt.title(\"Heatmap of Proportions between Categorical Variable Combinations and Claim Injury Type\")\n",
    "plt.xlabel(\"Claim Injury Type\")\n",
    "plt.ylabel(\"Combination of Categorical Variables\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap, several insights and potential actions can be identified:\n",
    "\n",
    "- Merging Special Funds: The proportions for the different SPECIAL FUND categories are quite low and similar across most claim injury types. This suggests that these categories (like CONS. COMM. (SECT. 25-A), POI CARRIER WCB MENANDS, UNKNOWN) could be merged into a single SPECIAL FUND category to simplify the data without losing significant information. This consolidation could help streamline the analysis process and make the insights more manageable.\n",
    "\n",
    "- Gender Differences: There are noticeable differences in proportions between genders within the same carrier type. For instance, PRIVATE-M has higher proportions across several claim injury types compared to PRIVATE-F and PRIVATE-U. This indicates that gender-specific analysis could be important for deriving more accurate and relevant insights.\n",
    "\n",
    "- High Proportions in Specific Categories: Certain combinations stand out with significantly higher proportions in specific claim injury types. For example, PRIVATE-M has a high proportion in the DEATH category, and SIF-M shows high proportions in TEMPORARY and PTD. These high proportions could indicate areas that need targeted interventions or further investigation.\n",
    "\n",
    "- Low Proportions in UNKNOWN Categories: The UNKNOWN categories generally have very low proportions across all claim injury types. This suggests that these categories might not be as relevant and could potentially be merged to reduce complexity without affecting the overall insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows where C-2 Date is earlier than Accident Date\n",
    "num_negative_values = (df['C-2 Date'] < df['Accident Date']).sum()\n",
    "\n",
    "# Print the number of rows with negative values\n",
    "print(f\"Number of rows where C-2 Date is earlier than Accident Date: {num_negative_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Test Split <a class=\"anchor\" id=\"chapter4\"></a>\n",
    "\n",
    "Splitting the dataset into training and test sets is a crucial step in building a machine learning model. In this project, we perform the train-test split after correcting inconsistencies but before data preprocessing, such as handling missing values, outlier treatment, and feature scaling. This sequence helps in maintaining the integrity of our data and ensures that the model is trained and evaluated fairly.\n",
    "\n",
    "The train-test split is performed after fixing inconsistencies but before preprocessing to ensure that we do not inadvertently introduce information from the test set into the training set. This allows us to maintain the integrity of our model evaluation and helps ensure that the results obtained during model evaluation are unbiased and reflect the model’s performance on truly unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.drop('Claim Injury Type',axis=1)\n",
    "y= df['Claim Injury Type']\n",
    "\n",
    "# Encode o target\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.3, \n",
    "                                                  random_state = 42, \n",
    "                                                  stratify = y, \n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preprocessing the Dataset ⚒️ <a class=\"anchor\" id=\"chapter5\"></a>\n",
    "\n",
    "In this chapter, we will perform the basic steps to get to know our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Outliers\n",
    "\n",
    "The columns below are those that may have outliers in our dataset:\n",
    "\n",
    "- **Age at Injury** – The age of the person at the time of the accident.\n",
    "- **Average Weekly Wage** – Average weekly wage.\n",
    "- **Number of Dependents** – Number of dependents (may have outliers in cases with a high number of dependents).\n",
    "- **Accident Year** – Year of the accident.\n",
    "- **WCIO Cause of Injury Code, WCIO Nature of Injury Code, WCIO Part Of Body Code** – Although these are codes, if they have continuously high numerical values, they may contain outliers.\n",
    "- **Zip Code** – Although it is a geographic identifier, we can check for unusual or infrequent postal codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check= [\n",
    "    'Age at Injury', 'Average Weekly Wage', \n",
    "    'Number of Dependents', 'Accident Year'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6 * len(columns_to_check)))\n",
    "\n",
    "# Loop through each numeric column and plot a separate box plot\n",
    "for i, col in enumerate(columns_to_check, 1):\n",
    "    plt.subplot(len(columns_to_check), 1, i)\n",
    "    sns.boxplot(x=X_train[col])\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Values')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Outliers\n",
    "1. Age at Injury\n",
    "The box plot for \"Age at Injury\" shows a fairly symmetrical distribution with no significant outliers.\n",
    "The data primarily falls between ages 20 and 80, which aligns with typical working age ranges, indicating there are no major age outliers in the data.\n",
    "2. Average Weekly Wage\n",
    "The \"Average Weekly Wage\" plot shows extreme outliers on the high end.\n",
    "A few values exceed the million mark, which is likely unusual for weekly wages and suggests possible outliers or data entry errors.\n",
    "This variable will likely need further investigation and treatment for these high outliers.\n",
    "3. Number of Dependents\n",
    "The distribution of \"Number of Dependents\" appears well-contained within 0 to 6 dependents.\n",
    "No significant outliers are observed, suggesting a reasonable distribution for this variable.\n",
    "4. Accident Year\n",
    "The \"Accident Year\" box plot shows several outliers before the year 2000.\n",
    "Most of the data is concentrated in recent years (post-2000), which is expected in a modern claims dataset.\n",
    "Outliers in earlier years could represent older claims or data entry errors and may warrant further review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar o DataFrame para remover valores zero na coluna 'Average Weekly Wage'\n",
    "X_train_filtered = X_train[X_train['Average Weekly Wage'] != 0]\n",
    "\n",
    "# Colunas numéricas selecionadas\n",
    "columns_to_check_3 = [ 'Average Weekly Wage']\n",
    "\n",
    "plt.figure(figsize=(10, 6 * len(columns_to_check_3)))\n",
    "\n",
    "# Loop para criar um boxplot para cada coluna\n",
    "for i, col in enumerate(columns_to_check_3, 1):\n",
    "    plt.subplot(len(columns_to_check_3), 1, i)\n",
    "    sns.boxplot(x=X_train_filtered[col].dropna())  # Remove NaNs e exibe o boxplot\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Values')\n",
    "\n",
    "# Ajustar layout para evitar sobreposição\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Outliers in `Average Weekly Wage`\n",
    "\n",
    "The box plot for `Average Weekly Wage` indicates significant outliers. Most values are close to the lower end of the distribution, with only a few observations spread across a wide range up to approximately 2.5.\n",
    "\n",
    "##### Observations:\n",
    "- The box plot shows a high concentration of wages near the lower end, with some extreme outliers extending significantly beyond the upper whisker.\n",
    "- Outliers may represent exceptionally high wages, which could be legitimate high-income cases or data entry anomalies.\n",
    "- Further analysis is recommended to determine if these high values align with known wage distributions or if they could impact model performance negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ver=X_train[X_train['Average Weekly Wage']>10000]\n",
    "X_train_ver.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_remove = X_train_ver.index\n",
    "\n",
    "# Remover as linhas de X_train com valores em 'Average Weekly Wage' maiores que 10,000\n",
    "X_train_novo = X_train.drop(X_train_ver.index)\n",
    "\n",
    "# Verificar a proporção de dados restantes em X_train_novo\n",
    "print(len(X_train_novo) / len(X_train))\n",
    "\n",
    "# Remover as mesmas linhas de y_train\n",
    "# Obter os índices dos valores a serem removidos\n",
    "\n",
    "# Remover os valores de y_train nos mesmos índices\n",
    "#y_train_novo = np.delete(y_train, indices_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_means_train = X_train.groupby('Industry Code')['Average Weekly Wage'].mean()\n",
    "industry_means_train_1 = X_train_novo.groupby('Industry Code')['Average Weekly Wage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_means_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_means_train_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** We must remove these outliers before entering salaries based on the industry code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot to check the possible separation of points by year\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.stripplot(data=X_train, x='Accident Year', jitter=0.3)\n",
    "plt.title('Accident Year Scatter Plot')\n",
    "plt.xlabel('Accident Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of `Accident Year` Distribution\n",
    "\n",
    "The scatter plot of `Accident Year` shows a dense distribution of incidents over the years, with most events concentrated from the 1980s onward. \n",
    "\n",
    "##### Observations:\n",
    "- **Pre-1980**: There are sparse data points, suggesting fewer recorded incidents or less complete data in earlier years.\n",
    "- **1980 and later**: There is a dense concentration, which increases as we move towards recent years. This may reflect more comprehensive data collection and record-keeping.\n",
    "- **Post-2010**: There is a consistently high density, indicating active and robust data recording in recent years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train['Accident Year']<1970].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Assembly Date Year'] = X_train['Assembly Date'].dt.year\n",
    "X_train['C-2 Date Year'] = X_train['C-2 Date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to check for date-based outliers\n",
    "columns_to_check_3 = ['Assembly Date Year', 'C-2 Date Year']\n",
    "\n",
    "# Plot box plots for each date column\n",
    "plt.figure(figsize=(10, 6 * len(columns_to_check_3)))\n",
    "for i, col in enumerate(columns_to_check_3, 1):\n",
    "    plt.subplot(len(columns_to_check_3), 1, i)\n",
    "    sns.boxplot(x=X_train[col])\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Values')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of `Assembly Date Year` and `C-2 Date Year` Distributions\n",
    "\n",
    "##### 1. Assembly Date Year\n",
    "The box plot for `Assembly Date Year` shows a narrow range of data, with all values clustered between 2020 and 2022. This indicates that the assembly records in this dataset are concentrated in recent years, as it is mentioned in the Project Description.\n",
    "\n",
    "- **Observation**: The absence of outliers suggests that data is consistently collected within this timeframe.\n",
    "\n",
    "##### 2. C-2 Date Year\n",
    "The box plot for `C-2 Date Year` displays a wider range, with data points spanning from 1995 to 2022. There are some older values that appear as individual points, which may indicate isolated entries or events from prior years. Most data points are recent, around the 2015-2022 range, with a few older records extending back to the late 1990s.\n",
    "\n",
    "- **Observation**: The presence of isolated older values suggests occasional reporting from earlier years.\n",
    "\n",
    "##### Overall Insights:\n",
    "The data appears to be recent and concentrated around 2020-2022 for assembly dates, with `C-2 Date Year` showing a slightly broader historical context. This distribution may reflect changes in data availability or policy for data retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train['C-2 Date Year']<2015].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Assembly Date Year column from the train\n",
    "X_train.drop(['Assembly Date Year','C-2 Date Year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Missing Values <a class=\"anchor\" id=\"section_5_2\"></a>\n",
    "\n",
    "We may also have missing values that are incorrectly set to zero. Therefore, we will use `.eq(0)` to identify if there are any columns with zeros that are not supposed to have them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of zeros in each column\n",
    "X_train.eq(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we identify columns with a high number of zero values, which might indicate potential missing values represented by zero instead of NaN. This helps us to locate inconsistencies or placeholders that we need to address during data preprocessing.\n",
    "\n",
    "Key observations from the output:\n",
    "\n",
    "Alternative Dispute Resolution: 399,988 entries are zeros, potentially representing missing values.\n",
    "\n",
    "Attorney/Representative: 274,491 entries are zeros, likely to be placeholders.\n",
    "\n",
    "Average Weekly Wage: 234,796 entries have zeros, indicating potential data issues.\n",
    "\n",
    "COVID-19 Indicator: 382,695 entries are zeros, which may represent cases without a COVID-related claim.\n",
    "\n",
    "Agreement Reached: 382,972 entries have zeros, which could indicate missing or null agreements.\n",
    "\n",
    "Number of Dependents: 57,123 entries with zeroes might indicate missing values in this column.\n",
    "\n",
    "This analysis allows us to consider the necessity of replacing or imputing these zero values during the data preprocessing steps, ensuring they do not interfere with model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling High Zero Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for zero values (e.g., more than 20% zeros)\n",
    "threshold = 0.2\n",
    "\n",
    "# Calculate the percentage of zeros for each column\n",
    "zero_percentage = X_train.eq(0).mean()\n",
    "\n",
    "# Filter columns that have more than the threshold of zero values\n",
    "high_zero_columns = zero_percentage[zero_percentage > threshold]\n",
    "\n",
    "# Display the columns with their zero percentage\n",
    "print(high_zero_columns*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a threshold to identify columns with a high proportion of zero values (more than 20%). This analysis helps us determine if any columns contain zeros that might represent missing data instead of actual values.\n",
    "\n",
    "The columns that have more than 20% of zero values are:\n",
    "- **Attorney/Representative**: 68.31% zeros\n",
    "- **Average Weekly Wage**: 58.43% zeros\n",
    "- **COVID-19 Indicator**: 95.24% zeros\n",
    "- **Agreement Reached**: 95.31% zeros\n",
    "\n",
    "We found that **Average Weekly Wage** contains an unexpectedly high number of zeros, which is likely incorrect for this dataset. Since every employee should have a weekly wage, these zeros should be treated as missing values. Therefore, we will replace the zeros in the **Average Weekly Wage** column with `NaN`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that only one column, Average Weekly Wage, contains an unexpectedly high number of zeros. Since this dataset is related to work insurance, every employee must have a weekly wage, meaning these zeros are likely incorrect and should be treated as missing values. Therefore, we will replace the zeros in the Average Weekly Wage column with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros in 'Average Weekly Wage' with NaN in the train, validation, and test sets\n",
    "X_train['Average Weekly Wage'] = X_train['Average Weekly Wage'].replace(0, np.nan)\n",
    "X_val['Average Weekly Wage'] = X_val['Average Weekly Wage'].replace(0, np.nan)\n",
    "df_test['Average Weekly Wage'] = df_test['Average Weekly Wage'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Remaining Missing Values\n",
    "\n",
    "After treating zeros as missing values in the **Average Weekly Wage** column, we will now examine the dataset for any remaining `NaN` values. This step ensures that all missing data points are identified, enabling us to address them systematically.\n",
    "\n",
    "The table below displays the count of missing values (NaN) for each column in the dataset. Some columns, such as **First Hearing Date** and **IME-4 Count**, contain a significant number of missing values, which may require further investigation and treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display count of NaN values for each column in the training dataset\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns with High Percentage of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for missing values (e.g., more than 30% missing)\n",
    "threshold = 0.2\n",
    "\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentage = X_train.isnull().mean()\n",
    "\n",
    "# Filter columns that have more than the threshold of missing values\n",
    "high_missing_columns = missing_percentage[missing_percentage > threshold]\n",
    "\n",
    "# Display the columns with their missing percentage\n",
    "print(high_missing_columns*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a threshold of 20%, we identified columns with a high percentage of missing values. Columns such as **First Hearing Date** and **IME-4 Count** have more than 70% missing data, which makes them candidates for potential feature engineering or alternative handling.\n",
    "\n",
    "In particular:\n",
    "- **First Hearing Date**: Given its high missing rate, this feature could potentially be transformed into a binary indicator, where 0 indicates missing and 1 indicates available data. This approach allows us to retain any signal that might be present in the presence or absence of this information.\n",
    "\n",
    "Let's explore more about the rest of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Handling Missing Values in `IME-4 Count`\n",
    "\n",
    "The `IME-4 Count` feature represents the number of independent medical audit examinations. A missing value (`NaN`) in this column likely indicates that no audits were conducted for a specific case. Therefore, we have decided to replace all `NaN` values with 0 to indicate that there was no audit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values in 'IME-4 Count' to understand the range\n",
    "X_train['IME-4 Count'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As IME-4 represents the number of independent medical audicts examination we decided to convert nan to 0 meaning that there wasn't any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in 'IME-4 Count' with 0 in the training, validation, and test sets\n",
    "X_train['IME-4 Count'].fillna(0, inplace=True)\n",
    "X_val['IME-4 Count'].fillna(0, inplace=True)\n",
    "df_test['IME-4 Count'].fillna(0, inplace=True)\n",
    "\n",
    "# Confirm replacement by displaying unique values again\n",
    "X_train['IME-4 Count'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Handling Missing Values in `C-3 Date`\n",
    "\n",
    "The `C-3 Date` feature was dropped because it exceeded the threshold for missing values that we predefined. This indicates that the feature likely lacks sufficient predictive power due to the high amount of missing information.\n",
    "\n",
    "We decided to drop C-3 Date based on the threshold that we pre defined meaning those features don't have enough predictive power based on the amount of missing information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'C-3 Date' column from training, validation, and test sets\n",
    "X_train = X_train.drop('C-3 Date', axis=1)\n",
    "\n",
    "# Apply to the validation set\n",
    "X_val = X_val.drop('C-3 Date', axis=1)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test = df_test.drop('C-3 Date', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Defining a threshold for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for missing values - in this case, 30% of the columns\n",
    "threshold = X_train.shape[1] * 0.30\n",
    "\n",
    "# Count the number of NaNs in each row\n",
    "nan_counts_per_row = X_train.isna().sum(axis=1)\n",
    "\n",
    "# Find rows with more than 30% NaNs\n",
    "rows_with_high_nan = nan_counts_per_row > threshold\n",
    "\n",
    "# Calculate the percentage of rows with more than 30% NaNs\n",
    "percentage_rows_with_high_nan = (rows_with_high_nan.sum() / X_train.shape[0]) * 100\n",
    "\n",
    "print(f\"Percentage of rows with more than 30% missing values: {percentage_rows_with_high_nan:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows with more than 30% NaNs from X_train and update y_train accordingly\n",
    "rows_to_keep = ~rows_with_high_nan  # Rows that we are keeping in X_train\n",
    "X_train = X_train[rows_to_keep].copy()\n",
    "y_train = y_train[rows_to_keep].copy()  # Keep the same indices in y_train\n",
    "\n",
    "# Print the shape of the cleaned training set\n",
    "print(f\"Shape of X_train after removing rows with more than 30% NaNs: {X_train.shape}\")\n",
    "print(f\"Shape of y_train after removing corresponding rows: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Threshold for Missing Values**:\n",
    "   - We have decided to focus on rows with **more than 30% missing values** in `X_train` dataset. \n",
    "\n",
    "2. **Remove Rows with High Missing Values**:\n",
    "   - We will **delete the rows in `X_train`** that have **more than 30% of the columns with missing values**. Since this only affects **0.25% of the rows**, it won't significantly reduce your dataset size and will help maintain a good quality of training data.\n",
    "\n",
    "3. **Preserve Data Quality in `X_train`**:\n",
    "   - The purpose of removing rows from `X_train` is to have a **high-quality training dataset**. By improving the quality of `X_train`, we will also have better statistics (mean/mode) for imputing missing values in X_val and df_test. This ensures that your imputation values are more reliable, coming from data that has less noise and fewer inconsistencies.\n",
    "\n",
    "4. **Impute Missing Values in X_val and df_test**:\n",
    "    - After deleting the rows with high missing values in `X_train`, we will use the mode or mean from X_train to impute the missing values in both X_val and df_test to maintain consistency across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Imputing Missing Values in `Industry Code`\n",
    "\n",
    "To impute missing values in the `Industry Code` feature, we calculate the mode of `Industry Code` based on the combination of `Carrier Name` and `Carrier Type`. This approach allows us to assign the most common `Industry Code` for each carrier and carrier type combination, providing a relevant approximation for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mode of 'Industry Code' for each ('Carrier Name', 'Carrier Type') pair in the training dataset\n",
    "industry_code_mode_train = X_train.groupby(['Carrier Name', 'Carrier Type'])['Industry Code'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "\n",
    "from utils import impute_industry_code\n",
    "\n",
    "# Apply the function to fill missing values in 'Industry Code' for X_train, X_val, and df_test\n",
    "X_train['Industry Code'] = X_train.apply(lambda row: impute_industry_code(row) if pd.isna(row['Industry Code']) else row['Industry Code'], axis=1)\n",
    "X_val['Industry Code'] = X_val.apply(lambda row: impute_industry_code(row) if pd.isna(row['Industry Code']) else row['Industry Code'], axis=1)\n",
    "df_test['Industry Code'] = df_test.apply(lambda row: impute_industry_code(row) if pd.isna(row['Industry Code']) else row['Industry Code'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Imputing Missing Values in `Average Weekly Wage`\n",
    "\n",
    "For now, we will impute the missing values in the `Average Weekly Wage` feature based on the mean values grouped by `Industry Code`. This allows us to approximate the missing values by considering the industry context of each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of 'Average Weekly Wage' for each 'Industry Code' in the training dataset\n",
    "industry_means_train = X_train.groupby('Industry Code')['Average Weekly Wage'].mean()\n",
    "\n",
    "from utils import impute_average_weekly_wage\n",
    "\n",
    "# Apply the function to fill missing values in 'Average Weekly Wage' for X_train, X_val, and df_test\n",
    "X_train['Average Weekly Wage'] = X_train.apply(lambda row: impute_average_weekly_wage(row) if pd.isna(row['Average Weekly Wage']) else row['Average Weekly Wage'], axis=1)\n",
    "X_val['Average Weekly Wage'] = X_val.apply(lambda row: impute_average_weekly_wage(row) if pd.isna(row['Average Weekly Wage']) else row['Average Weekly Wage'], axis=1)  \n",
    "df_test['Average Weekly Wage'] = df_test.apply(lambda row: impute_average_weekly_wage(row) if pd.isna(row['Average Weekly Wage']) else row['Average Weekly Wage'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Imputing Missing Values in `Zip Code`\n",
    "\n",
    "To impute missing values in the `Zip Code` feature, we use the mode of `Zip Code` based on the combination of `County of Injury` and `District Name`. This approach ensures that missing values are filled with the most frequent `Zip Code` for each unique combination of county and district, providing a region-specific approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mode of 'Zip Code' for each ('County of Injury', 'District Name') pair in the training dataset\n",
    "zip_code_mode_train = X_train.groupby(['County of Injury', 'District Name'])['Zip Code'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "\n",
    "from utils import impute_zip_code\n",
    "\n",
    "# Apply the imputation function only for missing values in 'Zip Code' for X_train, X_val, and df_test\n",
    "X_train['Zip Code'] = X_train.apply(lambda row: impute_zip_code(row) if pd.isna(row['Zip Code']) else row['Zip Code'], axis=1)\n",
    "X_val['Zip Code'] = X_val.apply(lambda row: impute_zip_code(row) if pd.isna(row['Zip Code']) else row['Zip Code'], axis=1)\n",
    "df_test['Zip Code'] = df_test.apply(lambda row: impute_zip_code(row) if pd.isna(row['Zip Code']) else row['Zip Code'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing `Birth Year`\n",
    "\n",
    "To impute missing values in the `Birth Year` feature, we use the `Assembly Date` and `Age at Injury`. If both `Assembly Date` and `Age at Injury` are available, we can calculate the `Birth Year` by subtracting `Age at Injury` from the year of `Assembly Date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Assembly Date' is in datetime format for all datasets\n",
    "X_train['Assembly Date'] = pd.to_datetime(X_train['Assembly Date'], errors='coerce')\n",
    "X_val['Assembly Date'] = pd.to_datetime(X_val['Assembly Date'], errors='coerce')\n",
    "df_test['Assembly Date'] = pd.to_datetime(df_test['Assembly Date'], errors='coerce')\n",
    "\n",
    "from utils import impute_birth_year\n",
    "\n",
    "# Apply the imputation function only for missing values in 'Birth Year' for X_train, X_val, and df_test\n",
    "X_train['Birth Year'] = X_train.apply(lambda row: impute_birth_year(row) if pd.isna(row['Birth Year']) else row['Birth Year'], axis=1)\n",
    "X_val['Birth Year'] = X_val.apply(lambda row: impute_birth_year(row) if pd.isna(row['Birth Year']) else row['Birth Year'], axis=1)\n",
    "df_test['Birth Year'] = df_test.apply(lambda row: impute_birth_year(row) if pd.isna(row['Birth Year']) else row['Birth Year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Age at Injury"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `Age at Injury` contains missing values which represent key data about the age of the person at the time of the injury. We observe that by retaining this feature and carefully imputing its missing values, we ensure that we preserve potentially valuable information for the predictive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop 'Birth Year' from X_train, X_val, and df_test\n",
    "# X_train = X_train.drop(columns=['Birth Year'])\n",
    "# X_val = X_val.drop(columns=['Birth Year'])\n",
    "# df_test = df_test.drop(columns=['Birth Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After addressing `Birth Year`, we check the percentage of missing values in `df_test`. This will guide us on further preprocessing steps to handle any remaining missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values in df_test\n",
    "missing_percentage = df_test.isnull().mean() * 100\n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete Accident Year\n",
    "X_train.drop(['Accident Year'], axis=1, inplace=True)\n",
    "X_val.drop(['Accident Year'], axis=1, inplace=True)\n",
    "df_test.drop(['Accident Year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Exclude `First Hearing Date`**:  \n",
    "We check the percentage of rows with missing values after excluding `First Hearing Date`, which has a high percentage of missing entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percentage of rows with missing values, excluding 'First Hearing Date'\n",
    "missing_values = X_train[X_train.columns.difference(['First Hearing Date'])].isna().any(axis=1)\n",
    "percentage_missing = (missing_values.sum() / len(X_train)) * 100\n",
    "print(f\"Percentage of rows with missing values (excluding 'First Hearing Date'): {percentage_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Exclude `Age at Injury`**:  \n",
    "Additionally, we explore the impact of excluding both `First Hearing Date` and `Age at Injury` to see how much of the data remains usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percentage of rows with missing values, excluding both 'First Hearing Date' and 'Age at Injury'\n",
    "missing_values = X_train[X_train.columns.difference(['First Hearing Date', 'Age at Injury','Birth Year'])].isna().any(axis=1)\n",
    "percentage_missing = (missing_values.sum() / len(X_train)) * 100\n",
    "print(f\"Percentage of rows with missing values (excluding 'First Hearing Date' and 'Age at Injury'): {percentage_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can consider this: We decrease a lot the percentage of rows with missing values, so, for this motive we will delete the variable Age at Injury."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Drop Age at Injury from the train, val and test set\n",
    "# X_train = X_train.drop(columns=['Age at Injury'])\n",
    "# X_val = X_val.drop(columns=['Age at Injury'])\n",
    "# df_test = df_test.drop(columns=['Age at Injury']) 3.72??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only 3.89% of the values are missing, we decided to use KNNImputer to fill in these missing values. It’s important to note that we are not including 'First Hearing Date' in this imputation, as missing values in this column have a specific meaning. We plan to create a new feature based on the presence or absence of 'First Hearing Date' as part of the feature engineering process, after which the original column will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the datasets to avoid modifying the original\n",
    "X_train_copy = X_train.copy()\n",
    "X_val_copy = X_val.copy()\n",
    "df_test_copy = df_test.copy()\n",
    "\n",
    "# Drop the column 'Agreement Reached' if it's not in the test set\n",
    "columns_to_drop = ['Agreement Reached']\n",
    "X_train_copy.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "X_val_copy.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "df_test_copy.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "\n",
    "# Define lists for categorical/code features and numeric features\n",
    "categorical_features = ['Zip Code', 'Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "numeric_features = ['Age at Injury', 'Average Weekly Wage']\n",
    "\n",
    "# Mode imputation for categorical/code features (using mode of X_train)\n",
    "for col in categorical_features:\n",
    "    if col in X_train_copy.columns:\n",
    "        mode_value = X_train_copy[col].mode()[0]  # Calculate mode from the training set\n",
    "\n",
    "        # Apply mode imputation to X_train, X_val, and df_test\n",
    "        X_train_copy[col].fillna(mode_value, inplace=True)\n",
    "        X_val_copy[col].fillna(mode_value, inplace=True)\n",
    "        if col in df_test_copy.columns:\n",
    "            df_test_copy[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Mean imputation for numeric features (using mean of X_train)\n",
    "for col in numeric_features:\n",
    "    if col in X_train_copy.columns:\n",
    "        mean_value = X_train_copy[col].mean()  # Calculate mean from the training set\n",
    "\n",
    "        # Apply mean imputation to X_train, X_val, and df_test\n",
    "        X_train_copy[col].fillna(mean_value, inplace=True)\n",
    "        X_val_copy[col].fillna(mean_value, inplace=True)\n",
    "        if col in df_test_copy.columns:\n",
    "            df_test_copy[col].fillna(mean_value, inplace=True)\n",
    "\n",
    "# Convert date features to datetime before imputation\n",
    "date_columns = ['Accident Date', 'Assembly Date', 'C-2 Date']\n",
    "for col in date_columns:\n",
    "    X_train_copy[col] = pd.to_datetime(X_train_copy[col], errors='coerce')\n",
    "    X_val_copy[col] = pd.to_datetime(X_val_copy[col], errors='coerce')\n",
    "    df_test_copy[col] = pd.to_datetime(df_test_copy[col], errors='coerce')\n",
    "\n",
    "# Impute 'Accident Date' before 'C-2 Date'\n",
    "# Impute missing 'Accident Date' with mean or some logical estimate\n",
    "accident_date_mean = X_train_copy['Accident Date'].mean()\n",
    "X_train_copy['Accident Date'].fillna(accident_date_mean, inplace=True)\n",
    "X_val_copy['Accident Date'].fillna(accident_date_mean, inplace=True)\n",
    "df_test_copy['Accident Date'].fillna(accident_date_mean, inplace=True)\n",
    "\n",
    "# Impute missing 'Assembly Date Date' with mean or some logical estimate\n",
    "assembly_date_mean = X_train_copy['Assembly Date'].mean()\n",
    "X_train_copy['Assembly Date'].fillna(assembly_date_mean, inplace=True)\n",
    "X_val_copy['Assembly Date'].fillna(assembly_date_mean, inplace=True)\n",
    "df_test_copy['Assembly Date'].fillna(assembly_date_mean, inplace=True)\n",
    "\n",
    "\n",
    "# Define a function to impute or adjust 'C-2 Date' based on 'Accident Date'\n",
    "def correct_c2_date(row):\n",
    "    accident_date = row['Accident Date']\n",
    "    c2_date = row['C-2 Date']\n",
    "\n",
    "    # If 'C-2 Date' is missing or earlier than 'Accident Date', impute\n",
    "    if pd.isna(c2_date) or (pd.notna(accident_date) and c2_date < accident_date):\n",
    "        return accident_date + pd.Timedelta(days=30)\n",
    "    return c2_date\n",
    "\n",
    "# Apply the correction to all rows in 'C-2 Date' for X_train, X_val, and df_test\n",
    "X_train_copy['C-2 Date'] = X_train_copy.apply(correct_c2_date, axis=1)\n",
    "X_val_copy['C-2 Date'] = X_val_copy.apply(correct_c2_date, axis=1)\n",
    "df_test_copy['C-2 Date'] = df_test_copy.apply(correct_c2_date, axis=1)\n",
    "\n",
    "# Convert date features back to datetime (to ensure correct type after operations)\n",
    "for col in date_columns:\n",
    "    X_train_copy[col] = pd.to_datetime(X_train_copy[col], errors='coerce')\n",
    "    X_val_copy[col] = pd.to_datetime(X_val_copy[col], errors='coerce')\n",
    "    df_test_copy[col] = pd.to_datetime(df_test_copy[col], errors='coerce')\n",
    "\n",
    "# Final dataframes after mode and mean imputation with type corrections\n",
    "X_train_processed = X_train_copy\n",
    "X_val_processed = X_val_copy\n",
    "df_test_processed = df_test_copy\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Missing values imputed using mode for categorical features and mean for numeric features. Types corrected for integer and date columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we impute the Age at Injury, we can recalculate the Birth Year\n",
    "X_train_processed['Birth Year'] = X_train_processed['Accident Date'].dt.year - X_train_processed['Age at Injury']\n",
    "X_val_processed['Birth Year'] = X_val_processed['Accident Date'].dt.year - X_val_processed['Age at Injury']\n",
    "df_test_processed['Birth Year'] = df_test_processed['Accident Date'].dt.year - df_test_processed['Age at Injury']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_processed.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_processed.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_columns=['Age at Injury','WCIO Cause of Injury Code','WCIO Nature of Injury Code','WCIO Part Of Body Code','Industry Code']\n",
    "# Convert numeric features back to integer where needed\n",
    "\n",
    "for col in int_columns:\n",
    "    # Round the column values to ensure they are integer-compatible\n",
    "    X_train_processed[col] = X_train_processed[col].round().astype('Int64')  # Convert to nullable integer type\n",
    "    X_val_processed[col] = X_val_processed[col].round().astype('Int64')\n",
    "    df_test_processed[col] = df_test_processed[col].round().astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Categorical Features <a class=\"anchor\" id=\"section_5_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will handle the categorical features by selecting them for encoding and transformation.\n",
    "\n",
    "The categorical features selected for encoding are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_categorical= [\n",
    " 'Attorney/Representative',\n",
    " 'COVID-19 Indicator',\n",
    " 'County of Injury',\n",
    " 'Carrier Type',\n",
    " 'District Name',\n",
    " 'Gender',\n",
    " 'Medical Fee Region']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Categorical Features\n",
    "\n",
    "Below, we visualize the distributions of each categorical feature in our dataset. Each plot provides an overview of the value counts for each category, helping us understand the distribution of these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features_categorical:\n",
    "    print(feature)\n",
    "    print(X_train_processed[feature].dtype)\n",
    "    X_train_processed[feature].value_counts().plot(kind='bar', figsize=(7, 3))\n",
    "    plt.title(f'Bar Plot of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Attorney/Representative\n",
    "Most cases do not have an attorney or representative involved (0).\n",
    "Cases with attorney/representative involvement (1) are fewer but still significant.\n",
    "This could be a factor in the complexity or severity of the case.\n",
    "\n",
    "### 2. COVID-19 Indicator\n",
    "The vast majority of records do not have the COVID-19 indicator marked (0).\n",
    "Only a minimal amount (1) has this indicator, reflecting cases potentially affected by COVID-19.\n",
    "\n",
    "### 3. County of Injury\n",
    "The distribution of injuries across counties is highly uneven.\n",
    "Some counties (e.g., SUFFOLK and NYC) show a much higher count than others, possibly reflecting population density or workplace concentration.\n",
    "Counties with few entries could be combined to prevent sparsity issues in further analysis.\n",
    "\n",
    "### 4. Carrier Type\n",
    "The majority of carriers are private, followed by self-public and SIF.\n",
    "There are a few specific types, such as UNKNOWN and SPECIAL FUND, with lower frequencies.\n",
    "The private carrier type dominance may be significant in understanding claim patterns.\n",
    "\n",
    "### 5. District Name\n",
    "NYC has the highest number of claims by a large margin, followed by ALBANY and other regions.\n",
    "This distribution suggests that urban areas like NYC have a higher frequency of incidents.\n",
    "Lesser-known districts could be grouped to manage sparsity.\n",
    "\n",
    "### 6. Gender\n",
    "The gender distribution shows a higher frequency of male (M) entries, followed by female (F).\n",
    "There is also a small category marked with an empty or undefined gender, which may need addressing.\n",
    "The imbalance could impact analyses related to gender if not properly managed.\n",
    "\n",
    "\n",
    "### 7. Medical Fee Region\n",
    "Region IV has the highest number of entries, followed by '-', '=', and '≡'.\n",
    "There's a small number marked as 'UK' (Unknown), which may require imputation or separate handling.\n",
    "The distribution across regions could correlate with regional policies or practices in medical fees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Engineering <a class=\"anchor\" id=\"chapter6\"></a>\n",
    "\n",
    "Feature engineering is the process of preparing data for machine learning models by transforming raw data into meaningful features that enhance model performance. In this section, we create, select, and modify variables to capture significant patterns within the data, making it more informative and useful for the model’s learning process. Through these transformations, we aim to improve the model’s accuracy and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Carrier-District Interaction <a class=\"anchor\" id=\"section_6_1\"></a>\n",
    "Combining **Carrier Type** with **District Name** may reveal regional preferences for certain insurance carriers, which could be useful in understanding regional biases or regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature by combining Carrier Type and District Name\n",
    "X_train_processed['Carrier_District_Interaction'] = X_train_processed['Carrier Type'] + \"_\" + X_train_processed['District Name']\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Carrier_District_Interaction'] = X_val_processed['Carrier Type'] + \"_\" + X_val_processed['District Name']\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Carrier_District_Interaction'] = df_test_processed['Carrier Type'] + \"_\" + df_test_processed['District Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Income Category  <a class=\"anchor\" id=\"section_6_2\"></a>\n",
    "\n",
    "Creating categories for **Average Weekly Wage** can simplify the continuous nature of income into meaningful segments such as Low, Average, and High, which could help the model understand different socioeconomic statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key percentiles\n",
    "percentiles = X_train_processed['Average Weekly Wage'].quantile([0.25, 0.5, 0.75, 0.9])\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the bins and labels for categorizing income based on percentiles\n",
    "income_bins = [0, 865.022500, 1197.862575, 1530.970000, float('inf')]  # float('inf') allows us to set an open-ended range\n",
    "income_labels = ['Low Income', 'Lower-Middle Income', 'Upper-Middle Income', 'High Income']\n",
    "\n",
    "# Creating the new feature for income categories for the train set\n",
    "X_train_processed['Income_Category'] = pd.cut(X_train_processed['Average Weekly Wage'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Income_Category'] = pd.cut(X_val_processed['Average Weekly Wage'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Income_Category'] = pd.cut(df_test_processed['Average Weekly Wage'], bins=income_bins, labels=income_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating this categorical feature, we drop the original Average Weekly Wage column since it’s now represented by Income_Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the 'Average Weekly Wage' column as it's represented by 'Income_Category'\n",
    "# X_train_processed = X_train_processed.drop(columns=['Average Weekly Wage'])\n",
    "# X_val_processed = X_val_processed.drop(columns=['Average Weekly Wage'])\n",
    "# df_test_processed = df_test_processed.drop(columns=['Average Weekly Wage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Days_To_First_Hearing  <a class=\"anchor\" id=\"section_6_3\"></a>\n",
    "\n",
    "The feature **Days_To_First_Hearing** was created to capture the number of days between the Accident Date and the First Hearing Date. If a First Hearing Date is available, the feature represents the time elapsed, which can help the model understand the speed of the claim process. If the First Hearing Date is missing, it is represented as 0, indicating that a hearing has not occurred yet. This approach provides more nuanced information than simply indicating whether the hearing occurred or not, allowing the model to learn from both the presence and timing of the first hearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create the new feature\n",
    "X_train_copy['Days_To_First_Hearing'] = X_train_copy.apply(calculate_hearing_days, axis=1)\n",
    "X_val_copy['Days_To_First_Hearing'] = X_val_copy.apply(calculate_hearing_days, axis=1)\n",
    "df_test_copy['Days_To_First_Hearing'] = df_test_copy.apply(calculate_hearing_days, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating this binary feature, we can drop the original First Hearing Date column from the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop First Hearing Date from the train, val, and test sets\n",
    "X_train_processed = X_train_processed.drop(columns=['First Hearing Date'])\n",
    "X_val_processed = X_val_processed.drop(columns=['First Hearing Date'])\n",
    "df_test_processed = df_test_processed.drop(columns=['First Hearing Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Accident Quarter  <a class=\"anchor\" id=\"section_6_4\"></a>\n",
    "\n",
    "Temporal data can often influence outcomes. Extracting the quarter of the accident (e.g., 1st, 2nd, etc.) helps the model capture seasonal patterns that may impact accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the quarter of the Accident Date\n",
    "X_train_processed['Accident_Quarter'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce').dt.quarter\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Accident_Quarter'] = pd.to_datetime(X_val_processed['Accident Date'], errors='coerce').dt.quarter\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Accident_Quarter'] = pd.to_datetime(df_test_processed['Accident Date'], errors='coerce').dt.quarter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Accident Year <a class=\"anchor\" id=\"section_6_5\"></a>\n",
    "The year can help the model understand seasonal or yearly effects, like accident patterns during different times of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the year from the Accident Date\n",
    "X_train_processed['Accident_Year'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce').dt.year\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Accident_Year'] = pd.to_datetime(X_val_processed['Accident Date'], errors='coerce').dt.year\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Accident_Year'] = pd.to_datetime(df_test_processed['Accident Date'], errors='coerce').dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Accident on Day and Weekend <a class=\"anchor\" id=\"section_6_6\"></a>\n",
    "\n",
    "The day of the accident could be significant, as weekends might have different risk factors compared to weekdays. We will extract the day of the week and create a feature to indicate if the accident occurred on a weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the day of the week and creating a feature to indicate if the accident occurred on a weekend\n",
    "X_train_processed['Accident Day'] = pd.to_datetime(X_train_processed['Accident Date'], errors='coerce').dt.dayofweek\n",
    "X_train_processed['Accident on Weekend'] = X_train_processed['Accident Day'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Accident Day'] = pd.to_datetime(X_val_processed['Accident Date'], errors='coerce').dt.dayofweek\n",
    "X_val_processed['Accident on Weekend'] = X_val_processed['Accident Day'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Accident Day'] = pd.to_datetime(df_test_processed['Accident Date'], errors='coerce').dt.dayofweek\n",
    "df_test_processed['Accident on Weekend'] = df_test_processed['Accident Day'].apply(lambda x: 1 if x >= 5 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Age Group <a class=\"anchor\" id=\"section_6_7\"></a>\n",
    "Grouping ages can help simplify the model’s understanding of different age demographics (e.g., Youth, Young Adult, Middle Age, Senior). This could potentially improve model interpretability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values in 'Age at Injury' to understand the range\n",
    "np.sort(X_train_processed['Age at Injury'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins and labels for age groups\n",
    "age_bins = [0, 25, 45, 65, float('inf')]\n",
    "age_labels = ['Youth', 'Young Adult', 'Middle Age', 'Senior']\n",
    "\n",
    "# Creating a new feature for age groups\n",
    "X_train_processed['Age Group'] = pd.cut(X_train_processed['Age at Injury'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Age Group'] = pd.cut(X_val_processed['Age at Injury'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Age Group'] = pd.cut(df_test_processed['Age at Injury'], bins=age_bins, labels=age_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop 'Age at Injury' from the train, val and test set\n",
    "# X_train_processed = X_train_processed.drop(columns=['Age at Injury'])\n",
    "# X_val_processed = X_val_processed.drop(columns=['Age at Injury'])\n",
    "# df_test_processed = df_test_processed.drop(columns=['Age at Injury'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8. Promptness_category <a class=\"anchor\" id=\"section_6_8\"></a>\n",
    "\n",
    "The `promptness_category` feature categorizes the time taken between key events in the claims process, specifically measuring the difference between the `Accident Date` and the `Assembly Date`. This feature quantifies the speed or delay in assembling the claim and provides insight into how promptly claims are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to X_train_processed\n",
    "X_train_processed = categorize_promptness(X_train_processed, 'Assembly Date', 'Accident Date', 'promptness_category')\n",
    "\n",
    "# Apply the function to X_val_processed\n",
    "X_val_processed = categorize_promptness(X_val_processed, 'Assembly Date', 'Accident Date', 'promptness_category')\n",
    "\n",
    "# Apply the function to df_test_processed\n",
    "df_test_processed = categorize_promptness(df_test_processed, 'Assembly Date', 'Accident Date', 'promptness_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value counts for the new column\n",
    "X_train_processed['promptness_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These categories allow us to observe the promptness in claim processing, with the majority falling within Until 1 week, indicating a generally swift assembly of claims. However, a significant portion extends beyond a month, with a small subset taking more than a year. This feature can provide insights into patterns of delays or rapid processing, possibly indicating areas for improvement in claim management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9. promptness_C2_category <a class=\"anchor\" id=\"section_6_9\"></a>\n",
    "The \"promptness_C2_category\" feature tracks the time taken to register the C-2 Date (the receipt of the employer's report of work-related injury/illness) after the Accident Date. It evaluates employers' promptness in reporting accidents, offering insights into compliance and potential administrative delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows where C-2 Date is earlier than Accident Date\n",
    "num_negative_values = (X_train_processed['C-2 Date'] < X_train_processed['Accident Date']).sum()\n",
    "\n",
    "# Print the number of rows with negative values\n",
    "print(f\"Number of rows where C-2 Date is earlier than Accident Date: {num_negative_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to X_train_processed\n",
    "X_train_processed = categorize_promptness(X_train_processed, 'C-2 Date', 'Accident Date', 'promptness_C2_category')\n",
    "\n",
    "# Apply the function to X_val_processed\n",
    "X_val_processed = categorize_promptness(X_val_processed, 'C-2 Date', 'Accident Date', 'promptness_C2_category')\n",
    "\n",
    "# Apply the function to df_test_processed\n",
    "df_test_processed = categorize_promptness(df_test_processed, 'C-2 Date', 'Accident Date', 'promptness_C2_category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating new features based on the existing date columns, we will remove the original date features to avoid redundancy and simplify the dataset. We believe that the impact of these date features is adequately captured in the newly engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = X_train_processed.drop(columns=['Accident Date', 'Assembly Date', 'C-2 Date'])\n",
    "\n",
    "#Apply to the val set\n",
    "X_val_processed = X_val_processed.drop(columns=['Accident Date','Assembly Date', 'C-2 Date'])\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed = df_test_processed.drop(columns=['Accident Date', 'Assembly Date', 'C-2 Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.10. Zip_Code_Simplified <a class=\"anchor\" id=\"section_6_10\"></a>\n",
    "\n",
    "To reduce the dimensionality of the Zip Code feature, we will create a new feature called Zip_Code_Simplified. This feature will group all zip codes that appear less than 2,000 times in the training dataset into a category labeled as 'Other'. By doing this, we effectively reduce the number of unique zip codes, simplifying the model while retaining the most significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the most frequent Carrier Names along with their counts\n",
    "most_frequent_zipcode = X_train_processed['Zip Code'].value_counts().head(25)  # Adjust the number if you need more\n",
    "print(\"Most frequent Zip Codes with their counts:\")\n",
    "print(most_frequent_zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature called 'Zip_Code_Simplified' based on 'Zip Code' for train, validation, and test sets\n",
    "X_train_processed['Zip_Code_Simplified'] = X_train_processed['Zip Code']\n",
    "X_val_processed['Zip_Code_Simplified'] = X_val_processed['Zip Code']\n",
    "df_test_processed['Zip_Code_Simplified'] = df_test_processed['Zip Code']\n",
    "\n",
    "# Identify carrier names that occur fewer than 1000 times in X_train_processed\n",
    "zipcode_counts = X_train_processed['Zip Code'].value_counts()\n",
    "zipcode_to_replace = zipcode_counts[zipcode_counts < 1000].index\n",
    "\n",
    "# Replace carrier names with fewer than 1000 occurrences with 'OTHER' in all datasets using the identified carriers from X_train\n",
    "for dataset in [X_train_processed, X_val_processed, df_test_processed]:\n",
    "    dataset['Zip_Code_Simplified'] = dataset['Zip_Code_Simplified'].replace(zipcode_to_replace, 'OTHER')\n",
    "\n",
    "# Print the counts of the simplified carrier names in X_train_processed to verify the result\n",
    "print(\"Counts of 'Zip_Code_Simplified' feature in X_train_processed:\")\n",
    "print(X_train_processed['Zip_Code_Simplified'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique counts to compare the dimensionality reduction\n",
    "print(f\"Original ZIP Code uniqueness: {X_train_processed['Zip Code'].nunique()}\")\n",
    "print(f\"Simplified ZIP Code uniqueness: {X_train_processed['Zip_Code_Simplified'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation retains regional information while reducing the feature dimensionality, which can be beneficial for model interpretability and efficiency. The original Zip Code column has been removed to avoid redundancy. For this motive we will delete also the Zip Code, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_processed = X_train_processed.drop(columns=['Zip Code'])\n",
    "# # Apply to the val set\n",
    "# X_val_processed = X_val_processed.drop(columns=['Zip Code'])\n",
    "# # Apply to the test set\n",
    "# df_test_processed = df_test_processed.drop(columns=['Zip Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.11. Carrier Type Merged <a class=\"anchor\" id=\"section_6_11\"></a>\n",
    "\n",
    "Since there are several categories under \"Special Fund\" with very few occurrences, combining them into a single category can reduce noise in the data and make the feature more manageable for the model.\n",
    "\n",
    "After merging, we observe the following distribution of Carrier Type Merged values in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature that merges all 'Special Fund' categories into a single category for train, validation, and test sets\n",
    "for dataset in [X_train_processed, X_val_processed, df_test_processed]:\n",
    "    dataset['Carrier Type Merged'] = dataset['Carrier Type'].replace({\n",
    "        'SPECIAL FUND - UNKNOWN': 'SPECIAL FUND',\n",
    "        'SPECIAL FUND - POI CARRIER WCB MENANDS': 'SPECIAL FUND',\n",
    "        'SPECIAL FUND - CONS. COMM. (SECT. 25-A)': 'SPECIAL FUND'\n",
    "    })\n",
    "\n",
    "# Verifying the updated column for X_train_processed\n",
    "print(X_train_processed['Carrier Type Merged'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now let's delete Carrier Type from the train, val and test set\n",
    "# X_train_processed = X_train_processed.drop(columns=['Carrier Type'])\n",
    "# # Apply to the val set\n",
    "# X_val_processed = X_val_processed.drop(columns=['Carrier Type'])\n",
    "# # Apply to the test set\n",
    "# df_test_processed = df_test_processed.drop(columns=['Carrier Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.12. Carrier_Name_Simplified <a class=\"anchor\" id=\"section_6_12\"></a>\n",
    "\n",
    "The 'Carrier Name' feature has high cardinality, with 1951 unique values. This level of uniqueness can complicate machine learning models, especially if some categories have very few instances. To simplify the analysis and potentially improve model performance, we will group carrier names with fewer than 500 occurrences under a single category called 'OTHER'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the most frequent Carrier Names along with their counts\n",
    "most_frequent_carriers = X_train_processed['Carrier Name'].value_counts().head(25)  # Adjust the number if you need more\n",
    "print(\"Most frequent Carrier Names with their counts:\")\n",
    "print(most_frequent_carriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature called 'Carrier_Name_Simplified' based on 'Carrier Name' for train, validation, and test sets\n",
    "X_train_processed['Carrier_Name_Simplified'] = X_train_processed['Carrier Name']\n",
    "X_val_processed['Carrier_Name_Simplified'] = X_val_processed['Carrier Name']\n",
    "df_test_processed['Carrier_Name_Simplified'] = df_test_processed['Carrier Name']\n",
    "\n",
    "# Identify carrier names that occur fewer than 500 times in X_train_processed\n",
    "carrier_counts = X_train_processed['Carrier Name'].value_counts()\n",
    "carriers_to_replace = carrier_counts[carrier_counts < 500].index\n",
    "\n",
    "# Replace carrier names with fewer than 500 occurrences with 'OTHER' in all datasets using the identified carriers from X_train\n",
    "for dataset in [X_train_processed, X_val_processed, df_test_processed]:\n",
    "    dataset['Carrier_Name_Simplified'] = dataset['Carrier_Name_Simplified'].replace(carriers_to_replace, 'OTHER')\n",
    "\n",
    "# Print the counts of the simplified carrier names in X_train_processed to verify the result\n",
    "print(\"Counts of 'Carrier_Name_Simplified' feature in X_train_processed:\")\n",
    "print(X_train_processed['Carrier_Name_Simplified'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of unique values in the original 'Carrier Name' feature\n",
    "print(f\"Number of unique values in 'Carrier Name': {X_train_processed['Carrier Name'].nunique()}\")\n",
    "\n",
    "#print the number of unique values in the simplified 'Carrier_Name_Simplified' feature\n",
    "print(f\"Number of unique values in 'Carrier_Name_Simplified': {X_train_processed['Carrier_Name_Simplified'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the 'Carrier Name' column after creating 'Carrier_Name_Simplified'\n",
    "# X_train_processed = X_train_processed.drop(columns=['Carrier Name'])\n",
    "\n",
    "# # Apply to the val set\n",
    "# X_val_processed = X_val_processed.drop(columns=['Carrier Name'])\n",
    "\n",
    "# # Apply to the test set\n",
    "# df_test_processed = df_test_processed.drop(columns=['Carrier Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.13. Body_Part_Category <a class=\"anchor\" id=\"section_6_13\"></a>\n",
    "The Body_Part_Category feature will group the WCIO_Part_of_Body_Code into broader categories. Based on the codes in your document, each range of codes represents a specific body part region (e.g., codes from 10 to 19 represent the head). We’ll map these codes to corresponding regions like “Head,” “Neck,” etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of WCIO Part of Body codes to broader categories\n",
    "part_of_body_mapping = {\n",
    "    **dict.fromkeys(range(10, 20), 'Head'),\n",
    "    **dict.fromkeys(range(20, 30), 'Neck'),\n",
    "    **dict.fromkeys(range(30, 40), 'Upper Extremities'),\n",
    "    **dict.fromkeys(range(40, 50), 'Trunk'),\n",
    "    **dict.fromkeys(range(50, 60), 'Lower Extremities'),\n",
    "    **dict.fromkeys([64, 65, 66, 90, 91, 99], 'Multiple Body Parts'),\n",
    "    **dict.fromkeys([101], 'NonClassificable')\n",
    "\n",
    "}\n",
    "\n",
    "# Creating the Body_Part_Category column by mapping Part of Body codes to categories\n",
    "X_train_processed['Body_Part_Category'] = X_train_processed['WCIO Part Of Body Code'].map(part_of_body_mapping)\n",
    "\n",
    "#Apply to the val set\n",
    "X_val_processed['Body_Part_Category'] = X_train_processed['WCIO Part Of Body Code'].map(part_of_body_mapping)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Body_Part_Category'] = df_test_processed['WCIO Part Of Body Code'].map(part_of_body_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.14. Injury_Nature_Category <a class=\"anchor\" id=\"section_6_14\"></a>\n",
    "The Body_Part_Category feature will group the WCIO_Part_of_Body_Code into broader categories. Based on the codes in the document, each range of codes represents a specific body part region (e.g., codes from 10 to 19 represent the head). We will map these codes to corresponding regions like \"Head,\" \"Neck,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of WCIO Nature of Injury codes to broader categories\n",
    "nature_of_injury_mapping = {\n",
    "    **dict.fromkeys([1], 'No Physical Injury'),\n",
    "    **dict.fromkeys(range(2, 59), 'Specific Injury'),\n",
    "    **dict.fromkeys(range(60, 80), 'Occupational Disease or Cumulative Injury'),\n",
    "    **dict.fromkeys([90, 91], 'Multiple Injuries')\n",
    "}\n",
    "\n",
    "# Creating the Injury_Nature_Category column by mapping Nature of Injury codes to categories\n",
    "X_train_processed['Injury_Nature_Category'] = X_train_processed['WCIO Nature of Injury Code'].map(nature_of_injury_mapping)\n",
    "\n",
    "#Apply to the val set\n",
    "X_val_processed['Injury_Nature_Category'] = X_val_processed['WCIO Nature of Injury Code'].map(nature_of_injury_mapping)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Injury_Nature_Category'] = df_test_processed['WCIO Nature of Injury Code'].map(nature_of_injury_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.15. Injury_Cause_Category <a class=\"anchor\" id=\"section_6_15\"></a>\n",
    "The Injury_Cause_Category feature will classify the WCIO_Cause_of_Injury_Code values into broader cause categories. For example, codes related to burns or scalds can be grouped together, as well as those for falls or motor vehicle accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of WCIO Cause of Injury codes to broader categories\n",
    "cause_of_injury_mapping = {\n",
    "    **dict.fromkeys(range(1, 10), 'Burn or Scald'),\n",
    "    **dict.fromkeys(range(10, 20), 'Caught In, Under, or Between'),\n",
    "    **dict.fromkeys(range(15, 20), 'Cut, Puncture, Scrape'),\n",
    "    **dict.fromkeys(range(25, 35), 'Fall, Slip, or Trip'),\n",
    "    **dict.fromkeys(range(40, 50), 'Motor Vehicle'),\n",
    "    **dict.fromkeys(range(50, 70), 'Strain or Injury By'),\n",
    "    **dict.fromkeys(range(70, 90), 'Striking Against or Stepping On'),\n",
    "    **dict.fromkeys(range(90, 100), 'Miscellaneous Causes')\n",
    "}\n",
    "\n",
    "# Creating the Injury_Cause_Category column by mapping Cause of Injury codes to categories\n",
    "X_train_processed['Injury_Cause_Category'] = X_train_processed['WCIO Cause of Injury Code'].map(cause_of_injury_mapping)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val_processed['Injury_Cause_Category'] = X_val_processed['WCIO Cause of Injury Code'].map(cause_of_injury_mapping)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test_processed['Injury_Cause_Category'] = df_test_processed['WCIO Cause of Injury Code'].map(cause_of_injury_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have created new categorical features (Injury_Nature_Category, Body_Part_Category, Injury_Cause_Category) that provide a more meaningful representation of the original codes, it makes sense to remove the original code features. Keeping them would add redundancy, decrease interpretability, and unnecessarily increase the dimensionality of the dataset, potentially affecting model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removing the code features from train, validation, and test datasets\n",
    "# X_train_processed = X_train_processed.drop(columns=[\n",
    "#     'WCIO Cause of Injury Code',\n",
    "#     'WCIO Nature of Injury Code',\n",
    "#     'WCIO Part Of Body Code'\n",
    "# ])\n",
    "\n",
    "# X_val_processed = X_val_processed.drop(columns=[\n",
    "#     'WCIO Cause of Injury Code',\n",
    "#     'WCIO Nature of Injury Code',\n",
    "#     'WCIO Part Of Body Code'\n",
    "# ])\n",
    "\n",
    "# df_test_processed = df_test_processed.drop(columns=[\n",
    "#     'WCIO Cause of Injury Code',\n",
    "#     'WCIO Nature of Injury Code',\n",
    "#     'WCIO Part Of Body Code'\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.16. Risk of Each Job <a class=\"anchor\" id=\"section_6_16\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique industry descriptions\n",
    "unique_industries = X_train_processed['Industry Code Description'].unique()\n",
    "print(f\"Unique Industry Descriptions: {len(unique_industries)}\")\n",
    "print(unique_industries[:10])  # Display the first 10 industry descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Industry Code Description' and calculate the frequency of claims\n",
    "industry_injury_counts = X_train_processed.groupby('Industry Code Description')['Claim Injury Type'].count()\n",
    "\n",
    "# Normalize the injury frequencies to assign risk scores (1 = Low, 2 = Medium, 3 = High)\n",
    "min_count = industry_injury_counts.min()\n",
    "max_count = industry_injury_counts.max()\n",
    "industry_injury_normalized = (industry_injury_counts - min_count) / (max_count - min_count)\n",
    "\n",
    "# Assign risk levels based on normalized frequencies\n",
    "industry_risk_levels = industry_injury_normalized.apply(lambda x: 1 if x < 0.33 else (2 if x < 0.66 else 3))\n",
    "\n",
    "# Create a mapping dictionary\n",
    "industry_risk_mapping = industry_risk_levels.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new \"Job Risk Level\" column to the dataset\n",
    "X_train_processed['Job Risk Level'] = X_train_processed['Industry Code Description'].map(industry_risk_mapping)\n",
    "X_val_processed['Job Risk Level'] = X_val_processed['Industry Code Description'].map(industry_risk_mapping)\n",
    "df_test_processed['Job Risk Level'] = df_test_processed['Industry Code Description'].map(industry_risk_mapping)\n",
    "\n",
    "# Verify the new column\n",
    "print(X_train_processed[['Industry Code Description', 'Job Risk Level']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Selection <a class=\"anchor\" id=\"chapter7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature selection process involves summarizing the essential features in the dataset to help the model focus on the most relevant information. Here, we assess the categorical and numerical variables in `X_train_processed` by examining unique values, descriptive statistics, and verifying the correctness of categorical bins and continuous feature distributions.\n",
    "\n",
    "The X_train_processed.describe(include='O').T command provides an overview of the categorical features in the dataset, displaying the count, unique values, top (most frequent) category, and its frequency. This summary helps us understand the distribution and cardinality of each categorical feature, which is essential for feature selection and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying descriptive statistics for categorical features in the training dataset\n",
    "X_train_processed.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table provides insight into the number of unique categories within each feature, assisting in identifying high-cardinality features that may need simplification or encoding before modeling.\n",
    "\n",
    "The next output explores the Income_Category feature specifically, showing its categorical distribution across different income levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the distribution of Income_Category\n",
    "X_train_processed['Income_Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Income_Category` feature categorizes the `Average Weekly Wage` into four levels:\n",
    "- **Low Income**\n",
    "- **Lower-Middle Income**\n",
    "- **Upper-Middle Income**\n",
    "- **High Income**\n",
    "\n",
    "This feature provides a socioeconomic context for the claims, allowing the model to account for income-based disparities in claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics for Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features provide comprehensive information across demographics, injury details, socioeconomics, and temporal aspects, making them well-suited to predicting claim outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of `Age Group`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed['Age Group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature allows the model to understand age-related variations in claims, which could influence the type and nature of injuries.\n",
    "\n",
    "This structured feature selection summary provides insights into the main features, both categorical and numerical, used in the model. The focus on grouping, simplification, and categorization ensures the model captures the relevant patterns without unnecessary complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of `County of Injury` and `Zip Code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['County of Injury'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique values in the 'County of Injury' column\n",
    "unique_count = df_train['County of Injury'].nunique()\n",
    "\n",
    "print(f\"Number of unique values in 'County of Injury': {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure ZipCode is treated as a string\n",
    "df_train['Zip Code'] = df_train['Zip Code'].astype(str)\n",
    "\n",
    "# Filter out rows where the first three characters are not digits\n",
    "filtered_df = df_train[df_train['Zip Code'].str[:3].str.isdigit()]\n",
    "\n",
    "# Convert the first three digits to integers and count rows in the range 100-149\n",
    "count = filtered_df[filtered_df['Zip Code'].str[:3].astype(int).between(100, 149)].shape[0]\n",
    "\n",
    "print(f\"Number of rows where the first three digits of ZipCode are between 100 and 149: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8663995376353689"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "514183 / len(df_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_York_allcounties = [\n",
    "    \"Albany\", \"Allegany\", \"Bronx\", \"Broome\", \"Cattaraugus\", \"Cayuga\", \"Chautauqua\", \"Chemung\", \n",
    "    \"Chenango\", \"Clinton\", \"Columbia\", \"Cortland\", \"Delaware\", \"Dutchess\", \"Erie\", \"Essex\", \n",
    "    \"Franklin\", \"Fulton\", \"Genesee\", \"Greene\", \"Hamilton\", \"Herkimer\", \"Jefferson\", \"Kings\", \n",
    "    \"Lewis\", \"Livingston\", \"Madison\", \"Monroe\", \"Montgomery\", \"Nassau\", \"New York\", \"Niagara\", \n",
    "    \"Oneida\", \"Onondaga\", \"Ontario\", \"Orange\", \"Orleans\", \"Oswego\", \"Otsego\", \"Putnam\", \"Queens\", \n",
    "    \"Rensselaer\", \"Richmond\", \"Rockland\", \"Saratoga\", \"Schenectady\", \"Schoharie\", \"Schuyler\", \n",
    "    \"Seneca\", \"St. Lawrence\", \"Steuben\", \"Suffolk\", \"Sullivan\", \"Tioga\", \"Tompkins\", \"Ulster\", \n",
    "    \"Warren\", \"Washington\", \"Wayne\", \"Westchester\", \"Wyoming\", \"Yates\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_York_allcounties_caps = [\n",
    "    \"ALBANY\", \"ALLEGANY\", \"BRONX\", \"BROOME\", \"CATTARAUGUS\", \"CAYUGA\", \"CHAUTAUQUA\", \"CHEMUNG\",\n",
    "    \"CHENANGO\", \"CLINTON\", \"COLUMBIA\", \"CORTLAND\", \"DELAWARE\", \"DUTCHESS\", \"ERIE\", \"ESSEX\",\n",
    "    \"FRANKLIN\", \"FULTON\", \"GENESEE\", \"GREENE\", \"HAMILTON\", \"HERKIMER\", \"JEFFERSON\", \"KINGS\",\n",
    "    \"LEWIS\", \"LIVINGSTON\", \"MADISON\", \"MONROE\", \"MONTGOMERY\", \"NASSAU\", \"NEW YORK\", \"NIAGARA\",\n",
    "    \"ONEIDA\", \"ONONDAGA\", \"ONTARIO\", \"ORANGE\", \"ORLEANS\", \"OSWEGO\", \"OTSEGO\", \"PUTNAM\", \"QUEENS\",\n",
    "    \"RENSSELAER\", \"RICHMOND\", \"ROCKLAND\", \"SARATOGA\", \"SCHENECTADY\", \"SCHOHARIE\", \"SCHUYLER\",\n",
    "    \"SENECA\", \"ST. LAWRENCE\", \"STEUBEN\", \"SUFFOLK\", \"SULLIVAN\", \"TIOGA\", \"TOMPKINS\", \"ULSTER\",\n",
    "    \"WARREN\", \"WASHINGTON\", \"WAYNE\", \"WESTCHESTER\", \"WYOMING\", \"YATES\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification\n",
    "# Convert the 'county_of_injury' column to a set for comparison\n",
    "used_counties = set(df_train['County of Injury'].dropna().str.title())  # Normalize case\n",
    "\n",
    "# Compare with the New_York_allcounties list\n",
    "used_in_list = used_counties.intersection(New_York_allcounties_caps)\n",
    "not_used_in_list = set(New_York_allcounties_caps) - used_counties\n",
    "\n",
    "# Results\n",
    "print(f\"Number of counties used in the dataset that match the New York counties: {len(used_in_list)}\")\n",
    "print(f\"Counties used in the dataset that match: {sorted(used_in_list)}\")\n",
    "print(f\"Number of New York counties not used in the dataset: {len(not_used_in_list)}\")\n",
    "print(f\"Counties not used: {sorted(not_used_in_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows where 'County of Injury' is \"Unknown\"\n",
    "unknown_count = df_train['County of Injury'].str.contains(\"UNKNOWN\", na=False).sum()\n",
    "\n",
    "print(f\"Number of rows with 'Unknown' in 'County of Injury': {unknown_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above lines of code and the correlation between the variables county of injury we can conclude that is irrelevant to use both variables in our project. As our target variable is related to Work Claims we decided that is more important to keep County of Injury."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Categories\n",
    "The features have been organized into three main categories: **Categorical Features**, **Binary Features**, and **Discrete Features**. Each category represents a different data type or structure, allowing for tailored preprocessing steps and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Features: ['Carrier_Name_Simplified', 'County of Injury', 'District Name', 'Gender', 'Medical Fee Region', 'Carrier_District_Interaction', 'Zip_Code_Simplified', 'Carrier Type Merged', 'Income_Category', 'Age Group', 'Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code', 'Body_Part_Category', 'Injury_Nature_Category', 'Injury_Cause_Category', 'promptness_category', 'promptness_C2_category', 'Carrier Name', 'Carrier Type']\n",
      "Binary Features: ['Attorney/Representative', 'COVID-19 Indicator', 'Accident on Weekend']\n",
      "Discrete Features: ['Number of Dependents', 'IME-4 Count', 'Accident_Quarter', 'Accident_Year', 'Accident Day']\n",
      "Continuous Features: ['Days_To_First_Hearing', 'Average Weekly Wage', 'Age at Injury']\n"
     ]
    }
   ],
   "source": [
    "# Define the feature categories\n",
    "categorical_columns = [\n",
    "    'Carrier_Name_Simplified','County of Injury', \n",
    "    'District Name', 'Gender', 'Medical Fee Region', \n",
    "    'Carrier_District_Interaction',\n",
    "    'Zip_Code_Simplified', 'Carrier Type Merged','Income_Category','Age Group', 'Industry Code', \n",
    "    'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', \n",
    "    'WCIO Part Of Body Code','Body_Part_Category',\n",
    "    'Injury_Nature_Category', 'Injury_Cause_Category','promptness_category','promptness_C2_category','Carrier Name','Carrier Type',\n",
    "    \n",
    "]\n",
    "\n",
    "binary_columns = [\n",
    "    'Attorney/Representative', 'COVID-19 Indicator',\n",
    "     'Accident on Weekend'\n",
    "]\n",
    "\n",
    "discrete_columns = [ 'Number of Dependents', 'IME-4 Count',\n",
    "    'Accident_Quarter', 'Accident_Year', 'Accident Day'\n",
    "]\n",
    "\n",
    "continuous_columns = ['Days_To_First_Hearing','Average Weekly Wage','Age at Injury']\n",
    "\n",
    "# Printing the feature categories\n",
    "print(\"Categorical Features:\", categorical_columns)\n",
    "print(\"Binary Features:\", binary_columns)\n",
    "print(\"Discrete Features:\", discrete_columns)\n",
    "print(\"Continuous Features:\", continuous_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Preparing Data</b> <a class=\"anchor\" id=\"section_7_1\"></a>\n",
    "### Converting Discrete Columns to Integer\n",
    "In this step, we ensure all discrete columns are converted to integer data types across the training, validation, and test datasets. This is crucial for consistent data handling and processing in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert discrete columns to integer\n",
    "for column in discrete_columns:\n",
    "    X_train_processed[column] = X_train_processed[column].astype(int)\n",
    "    X_val_processed[column] = X_val_processed[column].astype(int)\n",
    "    df_test_processed[column] = df_test_processed[column].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Features\n",
    "We apply the RobustScaler to the discrete columns. RobustScaler is beneficial for data containing outliers, as it scales data based on the median and interquartile range. This helps normalize the distribution without being heavily affected by outliers. We also use the same scaler to transform validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling features\n",
    "# Usar robustscaler para dados que contêm outliers\n",
    "# Usar minmaxscaler para dados que não têm outliers \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "feat_scaler = MinMaxScaler()\n",
    "X_train_processed[discrete_columns + continuous_columns] = feat_scaler.fit_transform(X_train_processed[discrete_columns + continuous_columns])\n",
    "\n",
    "X_train_num_scaled = X_train_processed[discrete_columns + continuous_columns]\n",
    "\n",
    "#apply same transformation for val data\n",
    "X_val_processed[discrete_columns + continuous_columns] = feat_scaler.transform(X_val_processed[discrete_columns + continuous_columns])\n",
    "\n",
    "X_val_num_scaled = X_val_processed[discrete_columns + continuous_columns]\n",
    "\n",
    "#apply same transformation for test data\n",
    "df_test_processed[discrete_columns + continuous_columns] = feat_scaler.transform(df_test_processed[discrete_columns + continuous_columns])\n",
    "df_test_num_scaled = df_test_processed[discrete_columns + continuous_columns]\n",
    "\n",
    "#X_val, X_train\n",
    "X_train_num_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Check After Scaling\n",
    "After scaling, we check the variance of each scaled feature to confirm that the features have been scaled properly. Lower variance after scaling indicates that data is normalized and less affected by magnitude differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variance of scaled data\n",
    "X_train_num_scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the variance of each discrete column in the scaled training set. These values help validate that the scaling process has achieved a consistent data distribution across features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7.2. Numerical Features</b> <a class=\"anchor\" id=\"section_7_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > ### Spearman and Pearson correlation matrix <a class=\"anchor\" id=\"sub_section_4_1_2\"></a>\n",
    "\n",
    "To better understand the relationships between numerical features, we generate Spearman and Pearson correlation matrices. Correlation matrices help identify linear and monotonic relationships between variables, which can be useful in feature selection and engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap Function\n",
    "The `cor_heatmap` function displays a heatmap of a given correlation matrix. We set various visual parameters to enhance readability:\n",
    "- **figsize**: Sets the figure size.\n",
    "- **cmap**: Chooses the color palette ('viridis').\n",
    "- **mask**: Hides the upper triangle of the matrix for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_heatmap(cor, name):\n",
    "    plt.figure(figsize=(18,12))\n",
    "    sns.heatmap(data = cor.round(2), annot = True, cmap = 'viridis', linecolor = 'white', linewidth=0.5, fmt='.2', mask=np.triu(cor, k=0))\n",
    "    plt.title(f'{name} Correlation Matrix', fontdict = {'fontsize': 20})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman Correlation Calculation and Plotting\n",
    "Here, we calculate the Spearman correlation on the scaled numerical features and then plot the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data_numerical = X_train_num_scaled \n",
    "cor_spearman = corr_data_numerical.corr(method='spearman')\n",
    "\n",
    "cor_heatmap(cor_spearman, 'Spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > ### LASSO Regression <a class=\"anchor\" id=\"sub_section_4_1_3\"></a>\n",
    "\n",
    " The LASSO (Least Absolute Shrinkage and Selection Operator) regression is used here for feature selection by fitting a model to the standardized dataset and analyzing the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression\n",
    "reg = LassoCV().fit(X_train_num_scaled, y_train)\n",
    "#get coefficients\n",
    "coef = pd.Series(reg.coef_, index = X_train_num_scaled.columns)\n",
    "# Sort coefficients to observe importance\n",
    "coef.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "The LASSO model assigns non-zero coefficients only to the features it considers important.\n",
    "\n",
    "IME-4 Count has the highest positive coefficient, indicating a strong positive relationship with the target variable, as well as Days_To_First_Hearing\n",
    "\n",
    "\n",
    "Accident Day and Accident Quarter have smaller negative coefficients, implying a weaker inverse relationship with the target.\n",
    "\n",
    "Average Weekly Wage and Number of Dependents has close to zero impact, suggesting it may not be a significant predictor in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(coef, name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(3,5))\n",
    "    imp_coef.plot(kind=\"barh\", color=\"c\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates a horizontal bar plot of the feature importance values derived from the LASSO model, helping to visualize which features are prioritized by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(coef,'Lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > ### Recursive Feature Elimination - RFE <a class=\"anchor\" id=\"sub_section_4_1_4\"></a>\n",
    "\n",
    " RFE is employed here to further validate the important features as identified by LASSO. By sequentially removing the least important features, RFE helps to refine the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns used in RFE process\n",
    "X_train_num_scaled.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "The selected features after RFE likely overlap with those identified by LASSO, suggesting consistency in feature importance.\n",
    "Using both LASSO and RFE provides a more robust feature selection by cross-validating the importance of individual features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code performs RFE to identify the best subset of features by iterating over a range of feature numbers. The code aims to maximize model performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features\n",
    "nof_list=np.arange(1,8)            \n",
    "high_score=0\n",
    "\n",
    "# Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    model = LogisticRegression()\n",
    "    rfe = RFE(model,n_features_to_select = nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train_num_scaled,y_train)\n",
    "    X_val_rfe = rfe.transform(X_val_num_scaled)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    \n",
    "    score = model.score(X_val_rfe,y_val)\n",
    "    score_list.append(score)\n",
    "    \n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "        \n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "The loop evaluates models with 1 to 3 features, tracking the score for each.\n",
    "\n",
    "The output indicates that 3 features yielded the highest validation score of 0.623391."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying RFE with the Optimal Number of Features\n",
    "Once the optimal number of features is identified, the RFE model is finalized with this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfscv- to select 1st the number of features we should select\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(estimator = model, n_features_to_select = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rfe= rfe.fit_transform(X=X_train_num_scaled, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the results of the feature selection process, listing the features selected by RFE. The code snippet confirms which features were selected as relevant by the RFE model and displays them in a boolean Series, where True indicates the feature was selected and False means it was not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = pd.Series(rfe.support_, index = X_train_num_scaled.columns)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE with SVM will be left to run during days since it takes a lot of time we decided to leave the code and run after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #no of features\n",
    "# nof_list=np.arange(1,8)            \n",
    "# high_score=0\n",
    "# #Variable to store the optimum features\n",
    "# nof=0           \n",
    "# train_score_list =[]\n",
    "# val_score_list = []\n",
    "\n",
    "# for n in range(len(nof_list)):\n",
    "#     #call support vector machines classifier\n",
    "#     model = SVC(kernel = 'linear')\n",
    "    \n",
    "#     #create RFE instance\n",
    "#     rfe = RFE(estimator = model,n_features_to_select = nof_list[n])\n",
    "    \n",
    "#     #fit and transform for training data, transform for val data\n",
    "#     X_train_rfe = rfe.fit_transform(X_train_num_scaled,y_train)\n",
    "#     X_val_rfe = rfe.transform(X_val_num_scaled)\n",
    "    \n",
    "#     model.fit(X_train_rfe,y_train)\n",
    "    \n",
    "#     #storing results on training data\n",
    "#     train_score = model.score(X_train_rfe,y_train)\n",
    "#     train_score_list.append(train_score)\n",
    "    \n",
    "#     #storing results on training data\n",
    "#     val_score = model.score(X_val_rfe,y_val)\n",
    "#     val_score_list.append(val_score)\n",
    "    \n",
    "#     #check best score\n",
    "#     if(val_score > high_score):\n",
    "#         high_score = val_score\n",
    "#         nof = nof_list[n]\n",
    "        \n",
    "#         #adding mention of variables to keep\n",
    "#         features_to_select = pd.Series(rfe.support_, index = X_train_num_scaled.columns)\n",
    "        \n",
    "# print(\"Optimum number of features: %d\" %nof)\n",
    "# print(\"Score with %d features: %f\" % (nof, high_score))\n",
    "# print(f\"Features to select: \\n{features_to_select}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Categorical Features</b> <a class=\"anchor\" id=\"section_7_3\"></a>\n",
    "Chi-squared and MIC for the categorical ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > ### $\\chi ^2$ Test for Independence <a class=\"anchor\" id=\"sub_section_4_2_1\"></a>\n",
    "\n",
    " We'll apply the Chi-squared test on categorical features to check for independence. This test is suitable for categorical data and helps identify features that have significant associations with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make sure that we keep the preprocessed dataset without the encoded columns\n",
    "X_train_processed_copy = X_train_processed.copy()\n",
    "X_val_processed_copy = X_val_processed.copy()\n",
    "df_test_processed_copy = df_test_processed.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: NAO ESQUECER DAR ENCONDING DA VARIAVEL COUNTY OF INJURY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At first: Encoding Categorical Features with LabelEncoder\n",
    "We encode categorical features using LabelEncoder, noting that unseen labels in the validation and test sets are replaced with \"Unknown.\" This approach is a compromise to manage high cardinality without drastically increasing feature dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ## for now lets use label encoder but that does not makes much more sense because we dont have an order... we need to use one hot encoder byt it will increase the dimensionality of the data\n",
    "\n",
    "# # Initialize a LabelEncoder\n",
    "# label_encoders = {}\n",
    "\n",
    "# # Apply LabelEncoder to each categorical column in training and then transform validation and test datasets\n",
    "# for col in categorical_columns:\n",
    "#     le = LabelEncoder()\n",
    "    \n",
    "#     # Fit LabelEncoder on the training data\n",
    "#     X_train_processed[col] = le.fit_transform(X_train_processed[col])\n",
    "    \n",
    "#     # Handle unseen labels in validation and test data\n",
    "#     # Add an \"Unknown\" label to classes to handle unseen labels\n",
    "#     le.classes_ = np.append(le.classes_, 'Unknown')\n",
    "\n",
    "#     # Replace unseen labels in validation and test data with 'Unknown'\n",
    "#     X_val_processed[col] = X_val_processed[col].apply(lambda x: x if x in le.classes_ else 'Unknown')\n",
    "#     df_test_processed[col] = df_test_processed[col].apply(lambda x: x if x in le.classes_ else 'Unknown')\n",
    "\n",
    "#     # Transform validation and test datasets using the fitted label encoder\n",
    "#     X_val_processed[col] = le.transform(X_val_processed[col])\n",
    "#     df_test_processed[col] = le.transform(df_test_processed[col])\n",
    "\n",
    "#     # Store the label encoder for future use if needed\n",
    "#     label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second approach: Encoding Categorical Features with LabelEncoder, OneHotEncoder and Frequency encoder\n",
    "As our second approach we decided to perform different type of encoding according with each categorical variable. For ordinal features (those with an inherent order like age groups), Label Encoding is applied to assign each category a unique numerical value. For nominal features with low cardinality (few unique values), One-Hot Encoding is used to create binary columns representing each category. For high-cardinality features (many unique values like zip codes), Frequency Encoding is applied to convert categories to their frequency of occurrence, reducing dimensionality without introducing too many columns.\n",
    "\n",
    "These encoding methods are used to properly represent categorical features in a numerical format while considering their characteristics (order or cardinality), which helps models interpret them effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Example columns to encode\n",
    "ordinal_features = ['promptness_category', 'promptness_C2_category', 'Age Group', 'Income_Category']\n",
    "low_cardinality_cols = ['Gender', 'District Name', 'Medical Fee Region']\n",
    "high_cardinality_cols = ['Carrier_Name_Simplified', 'Zip_Code_Simplified', 'Industry Code', 'County of Injury', \n",
    "                         'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "\n",
    "# Step 1: Label Encode ordinal features (features that have an intrinsic order)\n",
    "label_encoder_1 = LabelEncoder()\n",
    "for col in ordinal_features:\n",
    "    X_train_processed_copy[col] = label_encoder_1.fit_transform(X_train_processed_copy[col])\n",
    "    X_val_processed_copy[col] = label_encoder_1.transform(X_val_processed_copy[col])\n",
    "    df_test_processed_copy[col] = label_encoder_1.transform(df_test_processed_copy[col])\n",
    "\n",
    "# Step 2: One-Hot Encode low-cardinality nominal features\n",
    "one_hot_encoder = OneHotEncoder(drop='first', sparse_output=False)  # drop='first' to avoid multicollinearity\n",
    "one_hot_encoded_train = one_hot_encoder.fit_transform(X_train_processed_copy[low_cardinality_cols])\n",
    "one_hot_encoded_val = one_hot_encoder.transform(X_val_processed_copy[low_cardinality_cols])\n",
    "one_hot_encoded_test = one_hot_encoder.transform(df_test_processed_copy[low_cardinality_cols])\n",
    "\n",
    "# Convert the one-hot encoded arrays back to DataFrame\n",
    "one_hot_encoded_train_df = pd.DataFrame(one_hot_encoded_train, columns=one_hot_encoder.get_feature_names_out(), index=X_train_processed_copy.index)\n",
    "one_hot_encoded_val_df = pd.DataFrame(one_hot_encoded_val, columns=one_hot_encoder.get_feature_names_out(), index=X_val_processed_copy.index)\n",
    "one_hot_encoded_test_df = pd.DataFrame(one_hot_encoded_test, columns=one_hot_encoder.get_feature_names_out(), index=df_test_processed_copy.index)\n",
    "\n",
    "# Concatenate the original datasets with the one-hot encoded columns\n",
    "X_train_processed_copy = pd.concat([X_train_processed_copy, one_hot_encoded_train_df], axis=1)\n",
    "X_val_processed_copy = pd.concat([X_val_processed_copy, one_hot_encoded_val_df], axis=1)\n",
    "df_test_processed_copy = pd.concat([df_test_processed_copy, one_hot_encoded_test_df], axis=1)\n",
    "\n",
    "# Drop the original low-cardinality nominal columns after encoding them\n",
    "X_train_processed_copy.drop(columns=low_cardinality_cols, inplace=True)\n",
    "X_val_processed_copy.drop(columns=low_cardinality_cols, inplace=True)\n",
    "df_test_processed_copy.drop(columns=low_cardinality_cols, inplace=True)\n",
    "\n",
    "# Step 3: Frequency Encoding for high-cardinality nominal features\n",
    "for col in high_cardinality_cols:\n",
    "    freq = X_train_processed_copy[col].value_counts()  # Fit on training set only\n",
    "    X_train_processed_copy[col + '_freq'] = X_train_processed_copy[col].map(freq)\n",
    "    X_val_processed_copy[col + '_freq'] = X_val_processed_copy[col].map(freq)\n",
    "    df_test_processed_copy[col + '_freq'] = df_test_processed_copy[col].map(freq)\n",
    "\n",
    "# Drop the original high-cardinality columns after encoding them\n",
    "X_train_processed_copy.drop(columns=high_cardinality_cols, inplace=True)\n",
    "X_val_processed_copy.drop(columns=high_cardinality_cols, inplace=True)\n",
    "df_test_processed_copy.drop(columns=high_cardinality_cols, inplace=True)\n",
    "\n",
    "# Final datasets after encoding\n",
    "X_train_final = X_train_processed_copy\n",
    "X_val_final = X_val_processed_copy\n",
    "X_test_final = df_test_processed_copy\n",
    "\n",
    "# Summary\n",
    "print(\"Encoded X_train_final columns:\", X_train_final.columns)\n",
    "print(\"Encoded X_val_final columns:\", X_val_final.columns)\n",
    "print(\"Encoded X_test_final columns:\", X_test_final.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Are There Missing Values (NaN) in Validation and Test Sets?\n",
    "\n",
    "During the encoding process, missing values (NaN) may appear in the validation and test sets because certain categories that exist in these sets might not be present in the training set. For instance, during frequency encoding, if a category exists only in the validation or test set but not in the training set, it will result in missing values when encoding those rows.\n",
    "\n",
    "To address this, we can fill these missing values with a placeholder such as `0`, ensuring consistent feature representation across all datasets, for now. For next deliverable we will try to implement better approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle this, we will fill the missing values with 0 for now.\n",
    "X_val_final.fillna(0, inplace=True)\n",
    "X_test_final.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare final dataset for MIC and χ² analysis\n",
    "# Use only the encoded features for the MIC and chi-squared tests\n",
    "mic_chi2_features = binary_columns + ordinal_features + list(one_hot_encoded_train_df.columns) + [f\"{col}_freq\" for col in high_cardinality_cols]\n",
    "X_train_mic_chi2 = X_train_processed_copy[mic_chi2_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_mic_chi2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mic_chi2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-Squared Test for Feature Selection\n",
    "The Chi-squared test helps us identify the most relevant categorical and binary features by evaluating their independence with the target variable. In this analysis, we set a threshold to keep the 25 most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi-square test\n",
    "threshold = 20 #we want to only keep the 20 most relevant features\n",
    "high_score_features_chi2 = []\n",
    "scores = []\n",
    "names_Chi2 = []\n",
    "\n",
    "feature_scores = SelectKBest(chi2, k=threshold).fit(X_train_mic_chi2,y_train).scores_\n",
    "\n",
    "high_score_features = []\n",
    "for score, f_name in sorted(zip(feature_scores,X_train_mic_chi2.columns), reverse=True)[:threshold]:\n",
    "        high_score_features_chi2.append(f_name)\n",
    "        scores.append(score)\n",
    "        names_Chi2.append(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_Chi2\n",
    "# high_score_features_chi2  # Output the list of selected top features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Mutual Information Criterion (MIC) for Feature Selection <a class=\"anchor\" id=\"sub_section_4_2_2\"></a>\n",
    "\n",
    "The Mutual Information Criterion is used to measure the dependence between each feature and the target variable. By setting a threshold, we retain only the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #MIC \n",
    "# Set the threshold for the top features to select\n",
    "threshold = 20\n",
    "high_score_features_MIC = []\n",
    "scores = []\n",
    "names_MIC = []\n",
    "\n",
    "# Apply mutual information criterion\n",
    "feature_scores = mutual_info_classif(X_train_mic_chi2, y_train, random_state=42)\n",
    "\n",
    "# Sort and select the top features\n",
    "for score, f_name in sorted(zip(feature_scores, X_train_mic_chi2.columns), reverse=True)[:threshold]:\n",
    "    high_score_features_MIC.append(f_name)\n",
    "    scores.append(score)\n",
    "    names_MIC.append(f_name)\n",
    "    \n",
    "# Display the list of selected top features by MIC\n",
    "names_MIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection of Selected Features by Chi-squared and MIC\n",
    "To identify the most relevant categorical features, we take the intersection of features selected by both Chi-squared and MIC methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common features between Chi-squared and MIC selections\n",
    "common_categorical_features = list(set(names_Chi2).intersection(set(names_MIC)))\n",
    "features_selected = common_categorical_features  # Final list of selected features\n",
    "features_selected  # Display the list of intersected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Final Features</b> <a class=\"anchor\" id=\"section_7_4\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Predictor             | Spearman | Lasso   | RFE LR   | RFE SVM | What to do? (One possible way to \"solve\")        |\n",
    "|-----------------------|----------|---------|----------|---------|--------------------------------------------------|\n",
    "| Number of Dependents  | Discard  | Discard | Discard  |         | Discard                                          |\n",
    "| IME-4 Count           | Keep     | Keep    | Keep     |         | Include in the model                             |\n",
    "| Accident_Quarter      | Keep  | Discard | Discard  |         | Discard                                          |\n",
    "| Accident_Year         | Keep     | Keep    | Keep     |         | Include in the model                             |\n",
    "| Accident Day          | Keep  | Discard | Discard  |         | Discard                                          |\n",
    "| Days_To_First_Hearing | Keep     | Keep    | Keep     |         | Include in the model                             |\n",
    "| Average Weekly Wage   | Keep  | Discard    | Discard  |         | Discard                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = [\n",
    "    # Categorical features selected through MIC and chi2\n",
    "    'WCIO Cause of Injury Code_freq',\n",
    "    'County of Injury_freq',\n",
    "    'District Name_HAUPPAUGE',\n",
    "    'Zip_Code_Simplified_freq',\n",
    "    'COVID-19 Indicator',\n",
    "    'Age Group',\n",
    "    'WCIO Part Of Body Code_freq',\n",
    "    'Carrier_Name_Simplified_freq',\n",
    "    'WCIO Nature of Injury Code_freq',\n",
    "    'promptness_C2_category',\n",
    "    'promptness_category',\n",
    "    'Gender_M',\n",
    "    'District Name_BINGHAMTON',\n",
    "    'District Name_NYC',\n",
    "    'Medical Fee Region_IV',\n",
    "    'Attorney/Representative',\n",
    "    'Income_Category',\n",
    "    'Industry Code_freq',\n",
    "    # Important numerical features\n",
    "    'IME-4 Count',\n",
    "    'Accident_Year',\n",
    "    'Days_To_First_Hearing'\n",
    "]\n",
    "\n",
    "\n",
    "# Extracting the final features from the processed datasets\n",
    "X_train_final = X_train_processed_copy[final_features]\n",
    "X_val_final = X_val_processed_copy[final_features]\n",
    "X_test_final = df_test_processed_copy[final_features]\n",
    "\n",
    "# Print the shape of the final datasets to confirm consistency\n",
    "print(f\"X_train_final shape: {X_train_final.shape}\")\n",
    "print(f\"X_val_final shape: {X_val_final.shape}\")\n",
    "print(f\"X_test_final shape: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build a Simple Model and Assess Performance\n",
    "#### Problem Type and Model Selection\n",
    "The problem we are dealing with is a classification problem since our goal is to predict the type of variable claim injury as our target variable. As it is a categorical variable we decided to try algorithms like Logistic Regression, Random Forest, XGBoost and other classification models. To start, a simple logistic regression was built due to its interpretability and ease of use. Moreover, it supports an effective starting point for further analysis and comparison against more complex models.\n",
    "\n",
    "#### Cross-Validation for Performance Assessment\n",
    "In order to assess the model’s performance we decided to use cross validation. This technique ensures a reliable evaluation by divding our data into multiple folds and having training/testing the model on different subsets. This really helps to avoid overfitting (which was a problem that from the beginning we identify we had) and provides a comprehensive view of how well the model generalizes to unseen data.\n",
    "\n",
    "#### Metrics for Evaluation\n",
    "For model evaluation we look into some metrics like:\n",
    "Precision, Recall, and F1-Score given that our target variable is claim injury type, and the dataset has an imbalanced class distributions. \n",
    "Precision tells us how many of the predicted positive instances are actually positive, while recall shows how many of the actual positives were correctly predicted. **F1 Score* provides a balance between precision and recall, which is crucial when handling imbalanced data.\n",
    "Even though accuracy can provide a general idea of overall correctness, it is not our main focus due to the class imbalance. Relying solely on accuracy could give misleading insights, as it may be high even if the model is not performing well on minority classes\n",
    "\n",
    "With these metrics we ensure a global evaluation of the model, especially considering the facts above mentioned such as imbalanced dataset and helps guiding model improvement effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Modeling <a class=\"anchor\" id=\"chapter8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Standard Scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "final_features_X_train_scaled_std = scaler.fit_transform(X_train_final)\n",
    "final_features_X_val_scaled_std = scaler.transform(X_val_final)\n",
    "final_features_df_test_scaled_std = scaler.transform(X_test_final)\n",
    "\n",
    "# Logistic Regression Model\n",
    "log_reg_std = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_std.fit(final_features_X_train_scaled_std, y_train)\n",
    "pred_train_log_final_std = log_reg_std.predict(final_features_X_train_scaled_std)\n",
    "pred_val_log_final_std = log_reg_std.predict(final_features_X_val_scaled_std)\n",
    "\n",
    "# Metrics for Logistic Regression\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "metrics(y_train, pred_train_log_final_std, y_val, pred_val_log_final_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the best model\n",
    "y_test_pred_best_std = log_reg_std.predict(final_features_df_test_scaled_std)\n",
    "\n",
    "# Decode the predictions \n",
    "y_test_pred_best_decoded_std = label_encoder.inverse_transform(y_test_pred_best_std)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "test_predictions_df_log_std = pd.DataFrame({\n",
    "    'Claim Identifier': X_test_final.index, \n",
    "    'Claim Injury Type': y_test_pred_best_decoded_std  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for Kaggle\n",
    "test_predictions_df_log_std.to_csv('kaggle_submission_predictions_log_std.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Min Max Scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "final_features_X_train_scaled_minmax = scaler.fit_transform(X_train_final)\n",
    "final_features_X_val_scaled_minmax = scaler.transform(X_val_final)\n",
    "final_features_df_test_scaled_minmax = scaler.transform(X_test_final)\n",
    "\n",
    "# Logistic Regression Model\n",
    "log_reg_minmax = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_minmax.fit(final_features_X_train_scaled_minmax, y_train)\n",
    "pred_train_log_final_minmax = log_reg_minmax.predict(final_features_X_train_scaled_minmax)\n",
    "pred_val_log_final_minmax = log_reg_minmax.predict(final_features_X_val_scaled_minmax)\n",
    "\n",
    "# Metrics for Logistic Regression\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "metrics(y_train, pred_train_log_final_minmax, y_val, pred_val_log_final_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the best model\n",
    "y_test_pred_best_minmax = log_reg_minmax.predict(final_features_df_test_scaled_minmax)\n",
    "\n",
    "# Decode the predictions \n",
    "y_test_pred_best_decoded_minmax = label_encoder.inverse_transform(y_test_pred_best_minmax)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "test_predictions_df_log_minmax = pd.DataFrame({\n",
    "    'Claim Identifier': X_test_final.index, \n",
    "    'Claim Injury Type': y_test_pred_best_decoded_minmax  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for Kaggle\n",
    "test_predictions_df_log_minmax.to_csv('kaggle_submission_predictions_log_minmax.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Robust Scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "final_features_X_train_scaled_rob = scaler.fit_transform(X_train_final)\n",
    "final_features_X_val_scaled_rob = scaler.transform(X_val_final)\n",
    "final_features_df_test_scaled_rob = scaler.transform(X_test_final)\n",
    "\n",
    "# Logistic Regression Model\n",
    "log_reg_rob = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_rob.fit(final_features_X_train_scaled_rob, y_train)\n",
    "pred_train_log_final_rob = log_reg_rob.predict(final_features_X_train_scaled_rob)\n",
    "pred_val_log_final_rob = log_reg_rob.predict(final_features_X_val_scaled_rob)\n",
    "\n",
    "# Metrics for Logistic Regression\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "metrics(y_train, pred_train_log_final_rob, y_val, pred_val_log_final_rob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the best model\n",
    "y_test_pred_best_rob = log_reg_rob.predict(final_features_df_test_scaled_rob)\n",
    "\n",
    "# Decode the predictions \n",
    "y_test_pred_best_decoded_rob = label_encoder.inverse_transform(y_test_pred_best_rob)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "test_predictions_df_log_rob  = pd.DataFrame({\n",
    "    'Claim Identifier': X_test_final.index, \n",
    "    'Claim Injury Type': y_test_pred_best_decoded_rob  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for Kaggle\n",
    "test_predictions_df_log_rob.to_csv('kaggle_submission_predictions_log_rob.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train test Split\n",
    "| Model Name | Scaler | Resampling Technique | F1 Score (Train)| F1 Score (Validation)| Kaggle Score |\n",
    "|------------|-------------|----------------------|------------------|-----------------------|--------------|\n",
    "| Logistic Regression | Standard | None  |        0.23       |           0.23          |     0.22619   |\n",
    "| Logistic Regression | MinMax | None  |         0.22         |          0.22            |     0.21721          |\n",
    "| Logistic Regression  | Robust | None   |        0.22         |          0.22           |      0.22046      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## KNN\n",
    "1. Simple KNN Model\n",
    "### **Standard Scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Model\n",
    "knn_std = KNeighborsClassifier(n_neighbors=6)\n",
    "knn_std.fit(final_features_X_train_scaled_std, y_train)\n",
    "pred_train_knn_final_std = knn_std.predict(final_features_X_train_scaled_std)\n",
    "pred_val_knn_final_std = knn_std.predict(final_features_X_val_scaled_std)\n",
    "\n",
    "# Metrics for K-Nearest Neighbors\n",
    "print(\"\\nK-Nearest Neighbors Metrics:\")\n",
    "metrics(y_train, pred_train_knn_final_std, y_val, pred_val_knn_final_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the best model\n",
    "y_test_pred_best_knn_std = knn_std.predict(final_features_df_test_scaled_std)\n",
    "\n",
    "# Decode the predictions \n",
    "y_test_pred_best_decoded_knn_std = label_encoder.inverse_transform(y_test_pred_best_knn_std)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "test_predictions_knn_std = pd.DataFrame({\n",
    "    'Claim Identifier': X_test_final.index, \n",
    "    'Claim Injury Type': y_test_pred_best_decoded_knn_std \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for Kaggle\n",
    "test_predictions_knn_std.to_csv('kaggle_submission_predictions_knn_std.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_knn_std.to_csv('/Users/peresgoncalo/Documents/Mestrado/C.U./ML/Project/project_data/SubmissionsGP/kaggle_submission_predictions_knn_std.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Standard Scaler CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Initialize the KNN model with 6 neighbors\n",
    "knn_std = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation for KNN\n",
    "cv_scores_knn = cross_val_score(knn_std, final_features_X_train_scaled_std, y_train, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"K-Nearest Neighbors Cross-Validation Scores:\", cv_scores_knn)\n",
    "print(\"Mean Cross-Validation Score for K-Nearest Neighbors:\", cv_scores_knn.mean())\n",
    "\n",
    "# Fit the KNN model on the full training data after cross-validation\n",
    "knn_std.fit(final_features_X_train_scaled_std, y_train)\n",
    "\n",
    "# Calculate metrics for training and validation sets\n",
    "pred_train_knn_final_std = knn_std.predict(final_features_X_train_scaled_std)\n",
    "pred_val_knn_final_std = knn_std.predict(final_features_X_val_scaled_std)\n",
    "\n",
    "# Metrics for K-Nearest Neighbors\n",
    "print(\"\\nK-Nearest Neighbors Metrics:\")\n",
    "metrics(y_train, pred_train_knn_final_std, y_val, pred_val_knn_final_std)\n",
    "\n",
    "# Predict on the test set with the trained model\n",
    "y_test_pred_best_knn_std = knn_std.predict(final_features_df_test_scaled_std)\n",
    "\n",
    "# Decode the predictions\n",
    "y_test_pred_best_decoded_knn_std = label_encoder.inverse_transform(y_test_pred_best_knn_std)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "test_predictions_knn_std = pd.DataFrame({\n",
    "    'Claim Identifier': X_test_final.index, \n",
    "    'Claim Injury Type': y_test_pred_best_decoded_knn_std \n",
    "})\n",
    "\n",
    "# Save the submission to a CSV file\n",
    "test_predictions_knn_std.to_csv('/Users/peresgoncalo/Documents/Mestrado/C.U./ML/Project/project_data/SubmissionsGP/kaggle_submission_predictions_knn_std_cv.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "##### Train test Split\n",
    "| Model Name | Scaler | Resampling Technique | F1 Score (Train)| F1 Score (Validation)| Kaggle Score |\n",
    "|------------|-------------|----------------------|------------------|-----------------------|--------------|\n",
    "| KNN  | Standard | None  |               |                 |        |\n",
    "| KNN | MinMax | None  |              |                    |      |\n",
    "| KNN  | Robust | None   |                  |                       |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. KNN with KD-Tree as algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K-Nearest Neighbors Model with KD-Tree\n",
    "knn_kd_tree = KNeighborsClassifier(n_neighbors=7, weights='distance', algorithm='kd_tree')\n",
    "knn_kd_tree.fit(final_features_X_train_scaled_rob, y_train)\n",
    "pred_train_knn_final_kd = knn_kd_tree.predict(final_features_X_train_scaled_rob)\n",
    "pred_val_knn_final_kd = knn_kd_tree.predict(final_features_X_val_scaled_rob)\n",
    "\n",
    "# # Metrics for K-Nearest Neighbors using KD-Tree\n",
    "print(\"\\nK-Nearest Neighbors (KD-Tree) Metrics:\")\n",
    "metrics(y_train, pred_train_knn_final_kd, y_val, pred_val_knn_final_kd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the best model\n",
    "y_test_pred_knn_kd_tree = knn_kd_tree.predict(final_features_df_test_scaled_rob) \n",
    "\n",
    "# # Decode the predictions if necessary\n",
    "y_test_pred_knn_kd_tree = label_encoder.inverse_transform(y_test_pred_knn_kd_tree)\n",
    "\n",
    "# # Prepare the submission DataFrame\n",
    "test_predictions_knn_kd_tree = pd.DataFrame({\n",
    "     'Claim Identifier': final_features_df_test_scaled_rob.index, \n",
    "     'Claim Injury Type': y_test_pred_knn_kd_tree  \n",
    " })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_predictions_knn_kd_tree.head(3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to CSV for Kaggle\n",
    " test_predictions_knn_kd_tree.to_csv('/Users/peresgoncalo/Documents/Mestrado/C.U./ML/Project/project_data/SubmissionsGP/predictions_knn_kd_tree.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. KNN with Brute as algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K-Nearest Neighbors Model with KD-Tree\n",
    "knn_brute = KNeighborsClassifier(n_neighbors=7, weights='distance', algorithm='brute')\n",
    "knn_brute.fit(final_features_X_train_scaled_rob, y_train)\n",
    "pred_train_knn_final_brute = knn_brute.predict(final_features_X_train_scaled_rob)\n",
    "pred_val_knn_final_brute = knn_brute.predict(final_features_X_val_scaled_rob)\n",
    "\n",
    "# # Metrics for K-Nearest Neighbors using KD-Tree\n",
    "print(\"\\nK-Nearest Neighbors (KD-Tree) Metrics:\")\n",
    "metrics(y_train, pred_train_knn_final_brute, y_val, pred_val_knn_final_brute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict on the test set with the best model\n",
    "y_test_pred_knn_brute = knn_brute.predict(final_features_df_test_scaled_rob) #subs for log_reg_smote\n",
    "\n",
    "# # Decode the predictions if necessary\n",
    "y_test_pred_knn_brute = label_encoder.inverse_transform(y_test_pred_knn_brute)\n",
    "\n",
    "# # Prepare the submission DataFrame\n",
    "test_predictions_knn_brute = pd.DataFrame({\n",
    "     'Claim Identifier': final_features_df_test_scaled_rob.index, \n",
    "     'Claim Injury Type': y_test_pred_knn_brute  \n",
    " })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for Kaggle\n",
    " test_predictions_knn_brute.to_csv('/Users/peresgoncalo/Documents/Mestrado/C.U./ML/Project/project_data/SubmissionsGP/predictions_knn_brute.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Name               | Split Method     | F1 Score (Train) | F1 Score (Validation) | Kaggle Score |\n",
    "|--------------------------|------------------|------------------|-----------------------|--------------|\n",
    "| K-Nearest Neighbors -  kd_tree   | Train-Test Split |             |                  |          |\n",
    "| K-Nearest Neighbors - brute   | Train-Test Split |             |                  |          |\n",
    "| K-Nearest Neighbors (KNN)     | Train-Test Split |             |                  |          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Decision Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decision Tree Model\n",
    "# dt_std = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Fit model with final features\n",
    "# dt_std.fit(final_features_X_train_scaled_std, y_train)\n",
    "# y_pred_dt_final = dt_std.predict(final_features_X_val_scaled_std)\n",
    "\n",
    "# # Metrics for Decision Tree\n",
    "# dt_std.fit(final_features_X_train_scaled_std, y_train)\n",
    "# pred_train_dt_final_std = dt_std.predict(final_features_X_train_scaled_std)\n",
    "# pred_val_dt_final_std = dt_std.predict(final_features_X_val_scaled_std)\n",
    "# metrics(y_train, pred_train_dt_final_std, y_val, pred_val_dt_final_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict on the test set with the best model\n",
    "# y_test_pred_best_dt_std = dt_std.predict(final_features_df_test_scaled_std)\n",
    "\n",
    "# # Decode the predictions if necessary\n",
    "# y_test_pred_best_decoded_dt_std = label_encoder.inverse_transform(y_test_pred_best_dt_std)\n",
    "\n",
    "# # Prepare the submission DataFrame\n",
    "# test_predictions_dt_std = pd.DataFrame({\n",
    "#     'Claim Identifier': X_test_final.index, \n",
    "#     'Claim Injury Type': y_test_pred_best_decoded_dt_std  \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to CSV for Kaggle\n",
    "# test_predictions_dt_std.to_csv('kaggle_submission_predictions_dt_std.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Decision Tree model with pruning parameters for final features and potential value features since we have overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pruned Decision Tree Classifier\n",
    "# pruned_dt_std = DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_split =150, min_samples_leaf=150)\n",
    "\n",
    "# # Fit pruned Decision Tree model with final features and calculate metrics for training and validation\n",
    "# pruned_dt_std.fit(final_features_X_train_scaled_std, y_train)\n",
    "# pred_train_pruned_dt_std = pruned_dt_std.predict(final_features_X_train_scaled_std)\n",
    "# pred_val_pruned_dt_std = pruned_dt_std.predict(final_features_X_val_scaled_std)\n",
    "# metrics(y_train, pred_train_pruned_dt_std, y_val, pred_val_pruned_dt_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict on the test set with the best model\n",
    "# y_test_pred_pruned_dt_std = dt_std.predict(final_features_df_test_scaled_std)\n",
    "\n",
    "# # Decode the predictions if necessary\n",
    "# y_test_pred_best_decoded_pruned_dt_std = label_encoder.inverse_transform(y_test_pred_pruned_dt_std)\n",
    "\n",
    "# # Prepare the submission DataFrame\n",
    "# test_predictions_pruned_dt_std = pd.DataFrame({\n",
    "#     'Claim Identifier': final_features_df_test.index, \n",
    "#     'Claim Injury Type': y_test_pred_best_decoded_pruned_dt_std  \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to CSV for Kaggle\n",
    "# test_predictions_pruned_dt_std.to_csv('kaggle_submission_predictions_pruned_dt_std.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Name | Scaler | Resampling Technique | F1 Score (Train)| F1 Score (Validation)| Kaggle Score |\n",
    "|------------|-------------|----------------------|------------------|-----------------------|--------------|\n",
    "| Decision Tree  | Standard | None  |             |               |        |\n",
    "| Pruned Decision Tree | Standard | None  |              |                    |      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_std = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(rf_std, final_features_X_train_scaled_std, y_train, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", cv_scores.mean())\n",
    "\n",
    "# After cross-validation, fit the model on the full training data\n",
    "rf_std.fit(final_features_X_train_scaled_std, y_train)\n",
    "\n",
    "# Calculate metrics for training and validation sets\n",
    "pred_train_rf_std = rf_std.predict(final_features_X_train_scaled_std)\n",
    "pred_val_rf_std = rf_std.predict(final_features_X_val_scaled_std)\n",
    "metrics(y_train, pred_train_rf_std, y_val, pred_val_rf_std)\n",
    "\n",
    "# Predict on the test set with the trained model\n",
    "y_test_pred_rf_std = rf_std.predict(final_features_df_test_scaled_std)\n",
    "\n",
    "# Decode predictions if necessary\n",
    "y_test_pred_best_decoded_rf_std = label_encoder.inverse_transform(y_test_pred_rf_std)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "test_predictions_rf_std = pd.DataFrame({\n",
    "    'Claim Identifier': X_test_final.index, \n",
    "    'Claim Injury Type': y_test_pred_best_decoded_rf_std  \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Random Forest model with final features and potential value features\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Classifier\n",
    "#rf_std = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "# Fit Random Forest model with final features and calculate metrics for training and validation\n",
    "#rf_std.fit(final_features_X_train_scaled_std, y_train)\n",
    "#pred_train_rf_std= rf_std.predict(final_features_X_train_scaled_std)\n",
    "#pred_val_rf_std = rf_std.predict(final_features_X_val_scaled_std)\n",
    "#metrics(y_train, pred_train_rf_std, y_val, pred_val_rf_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the best model\n",
    "#y_test_pred_rf_std = rf_std.predict(final_features_df_test_scaled_std)\n",
    "\n",
    "# Decode the predictions if necessary\n",
    "#y_test_pred_best_decoded_rf_std= label_encoder.inverse_transform(y_test_pred_rf_std)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "#test_predictions_rf_std = pd.DataFrame({\n",
    "#    'Claim Identifier': X_test_final.index, \n",
    "#    'Claim Injury Type': y_test_pred_best_decoded_rf_std  \n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for Kaggle\n",
    "test_predictions_rf_std.to_csv('kaggle_submission_predictions_rf_std.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "| Model Name | Scaler | Resampling Technique | F1 Score (Train)| F1 Score (Validation)| Kaggle Score |\n",
    "|------------|-------------|----------------------|------------------|-----------------------|--------------|\n",
    "| Random Forest  | Standard | None  |      0.22       |      0.22         |   0.21453     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit XGBoost model with final features and potential value features\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost Classifier\n",
    "# xgb_std = XGBClassifier(n_estimators=100, max_depth=5, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# # Fit XGBoost model with final features and calculate metrics for training and validation\n",
    "# xgb_std.fit(final_features_X_train_scaled_std, y_train)\n",
    "# pred_train_xgb_std = xgb_std.predict(final_features_X_train_scaled_std)\n",
    "# pred_val_xgb_std = xgb_std.predict(final_features_X_val_scaled_std)\n",
    "\n",
    "# metrics(y_train, pred_train_xgb_std, y_val, pred_val_xgb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
